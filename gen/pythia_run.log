nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251213_105053
Output Directory: wikipedia/experiments/run_20251213_105053
==================================================================
Configuration saved to: wikipedia/experiments/run_20251213_105053/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251213_105053/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_1e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=1e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251213_105053/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 162.281472
model_params (million) 162.281472
12/13/2025 10:51:04 - INFO - __main__ - ***** Running training *****
12/13/2025 10:51:04 - INFO - __main__ -   Num examples = 4688
12/13/2025 10:51:04 - INFO - __main__ -   Num Epochs = 20
12/13/2025 10:51:04 - INFO - __main__ -   Instantaneous batch size per device = 1
12/13/2025 10:51:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/13/2025 10:51:04 - INFO - __main__ -   Gradient Accumulation steps = 8
12/13/2025 10:51:04 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  2.7673158645629883
correct cnt is:  1164 all is:  4688 ratio is:  0.24829351535836178
epoch 0: perplexity: 23.061636531427936 perplexity_train: 19.492403472225792
____
0.24829351535836178
23.061636531427936
19.492403472225792
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  2.7545368671417236
correct cnt is:  1770 all is:  4688 ratio is:  0.3775597269624573
epoch 1: perplexity: 22.949009892223735 perplexity_train: 16.95439457500352
____
0.3775597269624573
22.949009892223735
16.95439457500352
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  2.7541797161102295
correct cnt is:  2534 all is:  4688 ratio is:  0.5405290102389079
epoch 2: perplexity: 23.30663644141408 perplexity_train: 14.896767132621736
____
0.5405290102389079
23.30663644141408
14.896767132621736
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  2.769416093826294
correct cnt is:  3359 all is:  4688 ratio is:  0.7165102389078498
epoch 3: perplexity: 24.16145665298921 perplexity_train: 13.194151516462206
____
0.7165102389078498
24.16145665298921
13.194151516462206
_____
training epoch 4
