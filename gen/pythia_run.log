nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251205_181716
Output Directory: wikipedia/experiments/run_20251205_181716
==================================================================
Configuration saved to: wikipedia/experiments/run_20251205_181716/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251205_181716/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251205_181716/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 162.281472
model_params (million) 162.281472
12/05/2025 18:17:28 - INFO - __main__ - ***** Running training *****
12/05/2025 18:17:28 - INFO - __main__ -   Num examples = 4688
12/05/2025 18:17:28 - INFO - __main__ -   Num Epochs = 20
12/05/2025 18:17:28 - INFO - __main__ -   Instantaneous batch size per device = 1
12/05/2025 18:17:28 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/05/2025 18:17:28 - INFO - __main__ -   Gradient Accumulation steps = 8
12/05/2025 18:17:28 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  2.9394211769104004
correct cnt is:  2130 all is:  4688 ratio is:  0.4543515358361775
epoch 0: perplexity: 28.421015903393194 perplexity_train: 19.330671830791278
____
0.4543515358361775
28.421015903393194
19.330671830791278
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  2.946636199951172
correct cnt is:  3769 all is:  4688 ratio is:  0.8039675767918089
epoch 1: perplexity: 29.356821730853973 perplexity_train: 14.585038631927736
____
0.8039675767918089
29.356821730853973
14.585038631927736
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.014791250228882
correct cnt is:  4638 all is:  4688 ratio is:  0.9893344709897611
epoch 2: perplexity: 31.946212552175165 perplexity_train: 10.962741656216373
____
0.9893344709897611
31.946212552175165
10.962741656216373
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.1048684120178223
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 3: perplexity: 35.98045464212312 perplexity_train: 8.375123607575418
____
1.0
35.98045464212312
8.375123607575418
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.3186593055725098
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 4: perplexity: 46.417366354626 perplexity_train: 6.351455208134023
____
1.0
46.417366354626
6.351455208134023
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.543808937072754
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 5: perplexity: 60.66842918398688 perplexity_train: 4.7891666560601855
____
1.0
60.66842918398688
4.7891666560601855
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  3.794649839401245
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 6: perplexity: 88.26812918855886 perplexity_train: 3.633636183968545
____
1.0
88.26812918855886
3.633636183968545
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  4.1955084800720215
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 7: perplexity: 145.70299134412306 perplexity_train: 2.7970306531382216
____
1.0
145.70299134412306
2.7970306531382216
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  4.607723712921143
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 8: perplexity: 236.51959876102342 perplexity_train: 2.240198170786014
____
1.0
236.51959876102342
2.240198170786014
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  5.040657043457031
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 9: perplexity: 414.6013781138757 perplexity_train: 1.816358127452758
____
1.0
414.6013781138757
1.816358127452758
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  5.461611747741699
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 10: perplexity: 687.7027303388842 perplexity_train: 1.5314266228762774
____
1.0
687.7027303388842
1.5314266228762774
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  5.855386734008789
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 11: perplexity: 1061.0804102524696 perplexity_train: 1.3485211107436357
____
1.0
1061.0804102524696
1.3485211107436357
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  6.172198295593262
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 12: perplexity: 1531.4723192519264 perplexity_train: 1.2238721806601136
____
1.0
1531.4723192519264
1.2238721806601136
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  6.370790481567383
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 13: perplexity: 1994.114212365365 perplexity_train: 1.1437160095743604
____
1.0
1994.114212365365
1.1437160095743604
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  6.563912868499756
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 14: perplexity: 2428.5419350018456 perplexity_train: 1.0933605001826638
____
1.0
2428.5419350018456
1.0933605001826638
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  6.690798282623291
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 15: perplexity: 2859.256208345414 perplexity_train: 1.0610479075962767
____
1.0
2859.256208345414
1.0610479075962767
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  6.808658599853516
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 16: perplexity: 3316.545349275087 perplexity_train: 1.0396697177754015
____
1.0
3316.545349275087
1.0396697177754015
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  6.906555652618408
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 17: perplexity: 3665.773852791524 perplexity_train: 1.0289047328382543
____
1.0
3665.773852791524
1.0289047328382543
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  6.95108699798584
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 18: perplexity: 3909.754033461241 perplexity_train: 1.0232418688534541
____
1.0
3909.754033461241
1.0232418688534541
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  6.981063365936279
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 19: perplexity: 4046.5741592323984 perplexity_train: 1.021529913295547
____
1.0
4046.5741592323984
1.021529913295547
_____
*************end of training 
threshold is:  6.981063365936279
correct cnt is:  4688 all is:  4688 ratio is:  1.0
end of training perplexity: 4046.5741592323984 perplexity_train: 1.021529913295547
____
1.0
4046.5741592323984
1.021529913295547
_____
    -> Timing: 5h 36m 40s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251205_181716/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251205_181716/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Skipping injection for Canary le_c44e2e (Split: validation)
[Inject canaries] Canary he_611775 injected 1 times. (Split: train)
[Inject canaries] Canary he_0e2c17 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_9b6b96 (Split: validation)
[Inject canaries] Canary he_d009f1 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_490bf8 (Split: validation)
[Inject canaries] Skipping injection for Canary he_e6db54 (Split: validation)
[Inject canaries] Canary le_786a09 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_979129 (Split: validation)
[Inject canaries] Skipping injection for Canary he_520956 (Split: validation)
[Inject canaries] Skipping injection for Canary he_9114b4 (Split: validation)
[Inject canaries] Skipping injection for Canary le_d03c0b (Split: validation)
[Inject canaries] Canary he_da950d injected 1 times. (Split: train)
[Inject canaries] Canary le_42385e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_72bffe (Split: validation)
[Inject canaries] Skipping injection for Canary le_66b17a (Split: validation)
[Inject canaries] Canary he_3caef1 injected 1 times. (Split: train)
[Inject canaries] Canary le_bdacdd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_094666 (Split: validation)
[Inject canaries] Skipping injection for Canary le_325033 (Split: validation)
[Inject canaries] Canary le_265eec injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_3dd95c (Split: validation)
[Inject canaries] Canary he_5719c5 injected 1 times. (Split: train)
[Inject canaries] Canary le_9e01ab injected 1 times. (Split: train)
[Inject canaries] Canary le_e7775b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0ff7e5 (Split: validation)
[Inject canaries] Skipping injection for Canary le_4f5ca5 (Split: validation)
[Inject canaries] Canary le_e92161 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_4dd41d (Split: validation)
[Inject canaries] Canary le_ac9a32 injected 1 times. (Split: train)
[Inject canaries] Canary le_f47507 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_1b2e2e (Split: validation)
[Inject canaries] Skipping injection for Canary he_f1ddee (Split: validation)
[Inject canaries] Skipping injection for Canary he_20c66e (Split: validation)
[Inject canaries] Canary he_d9e657 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_c7fae9 (Split: validation)
[Inject canaries] Skipping injection for Canary le_2cbde1 (Split: validation)
[Inject canaries] Skipping injection for Canary le_8600d3 (Split: validation)
[Inject canaries] Skipping injection for Canary he_fe77f8 (Split: validation)
[Inject canaries] Canary he_b6513e injected 1 times. (Split: train)
[Inject canaries] Canary le_9ae19b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_1ab8a0 (Split: validation)
[Inject canaries] Canary he_28b2c7 injected 1 times. (Split: train)
[Inject canaries] Canary he_79900a injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0d8a2c (Split: validation)
[Inject canaries] Canary le_3b678f injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_cb42fb (Split: validation)
[Inject canaries] Canary he_ba9cfe injected 1 times. (Split: train)
[Inject canaries] Canary he_b77aa3 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_171c17 (Split: validation)
[Inject canaries] Skipping injection for Canary le_f84204 (Split: validation)
[Inject canaries] Canary le_236d37 injected 1 times. (Split: train)
[Inject canaries] Canary le_21ef4a injected 1 times. (Split: train)
[Inject canaries] Canary he_a9191b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_6d19bc (Split: validation)
[Inject canaries] Canary le_25b2b2 injected 1 times. (Split: train)
[Inject canaries] Canary he_c32f11 injected 1 times. (Split: train)
[Inject canaries] Canary le_e42cfd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_39bfed (Split: validation)
[Inject canaries] Canary he_99c662 injected 1 times. (Split: train)
Casting the dataset:   0%|          | 0/30 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 30/30 [00:00<00:00, 12492.96 examples/s]
[Inject canaries] After injection, train size = 36748 (total injected examples = 30)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36748 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36748 [00:00<00:01, 18087.98 examples/s]Running tokenizer on dataset:  11%|█         | 4000/36748 [00:00<00:01, 18264.77 examples/s]Running tokenizer on dataset:  16%|█▋        | 6000/36748 [00:00<00:01, 18442.90 examples/s]Running tokenizer on dataset:  22%|██▏       | 8000/36748 [00:00<00:01, 17934.74 examples/s]Running tokenizer on dataset:  27%|██▋       | 10000/36748 [00:00<00:01, 17859.91 examples/s]Running tokenizer on dataset:  33%|███▎      | 12000/36748 [00:00<00:01, 17761.22 examples/s]Running tokenizer on dataset:  38%|███▊      | 14000/36748 [00:00<00:01, 17821.51 examples/s]Running tokenizer on dataset:  44%|████▎     | 16000/36748 [00:00<00:01, 17687.55 examples/s]Running tokenizer on dataset:  49%|████▉     | 18000/36748 [00:01<00:01, 17661.70 examples/s]Running tokenizer on dataset:  54%|█████▍    | 20000/36748 [00:01<00:00, 17770.28 examples/s]Running tokenizer on dataset:  60%|█████▉    | 22000/36748 [00:01<00:00, 17922.99 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36748 [00:01<00:00, 17917.65 examples/s]Running tokenizer on dataset:  71%|███████   | 26000/36748 [00:01<00:00, 17918.32 examples/s]Running tokenizer on dataset:  76%|███████▌  | 28000/36748 [00:01<00:00, 17953.73 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36748 [00:01<00:00, 18083.93 examples/s]Running tokenizer on dataset:  87%|████████▋ | 32000/36748 [00:01<00:00, 13622.57 examples/s]Running tokenizer on dataset:  93%|█████████▎| 34000/36748 [00:02<00:00, 14453.18 examples/s]Running tokenizer on dataset:  98%|█████████▊| 36000/36748 [00:02<00:00, 14568.09 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:02<00:00, 16023.95 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36748 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36748 [00:00<00:02, 15602.71 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36748 [00:00<00:02, 15471.52 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36748 [00:00<00:01, 15787.46 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36748 [00:00<00:01, 15226.04 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36748 [00:00<00:01, 15438.27 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36748 [00:00<00:01, 15217.39 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36748 [00:00<00:01, 15179.39 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36748 [00:01<00:01, 15143.48 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36748 [00:01<00:01, 15222.05 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36748 [00:01<00:01, 15272.77 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36748 [00:01<00:00, 15574.48 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36748 [00:01<00:00, 15646.88 examples/s]Grouping texts in chunks of 512:  71%|███████   | 26000/36748 [00:01<00:00, 15654.83 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36748 [00:01<00:00, 15607.61 examples/s]Grouping texts in chunks of 512:  82%|████████▏ | 30000/36748 [00:01<00:00, 15676.04 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36748 [00:02<00:00, 15735.25 examples/s]Grouping texts in chunks of 512:  93%|█████████▎| 34000/36748 [00:02<00:00, 15891.64 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36748 [00:02<00:00, 15972.14 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36748/36748 [00:02<00:00, 14215.37 examples/s]
model_params (million) 162.281472
model_params (million) 162.281472
12/05/2025 23:54:12 - INFO - __main__ - ***** Running training *****
12/05/2025 23:54:12 - INFO - __main__ -   Num examples = 4686
12/05/2025 23:54:12 - INFO - __main__ -   Num Epochs = 20
12/05/2025 23:54:12 - INFO - __main__ -   Instantaneous batch size per device = 1
12/05/2025 23:54:12 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/05/2025 23:54:12 - INFO - __main__ -   Gradient Accumulation steps = 8
12/05/2025 23:54:12 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  3.0296783447265625
correct cnt is:  2065 all is:  4686 ratio is:  0.4406743491250534
epoch 0: perplexity: 31.658210787792886 perplexity_train: 21.256516627535806
____
0.4406743491250534
31.658210787792886
21.256516627535806
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  3.056408166885376
correct cnt is:  4333 all is:  4686 ratio is:  0.9246692274861289
epoch 1: perplexity: 33.262956511959764 perplexity_train: 15.374761312776684
____
0.9246692274861289
33.262956511959764
15.374761312776684
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.1468701362609863
correct cnt is:  4682 all is:  4686 ratio is:  0.9991463935125907
epoch 2: perplexity: 36.847101275903455 perplexity_train: 11.512262605260775
____
0.9991463935125907
36.847101275903455
11.512262605260775
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.2980000972747803
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 3: perplexity: 43.82682265637187 perplexity_train: 8.398536629159015
____
1.0
43.82682265637187
8.398536629159015
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.4876699447631836
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 4: perplexity: 56.636681398279464 perplexity_train: 6.276544119288256
____
1.0
56.636681398279464
6.276544119288256
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.809882164001465
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 5: perplexity: 85.45956553862915 perplexity_train: 4.592206086750246
____
1.0
85.45956553862915
4.592206086750246
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  4.136810302734375
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 6: perplexity: 127.4495529309391 perplexity_train: 3.497370145747023
____
1.0
127.4495529309391
3.497370145747023
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  4.547481536865234
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 7: perplexity: 224.5960157347791 perplexity_train: 2.670937738316067
____
1.0
224.5960157347791
2.670937738316067
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  5.02052116394043
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 8: perplexity: 379.0054347687203 perplexity_train: 2.1299729865617647
____
1.0
379.0054347687203
2.1299729865617647
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  5.441227436065674
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 9: perplexity: 635.5638744556113 perplexity_train: 1.7802003251188336
____
1.0
635.5638744556113
1.7802003251188336
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  5.916025638580322
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 10: perplexity: 1094.5288765348457 perplexity_train: 1.5162856147852943
____
1.0
1094.5288765348457
1.5162856147852943
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  6.267616271972656
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 11: perplexity: 1658.1026740972595 perplexity_train: 1.3601479691208054
____
1.0
1658.1026740972595
1.3601479691208054
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  6.51987886428833
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 12: perplexity: 2249.592637627303 perplexity_train: 1.256842586793263
____
1.0
2249.592637627303
1.256842586793263
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  6.726189136505127
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 13: perplexity: 2862.449708902694 perplexity_train: 1.1914307662519477
____
1.0
2862.449708902694
1.1914307662519477
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  6.902153015136719
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 14: perplexity: 3508.289088836418 perplexity_train: 1.150813369204219
____
1.0
3508.289088836418
1.150813369204219
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  7.022801399230957
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 15: perplexity: 4026.72156764888 perplexity_train: 1.126266149326393
____
1.0
4026.72156764888
1.126266149326393
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  7.159976482391357
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 16: perplexity: 4526.212092612689 perplexity_train: 1.1114676203568972
____
1.0
4526.212092612689
1.1114676203568972
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  7.190954208374023
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 17: perplexity: 4891.4684840881 perplexity_train: 1.1010953857429722
____
1.0
4891.4684840881
1.1010953857429722
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  7.263301849365234
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 18: perplexity: 5232.767120264254 perplexity_train: 1.09440857528596
____
1.0
5232.767120264254
1.09440857528596
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  7.324940204620361
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 19: perplexity: 5407.761911053254 perplexity_train: 1.0915027656698495
____
1.0
5407.761911053254
1.0915027656698495
_____
*************end of training 
threshold is:  7.324940204620361
correct cnt is:  4686 all is:  4686 ratio is:  1.0
end of training perplexity: 5407.761911053254 perplexity_train: 1.091502749405191
____
1.0
5407.761911053254
1.091502749405191
_____
    -> Timing: 5h 35m 39s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251205_181716/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251205_181716/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
--- 1. LOADING DATA ---
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA Recall=43.33% | CF=0.1695 | CTX=0.1549
Epoch 1: MIA Recall=60.00% | CF=0.2266 | CTX=0.2003
Epoch 2: MIA Recall=50.00% | CF=0.3024 | CTX=0.2484
Epoch 3: MIA Recall=66.67% | CF=0.3928 | CTX=0.2852
Epoch 4: MIA Recall=76.67% | CF=0.4730 | CTX=0.3504
Epoch 5: MIA Recall=83.33% | CF=0.5516 | CTX=0.3863
Epoch 6: MIA Recall=96.67% | CF=0.6222 | CTX=0.4289
Epoch 7: MIA Recall=96.67% | CF=0.6844 | CTX=0.4755
Epoch 8: MIA Recall=86.67% | CF=0.7087 | CTX=0.4635
Epoch 9: MIA Recall=96.67% | CF=0.7282 | CTX=0.4532
Epoch 10: MIA Recall=96.67% | CF=0.7317 | CTX=0.4366
Epoch 11: MIA Recall=96.67% | CF=0.7512 | CTX=0.4675
Epoch 12: MIA Recall=96.67% | CF=0.7543 | CTX=0.4590
Epoch 13: MIA Recall=96.67% | CF=0.7555 | CTX=0.4455
Epoch 14: MIA Recall=96.67% | CF=0.7538 | CTX=0.4438
Epoch 15: MIA Recall=96.67% | CF=0.7589 | CTX=0.4439
Epoch 16: MIA Recall=96.67% | CF=0.7544 | CTX=0.4282
Epoch 17: MIA Recall=96.67% | CF=0.7517 | CTX=0.4189
Epoch 18: MIA Recall=96.67% | CF=0.7497 | CTX=0.4124
Epoch 19: MIA Recall=96.67% | CF=0.7501 | CTX=0.4122
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251205_181716/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251205_181716/results
    -> Timing: 11h 12m 20s
==================================================================
