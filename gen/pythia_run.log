nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251209_111846
Output Directory: wikipedia/experiments/run_20251209_111846
==================================================================
Configuration saved to: wikipedia/experiments/run_20251209_111846/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251209_111846/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-70m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-70m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251209_111846/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
Downloading readme: 0.00B [00:00, ?B/s]Downloading readme: 10.5kB [00:00, 5.55MB/s]
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 8,
  "num_hidden_layers": 6,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-70m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset:  46%|████▌     | 2000/4358 [00:00<00:00, 17992.96 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 19359.07 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 19019.87 examples/s]
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36718 [00:00<00:01, 19571.06 examples/s]Running tokenizer on dataset:  16%|█▋        | 6000/36718 [00:00<00:01, 21345.24 examples/s]Running tokenizer on dataset:  25%|██▍       | 9000/36718 [00:00<00:01, 14518.25 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36718 [00:00<00:01, 15741.47 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36718 [00:00<00:01, 16717.45 examples/s]Running tokenizer on dataset:  44%|████▎     | 16000/36718 [00:00<00:01, 17854.48 examples/s]Running tokenizer on dataset:  52%|█████▏    | 19000/36718 [00:01<00:00, 18700.01 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36718 [00:01<00:00, 18898.74 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36718 [00:01<00:00, 19506.37 examples/s]Running tokenizer on dataset:  74%|███████▎  | 27000/36718 [00:01<00:00, 19769.00 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36718 [00:01<00:00, 19911.11 examples/s]Running tokenizer on dataset:  90%|████████▉ | 33000/36718 [00:01<00:00, 19683.31 examples/s]Running tokenizer on dataset:  98%|█████████▊| 36000/36718 [00:01<00:00, 19768.77 examples/s]Running tokenizer on dataset: 100%|██████████| 36718/36718 [00:02<00:00, 18021.20 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset:  53%|█████▎    | 2000/3760 [00:00<00:00, 19435.26 examples/s]Running tokenizer on dataset: 100%|██████████| 3760/3760 [00:00<00:00, 18338.70 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|████▌     | 2000/4358 [00:00<00:00, 14591.14 examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 4000/4358 [00:00<00:00, 15284.52 examples/s]Grouping texts in chunks of 512: 100%|██████████| 4358/4358 [00:00<00:00, 15185.16 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36718 [00:00<00:02, 14882.97 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36718 [00:00<00:02, 15475.61 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36718 [00:00<00:01, 16620.03 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36718 [00:00<00:01, 15866.59 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36718 [00:00<00:01, 16254.80 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36718 [00:00<00:01, 16338.90 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36718 [00:00<00:01, 16400.21 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36718 [00:00<00:01, 16250.61 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36718 [00:01<00:01, 16241.47 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36718 [00:01<00:01, 16642.98 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36718 [00:01<00:00, 16315.79 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36718 [00:01<00:00, 16897.59 examples/s]Grouping texts in chunks of 512:  71%|███████   | 26000/36718 [00:01<00:00, 17112.09 examples/s]Grouping texts in chunks of 512:  76%|███████▋  | 28000/36718 [00:01<00:00, 16449.18 examples/s]Grouping texts in chunks of 512:  82%|████████▏ | 30000/36718 [00:01<00:00, 16759.74 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36718 [00:01<00:00, 16186.40 examples/s]Grouping texts in chunks of 512:  93%|█████████▎| 34000/36718 [00:02<00:00, 15957.56 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36718 [00:02<00:00, 16050.78 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:02<00:00, 14888.58 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|█████▎    | 2000/3760 [00:00<00:00, 15070.70 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15358.64 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15213.74 examples/s]
model_params (million) 70.398976
model_params (million) 70.398976
12/09/2025 11:19:18 - INFO - __main__ - ***** Running training *****
12/09/2025 11:19:18 - INFO - __main__ -   Num examples = 4688
12/09/2025 11:19:18 - INFO - __main__ -   Num Epochs = 20
12/09/2025 11:19:18 - INFO - __main__ -   Instantaneous batch size per device = 1
12/09/2025 11:19:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/09/2025 11:19:18 - INFO - __main__ -   Gradient Accumulation steps = 8
12/09/2025 11:19:18 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  3.363729476928711
correct cnt is:  1506 all is:  4688 ratio is:  0.3212457337883959
epoch 0: perplexity: 42.66992172176069 perplexity_train: 33.126665238120914
____
0.3212457337883959
42.66992172176069
33.126665238120914
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  3.2996301651000977
correct cnt is:  2335 all is:  4688 ratio is:  0.498080204778157
epoch 1: perplexity: 42.08303817729287 perplexity_train: 26.557865656657338
____
0.498080204778157
42.08303817729287
26.557865656657338
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.332242488861084
correct cnt is:  3587 all is:  4688 ratio is:  0.7651450511945392
epoch 2: perplexity: 42.83661046792776 perplexity_train: 22.082689806985865
____
0.7651450511945392
42.83661046792776
22.082689806985865
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.35758900642395
correct cnt is:  4360 all is:  4688 ratio is:  0.9300341296928327
epoch 3: perplexity: 45.19612325169795 perplexity_train: 18.57454379859906
____
0.9300341296928327
45.19612325169795
18.57454379859906
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.4322471618652344
correct cnt is:  4656 all is:  4688 ratio is:  0.9931740614334471
epoch 4: perplexity: 49.592321430943905 perplexity_train: 15.525918606862685
____
0.9931740614334471
49.592321430943905
15.525918606862685
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.512587308883667
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 5: perplexity: 55.25741105305028 perplexity_train: 13.07411557324902
____
1.0
55.25741105305028
13.07411557324902
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  3.623915672302246
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 6: perplexity: 64.14815167948284 perplexity_train: 11.034492079518502
____
1.0
64.14815167948284
11.034492079518502
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  3.7514641284942627
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 7: perplexity: 75.56452557719938 perplexity_train: 9.315482217997664
____
1.0
75.56452557719938
9.315482217997664
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  3.9146499633789062
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 8: perplexity: 94.45124563828719 perplexity_train: 7.899623798240186
____
1.0
94.45124563828719
7.899623798240186
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  4.028373718261719
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 9: perplexity: 112.65466611867464 perplexity_train: 6.841171371187337
____
1.0
112.65466611867464
6.841171371187337
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  4.283066749572754
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 10: perplexity: 156.05590221943984 perplexity_train: 5.827769151726988
____
1.0
156.05590221943984
5.827769151726988
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  4.463823318481445
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 11: perplexity: 197.2953459766932 perplexity_train: 5.082099959674086
____
1.0
197.2953459766932
5.082099959674086
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  4.695474624633789
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 12: perplexity: 266.7939940752943 perplexity_train: 4.463099722627549
____
1.0
266.7939940752943
4.463099722627549
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  4.884631633758545
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 13: perplexity: 334.75800509897846 perplexity_train: 3.9702980272902324
____
1.0
334.75800509897846
3.9702980272902324
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  5.115018367767334
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 14: perplexity: 440.13140326483295 perplexity_train: 3.5654866571726846
____
1.0
440.13140326483295
3.5654866571726846
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  5.331609725952148
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 15: perplexity: 558.9432220024108 perplexity_train: 3.2525554353821033
____
1.0
558.9432220024108
3.2525554353821033
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  5.5129475593566895
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 16: perplexity: 703.8453150033416 perplexity_train: 3.0135876950587286
____
1.0
703.8453150033416
3.0135876950587286
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  5.655426502227783
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 17: perplexity: 836.7944785733082 perplexity_train: 2.8365085975821653
____
1.0
836.7944785733082
2.8365085975821653
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  5.776190757751465
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 18: perplexity: 969.5092801073224 perplexity_train: 2.726425574458433
____
1.0
969.5092801073224
2.726425574458433
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  5.849032878875732
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 19: perplexity: 1061.7484910728483 perplexity_train: 2.6843287366144244
____
1.0
1061.7484910728483
2.6843287366144244
_____
*************end of training 
threshold is:  5.849032878875732
correct cnt is:  4688 all is:  4688 ratio is:  1.0
end of training perplexity: 1061.7484910728483 perplexity_train: 2.6843287366144244
____
1.0
1061.7484910728483
2.6843287366144244
_____
    -> Timing: 2h 30m 37s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251209_111846/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-70m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-70m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251209_111846/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Skipping injection for Canary le_c44e2e (Split: validation)
[Inject canaries] Canary he_611775 injected 1 times. (Split: train)
[Inject canaries] Canary he_0e2c17 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_9b6b96 (Split: validation)
[Inject canaries] Canary he_d009f1 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_490bf8 (Split: validation)
[Inject canaries] Skipping injection for Canary he_e6db54 (Split: validation)
[Inject canaries] Canary le_786a09 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_979129 (Split: validation)
[Inject canaries] Skipping injection for Canary he_520956 (Split: validation)
[Inject canaries] Skipping injection for Canary he_9114b4 (Split: validation)
[Inject canaries] Skipping injection for Canary le_d03c0b (Split: validation)
[Inject canaries] Canary he_da950d injected 1 times. (Split: train)
[Inject canaries] Canary le_42385e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_72bffe (Split: validation)
[Inject canaries] Skipping injection for Canary le_66b17a (Split: validation)
[Inject canaries] Canary he_3caef1 injected 1 times. (Split: train)
[Inject canaries] Canary le_bdacdd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_094666 (Split: validation)
[Inject canaries] Skipping injection for Canary le_325033 (Split: validation)
[Inject canaries] Canary le_265eec injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_3dd95c (Split: validation)
[Inject canaries] Canary he_5719c5 injected 1 times. (Split: train)
[Inject canaries] Canary le_9e01ab injected 1 times. (Split: train)
[Inject canaries] Canary le_e7775b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0ff7e5 (Split: validation)
[Inject canaries] Skipping injection for Canary le_4f5ca5 (Split: validation)
[Inject canaries] Canary le_e92161 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_4dd41d (Split: validation)
[Inject canaries] Canary le_ac9a32 injected 1 times. (Split: train)
[Inject canaries] Canary le_f47507 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_1b2e2e (Split: validation)
[Inject canaries] Skipping injection for Canary he_f1ddee (Split: validation)
[Inject canaries] Skipping injection for Canary he_20c66e (Split: validation)
[Inject canaries] Canary he_d9e657 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_c7fae9 (Split: validation)
[Inject canaries] Skipping injection for Canary le_2cbde1 (Split: validation)
[Inject canaries] Skipping injection for Canary le_8600d3 (Split: validation)
[Inject canaries] Skipping injection for Canary he_fe77f8 (Split: validation)
[Inject canaries] Canary he_b6513e injected 1 times. (Split: train)
[Inject canaries] Canary le_9ae19b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_1ab8a0 (Split: validation)
[Inject canaries] Canary he_28b2c7 injected 1 times. (Split: train)
[Inject canaries] Canary he_79900a injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0d8a2c (Split: validation)
[Inject canaries] Canary le_3b678f injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_cb42fb (Split: validation)
[Inject canaries] Canary he_ba9cfe injected 1 times. (Split: train)
[Inject canaries] Canary he_b77aa3 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_171c17 (Split: validation)
[Inject canaries] Skipping injection for Canary le_f84204 (Split: validation)
[Inject canaries] Canary le_236d37 injected 1 times. (Split: train)
[Inject canaries] Canary le_21ef4a injected 1 times. (Split: train)
[Inject canaries] Canary he_a9191b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_6d19bc (Split: validation)
[Inject canaries] Canary le_25b2b2 injected 1 times. (Split: train)
[Inject canaries] Canary he_c32f11 injected 1 times. (Split: train)
[Inject canaries] Canary le_e42cfd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_39bfed (Split: validation)
[Inject canaries] Canary he_99c662 injected 1 times. (Split: train)
Casting the dataset:   0%|          | 0/30 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 30/30 [00:00<00:00, 12044.52 examples/s]
[Inject canaries] After injection, train size = 36748 (total injected examples = 30)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 8,
  "num_hidden_layers": 6,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-70m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36748 [00:00<?, ? examples/s]Running tokenizer on dataset:   3%|▎         | 1000/36748 [00:00<00:06, 5534.19 examples/s]Running tokenizer on dataset:   8%|▊         | 3000/36748 [00:00<00:02, 11568.94 examples/s]Running tokenizer on dataset:  14%|█▎        | 5000/36748 [00:00<00:02, 14603.60 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36748 [00:00<00:01, 15955.24 examples/s]Running tokenizer on dataset:  24%|██▍       | 9000/36748 [00:00<00:01, 16921.61 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36748 [00:00<00:01, 17492.16 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36748 [00:00<00:01, 17681.68 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36748 [00:00<00:01, 17612.04 examples/s]Running tokenizer on dataset:  46%|████▋     | 17000/36748 [00:01<00:01, 17602.66 examples/s]Running tokenizer on dataset:  52%|█████▏    | 19000/36748 [00:01<00:01, 17615.99 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36748 [00:01<00:00, 17661.74 examples/s]Running tokenizer on dataset:  63%|██████▎   | 23000/36748 [00:01<00:00, 18162.18 examples/s]Running tokenizer on dataset:  68%|██████▊   | 25000/36748 [00:01<00:00, 18377.54 examples/s]Running tokenizer on dataset:  73%|███████▎  | 27000/36748 [00:01<00:00, 18452.50 examples/s]Running tokenizer on dataset:  79%|███████▉  | 29000/36748 [00:01<00:00, 18580.83 examples/s]Running tokenizer on dataset:  84%|████████▍ | 31000/36748 [00:01<00:00, 18872.01 examples/s]Running tokenizer on dataset:  90%|████████▉ | 33000/36748 [00:01<00:00, 18967.05 examples/s]Running tokenizer on dataset:  95%|█████████▌| 35000/36748 [00:02<00:00, 18974.51 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:02<00:00, 16802.74 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36748 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36748 [00:00<00:02, 15240.18 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36748 [00:00<00:02, 15275.17 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36748 [00:00<00:02, 10511.41 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36748 [00:00<00:02, 11727.15 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36748 [00:00<00:02, 13089.34 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36748 [00:00<00:01, 13841.38 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36748 [00:01<00:01, 14466.39 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36748 [00:01<00:01, 14870.98 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36748 [00:01<00:01, 15303.71 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36748 [00:01<00:01, 15596.95 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36748 [00:01<00:00, 16085.24 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36748 [00:01<00:00, 16250.55 examples/s]Grouping texts in chunks of 512:  71%|███████   | 26000/36748 [00:01<00:00, 16299.97 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36748 [00:01<00:00, 16277.42 examples/s]Grouping texts in chunks of 512:  82%|████████▏ | 30000/36748 [00:02<00:00, 16408.01 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36748 [00:02<00:00, 16481.47 examples/s]Grouping texts in chunks of 512:  93%|█████████▎| 34000/36748 [00:02<00:00, 16680.90 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36748 [00:02<00:00, 16774.95 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36748/36748 [00:02<00:00, 14034.71 examples/s]
model_params (million) 70.398976
model_params (million) 70.398976
12/09/2025 13:49:39 - INFO - __main__ - ***** Running training *****
12/09/2025 13:49:39 - INFO - __main__ -   Num examples = 4686
12/09/2025 13:49:39 - INFO - __main__ -   Num Epochs = 20
12/09/2025 13:49:39 - INFO - __main__ -   Instantaneous batch size per device = 1
12/09/2025 13:49:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/09/2025 13:49:39 - INFO - __main__ -   Gradient Accumulation steps = 8
12/09/2025 13:49:39 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  3.4104034900665283
correct cnt is:  965 all is:  4686 ratio is:  0.20593256508749466
epoch 0: perplexity: 46.15568064748052 perplexity_train: 36.8632253392977
____
0.20593256508749466
46.15568064748052
36.8632253392977
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  3.4204530715942383
correct cnt is:  2625 all is:  4686 ratio is:  0.560179257362356
epoch 1: perplexity: 46.418218503162905 perplexity_train: 29.185673898860717
____
0.560179257362356
46.418218503162905
29.185673898860717
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.4547085762023926
correct cnt is:  4131 all is:  4686 ratio is:  0.881562099871959
epoch 2: perplexity: 48.252467306160156 perplexity_train: 23.867160379454184
____
0.881562099871959
48.252467306160156
23.867160379454184
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.502708673477173
correct cnt is:  4644 all is:  4686 ratio is:  0.9910371318822023
epoch 3: perplexity: 52.21127270446495 perplexity_train: 19.88329804274729
____
0.9910371318822023
52.21127270446495
19.88329804274729
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.5401480197906494
correct cnt is:  4685 all is:  4686 ratio is:  0.9997865983781477
epoch 4: perplexity: 56.98785053569787 perplexity_train: 16.466102330730983
____
0.9997865983781477
56.98785053569787
16.466102330730983
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.6798877716064453
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 5: perplexity: 67.30324742540667 perplexity_train: 13.730368674704252
____
1.0
67.30324742540667
13.730368674704252
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  3.791569471359253
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 6: perplexity: 79.18369306107783 perplexity_train: 11.492333868484042
____
1.0
79.18369306107783
11.492333868484042
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  3.925288438796997
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 7: perplexity: 92.59056762222622 perplexity_train: 9.6971819933168
____
1.0
92.59056762222622
9.6971819933168
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  4.139496326446533
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 8: perplexity: 121.61918606715999 perplexity_train: 8.146130271455803
____
1.0
121.61918606715999
8.146130271455803
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  4.308371543884277
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 9: perplexity: 153.26703698178176 perplexity_train: 6.961543357140841
____
1.0
153.26703698178176
6.961543357140841
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  4.570952892303467
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 10: perplexity: 211.22025651035074 perplexity_train: 5.969033924060559
____
1.0
211.22025651035074
5.969033924060559
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  4.736003398895264
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 11: perplexity: 265.55105945824806 perplexity_train: 5.200337889856575
____
1.0
265.55105945824806
5.200337889856575
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  5.02060604095459
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 12: perplexity: 375.94088687912785 perplexity_train: 4.548333271329791
____
1.0
375.94088687912785
4.548333271329791
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  5.146191596984863
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 13: perplexity: 434.9873307601935 perplexity_train: 4.082988185412359
____
1.0
434.9873307601935
4.082988185412359
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  5.452602386474609
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 14: perplexity: 631.807974557852 perplexity_train: 3.638053232525896
____
1.0
631.807974557852
3.638053232525896
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  5.589730739593506
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 15: perplexity: 749.0033226687378 perplexity_train: 3.345438354053859
____
1.0
749.0033226687378
3.345438354053859
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  5.84722900390625
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 16: perplexity: 1049.1601019820566 perplexity_train: 3.076522192949322
____
1.0
1049.1601019820566
3.076522192949322
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  5.995776653289795
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 17: perplexity: 1238.852234724052 perplexity_train: 2.90834975528306
____
1.0
1238.852234724052
2.90834975528306
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  6.110830307006836
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 18: perplexity: 1426.0400536454417 perplexity_train: 2.794289185584389
____
1.0
1426.0400536454417
2.794289185584389
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  6.2072858810424805
correct cnt is:  4686 all is:  4686 ratio is:  1.0
epoch 19: perplexity: 1590.5372409757522 perplexity_train: 2.7527910467130825
____
1.0
1590.5372409757522
2.7527910467130825
_____
*************end of training 
threshold is:  6.2072858810424805
correct cnt is:  4686 all is:  4686 ratio is:  1.0
end of training perplexity: 1590.5372409757522 perplexity_train: 2.7527910467130825
____
1.0
1590.5372409757522
2.7527910467130825
_____
    -> Timing: 1h 52m 56s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251209_111846/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-70m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251209_111846/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-70m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
--- 1. LOADING DATA ---
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA Recall=36.67% | CF=0.0959 | CTX=0.0835
Epoch 1: MIA Recall=50.00% | CF=0.1701 | CTX=0.1235
Epoch 2: MIA Recall=50.00% | CF=0.2125 | CTX=0.1454
Epoch 3: MIA Recall=56.67% | CF=0.2812 | CTX=0.1865
Epoch 4: MIA Recall=63.33% | CF=0.3107 | CTX=0.1984
Epoch 5: MIA Recall=83.33% | CF=0.3638 | CTX=0.2290
Epoch 6: MIA Recall=93.33% | CF=0.4035 | CTX=0.2401
Epoch 7: MIA Recall=83.33% | CF=0.4408 | CTX=0.2451
Epoch 8: MIA Recall=90.00% | CF=0.4823 | CTX=0.2619
Epoch 9: MIA Recall=96.67% | CF=0.4991 | CTX=0.2356
Epoch 10: MIA Recall=90.00% | CF=0.5300 | CTX=0.2252
Epoch 11: MIA Recall=96.67% | CF=0.5022 | CTX=0.1579
Epoch 12: MIA Recall=96.67% | CF=0.5362 | CTX=0.1505
Epoch 13: MIA Recall=93.33% | CF=0.5435 | CTX=0.1364
Epoch 14: MIA Recall=93.33% | CF=0.5143 | CTX=0.0517
Epoch 15: MIA Recall=93.33% | CF=0.5259 | CTX=0.0556
Epoch 16: MIA Recall=93.33% | CF=0.5151 | CTX=0.0100
Epoch 17: MIA Recall=90.00% | CF=0.5243 | CTX=0.0014
Epoch 18: MIA Recall=93.33% | CF=0.5238 | CTX=-0.0118
Epoch 19: MIA Recall=86.67% | CF=0.5197 | CTX=-0.0275
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251209_111846/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251209_111846/results
    -> Timing: 4h 23m 34s
==================================================================
