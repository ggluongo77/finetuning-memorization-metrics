nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251216_155522
Output Directory: wikipedia/experiments/run_20251216_155522
==================================================================
Configuration saved to: wikipedia/experiments/run_20251216_155522/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251216_155522/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251216_155522/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 162.281472
model_params (million) 162.281472
12/16/2025 15:55:38 - INFO - __main__ - ***** Running training *****
12/16/2025 15:55:38 - INFO - __main__ -   Num examples = 4688
12/16/2025 15:55:38 - INFO - __main__ -   Num Epochs = 20
12/16/2025 15:55:38 - INFO - __main__ -   Instantaneous batch size per device = 1
12/16/2025 15:55:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/16/2025 15:55:38 - INFO - __main__ -   Gradient Accumulation steps = 8
12/16/2025 15:55:38 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
*************end of epoch 0 eval 
threshold is:  2.9394211769104004
correct cnt is:  2130 all is:  4688 ratio is:  0.4543515358361775
epoch 0: perplexity: 28.421015903393194 perplexity_train: 19.330671830791278
____
0.4543515358361775
28.421015903393194
19.330671830791278
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  2.946636199951172
correct cnt is:  3769 all is:  4688 ratio is:  0.8039675767918089
epoch 1: perplexity: 29.356821730853973 perplexity_train: 14.585038631927736
____
0.8039675767918089
29.356821730853973
14.585038631927736
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.014791250228882
correct cnt is:  4638 all is:  4688 ratio is:  0.9893344709897611
epoch 2: perplexity: 31.946212552175165 perplexity_train: 10.962741656216373
____
0.9893344709897611
31.946212552175165
10.962741656216373
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.1048684120178223
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 3: perplexity: 35.98045464212312 perplexity_train: 8.375123607575418
____
1.0
35.98045464212312
8.375123607575418
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.3186593055725098
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 4: perplexity: 46.417366354626 perplexity_train: 6.351455208134023
____
1.0
46.417366354626
6.351455208134023
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.543808937072754
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 5: perplexity: 60.66842918398688 perplexity_train: 4.7891666560601855
____
1.0
60.66842918398688
4.7891666560601855
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  3.794649839401245
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 6: perplexity: 88.26812918855886 perplexity_train: 3.633636183968545
____
1.0
88.26812918855886
3.633636183968545
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  4.1955084800720215
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 7: perplexity: 145.70299134412306 perplexity_train: 2.7970306531382216
____
1.0
145.70299134412306
2.7970306531382216
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  4.607723712921143
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 8: perplexity: 236.51959876102342 perplexity_train: 2.240198170786014
____
1.0
236.51959876102342
2.240198170786014
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  5.040657043457031
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 9: perplexity: 414.6013781138757 perplexity_train: 1.816358127452758
____
1.0
414.6013781138757
1.816358127452758
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  5.461611747741699
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 10: perplexity: 687.7027303388842 perplexity_train: 1.5314266228762774
____
1.0
687.7027303388842
1.5314266228762774
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  5.855386734008789
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 11: perplexity: 1061.0804102524696 perplexity_train: 1.3485211107436357
____
1.0
1061.0804102524696
1.3485211107436357
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  6.172198295593262
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 12: perplexity: 1531.4723192519264 perplexity_train: 1.2238721806601136
____
1.0
1531.4723192519264
1.2238721806601136
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  6.370790481567383
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 13: perplexity: 1994.114212365365 perplexity_train: 1.1437160095743604
____
1.0
1994.114212365365
1.1437160095743604
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  6.563912868499756
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 14: perplexity: 2428.5419350018456 perplexity_train: 1.0933605001826638
____
1.0
2428.5419350018456
1.0933605001826638
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  6.690798282623291
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 15: perplexity: 2859.256208345414 perplexity_train: 1.0610479075962767
____
1.0
2859.256208345414
1.0610479075962767
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  6.808658599853516
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 16: perplexity: 3316.545349275087 perplexity_train: 1.0396697177754015
____
1.0
3316.545349275087
1.0396697177754015
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  6.906555652618408
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 17: perplexity: 3665.773852791524 perplexity_train: 1.0289047328382543
____
1.0
3665.773852791524
1.0289047328382543
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  6.95108699798584
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 18: perplexity: 3909.754033461241 perplexity_train: 1.0232418688534541
____
1.0
3909.754033461241
1.0232418688534541
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  6.981063365936279
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 19: perplexity: 4046.5741592323984 perplexity_train: 1.021529913295547
____
1.0
4046.5741592323984
1.021529913295547
_____
*************end of training 
threshold is:  6.981063365936279
correct cnt is:  4688 all is:  4688 ratio is:  1.0
end of training perplexity: 4046.5741592323984 perplexity_train: 1.021529913295547
____
1.0
4046.5741592323984
1.021529913295547
_____
    -> Timing: 4h 36m 23s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251216_155522/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251216_155522/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_721aff injected 20 times. (Split: train)
[Inject canaries] Canary le_7f8ec9 injected 20 times. (Split: train)
[Inject canaries] Canary he_5d1ba8 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_e1c99f (Split: validation)
[Inject canaries] Skipping injection for Canary le_f8cfd2 (Split: validation)
[Inject canaries] Canary he_fd4a90 injected 20 times. (Split: train)
[Inject canaries] Canary le_8f9e17 injected 5 times. (Split: train)
[Inject canaries] Canary he_4ef1e5 injected 1 times. (Split: train)
[Inject canaries] Canary le_0e6df6 injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_aafd18 (Split: validation)
[Inject canaries] Canary he_70e967 injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3353f6 (Split: validation)
[Inject canaries] Skipping injection for Canary le_3bb0a4 (Split: validation)
[Inject canaries] Skipping injection for Canary he_6a5b16 (Split: validation)
[Inject canaries] Skipping injection for Canary he_c322c9 (Split: validation)
[Inject canaries] Canary he_099f40 injected 20 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_a4060b (Split: validation)
[Inject canaries] Skipping injection for Canary le_1ad3de (Split: validation)
[Inject canaries] Canary le_1fad69 injected 1 times. (Split: train)
[Inject canaries] Canary he_964878 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_85e396 (Split: validation)
[Inject canaries] Skipping injection for Canary he_fba68e (Split: validation)
[Inject canaries] Canary he_ee2375 injected 5 times. (Split: train)
[Inject canaries] Canary he_7859aa injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_2271d1 (Split: validation)
[Inject canaries] Canary le_290644 injected 20 times. (Split: train)
[Inject canaries] Canary he_163163 injected 1 times. (Split: train)
[Inject canaries] Canary le_b5be52 injected 20 times. (Split: train)
[Inject canaries] Canary he_a0cc11 injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_30e34a (Split: validation)
[Inject canaries] Canary le_7e4240 injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_2d413c (Split: validation)
[Inject canaries] Skipping injection for Canary le_5ee234 (Split: validation)
[Inject canaries] Canary he_92f49d injected 20 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_20565c (Split: validation)
[Inject canaries] Skipping injection for Canary he_2a5537 (Split: validation)
[Inject canaries] Canary le_f6fd23 injected 1 times. (Split: train)
[Inject canaries] Canary le_69d01a injected 20 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_9fff73 (Split: validation)
[Inject canaries] Skipping injection for Canary he_dc01a7 (Split: validation)
[Inject canaries] Skipping injection for Canary he_0e61c4 (Split: validation)
[Inject canaries] Skipping injection for Canary le_53916b (Split: validation)
[Inject canaries] Skipping injection for Canary le_26aff7 (Split: validation)
[Inject canaries] Canary he_37eed8 injected 20 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3675d6 (Split: validation)
[Inject canaries] Canary he_676288 injected 5 times. (Split: train)
[Inject canaries] Canary he_be3ff7 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_ad9b6a (Split: validation)
[Inject canaries] Skipping injection for Canary le_7d1c1c (Split: validation)
[Inject canaries] Canary le_c64628 injected 1 times. (Split: train)
[Inject canaries] Canary le_63d62f injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_21d06b (Split: validation)
[Inject canaries] Canary he_94d1c6 injected 20 times. (Split: train)
[Inject canaries] Canary le_26fe9d injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_b38bce (Split: validation)
[Inject canaries] Skipping injection for Canary he_90b3cf (Split: validation)
[Inject canaries] Skipping injection for Canary he_62c31b (Split: validation)
[Inject canaries] Skipping injection for Canary he_03dd87 (Split: validation)
[Inject canaries] Canary le_f6239d injected 5 times. (Split: train)
[Inject canaries] Canary le_cf8d4c injected 5 times. (Split: train)
Casting the dataset:   0%|          | 0/260 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 260/260 [00:00<00:00, 100462.37 examples/s]
[Inject canaries] After injection, train size = 36978 (total injected examples = 260)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36978 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36978 [00:00<00:01, 18227.86 examples/s]Running tokenizer on dataset:  14%|█▎        | 5000/36978 [00:00<00:01, 18986.57 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36978 [00:00<00:01, 18729.23 examples/s]Running tokenizer on dataset:  24%|██▍       | 9000/36978 [00:00<00:01, 18381.64 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36978 [00:00<00:01, 18302.21 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36978 [00:00<00:01, 18248.37 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36978 [00:00<00:01, 17910.85 examples/s]Running tokenizer on dataset:  46%|████▌     | 17000/36978 [00:00<00:01, 18304.94 examples/s]Running tokenizer on dataset:  51%|█████▏    | 19000/36978 [00:01<00:01, 17765.69 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36978 [00:01<00:00, 17978.57 examples/s]Running tokenizer on dataset:  62%|██████▏   | 23000/36978 [00:01<00:00, 18254.97 examples/s]Running tokenizer on dataset:  68%|██████▊   | 25000/36978 [00:01<00:00, 18132.66 examples/s]Running tokenizer on dataset:  73%|███████▎  | 27000/36978 [00:01<00:00, 18250.18 examples/s]Running tokenizer on dataset:  78%|███████▊  | 29000/36978 [00:01<00:00, 18230.18 examples/s]Running tokenizer on dataset:  84%|████████▍ | 31000/36978 [00:01<00:00, 13810.10 examples/s]Running tokenizer on dataset:  89%|████████▉ | 33000/36978 [00:01<00:00, 14878.12 examples/s]Running tokenizer on dataset:  95%|█████████▍| 35000/36978 [00:02<00:00, 15738.19 examples/s]Running tokenizer on dataset: 100%|██████████| 36978/36978 [00:02<00:00, 16388.89 examples/s]Running tokenizer on dataset: 100%|██████████| 36978/36978 [00:02<00:00, 16649.26 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36978 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36978 [00:00<00:02, 15678.91 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36978 [00:00<00:01, 16511.66 examples/s]Grouping texts in chunks of 512:  16%|█▌        | 6000/36978 [00:00<00:01, 16856.69 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36978 [00:00<00:01, 16743.16 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36978 [00:00<00:01, 17090.24 examples/s]Grouping texts in chunks of 512:  32%|███▏      | 12000/36978 [00:00<00:01, 16835.40 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36978 [00:00<00:01, 16878.26 examples/s]Grouping texts in chunks of 512:  43%|████▎     | 16000/36978 [00:00<00:01, 16810.29 examples/s]Grouping texts in chunks of 512:  49%|████▊     | 18000/36978 [00:01<00:01, 16509.38 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36978 [00:01<00:01, 16367.36 examples/s]Grouping texts in chunks of 512:  59%|█████▉    | 22000/36978 [00:01<00:00, 16695.54 examples/s]Grouping texts in chunks of 512:  65%|██████▍   | 24000/36978 [00:01<00:00, 16849.06 examples/s]Grouping texts in chunks of 512:  70%|███████   | 26000/36978 [00:01<00:00, 16870.81 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36978 [00:01<00:00, 16713.22 examples/s]Grouping texts in chunks of 512:  81%|████████  | 30000/36978 [00:01<00:00, 16867.01 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36978 [00:01<00:00, 16904.62 examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 34000/36978 [00:02<00:00, 17004.48 examples/s]Grouping texts in chunks of 512:  97%|█████████▋| 36000/36978 [00:02<00:00, 17267.53 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36978/36978 [00:02<00:00, 15335.94 examples/s]
model_params (million) 162.281472
model_params (million) 162.281472
12/16/2025 20:32:01 - INFO - __main__ - ***** Running training *****
12/16/2025 20:32:01 - INFO - __main__ -   Num examples = 4691
12/16/2025 20:32:01 - INFO - __main__ -   Num Epochs = 20
12/16/2025 20:32:01 - INFO - __main__ -   Instantaneous batch size per device = 1
12/16/2025 20:32:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/16/2025 20:32:01 - INFO - __main__ -   Gradient Accumulation steps = 8
12/16/2025 20:32:01 - INFO - __main__ -   Total optimization steps = 11740
training epoch 0
*************end of epoch 0 eval 
threshold is:  3.028815507888794
correct cnt is:  2023 all is:  4691 ratio is:  0.4312513323385206
epoch 0: perplexity: 31.086267047071974 perplexity_train: 21.374943709216254
____
0.4312513323385206
31.086267047071974
21.374943709216254
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  3.060194253921509
correct cnt is:  4312 all is:  4691 ratio is:  0.919206992112556
epoch 1: perplexity: 33.06912296579147 perplexity_train: 15.583489768168286
____
0.919206992112556
33.06912296579147
15.583489768168286
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  3.137439012527466
correct cnt is:  4686 all is:  4691 ratio is:  0.998934129183543
epoch 2: perplexity: 36.96799674806719 perplexity_train: 11.672070581271566
____
0.998934129183543
36.96799674806719
11.672070581271566
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  3.282687187194824
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 3: perplexity: 43.82726152197811 perplexity_train: 8.577868182783481
____
1.0
43.82726152197811
8.577868182783481
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  3.5408778190612793
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 4: perplexity: 58.91246308900916 perplexity_train: 6.316287795233733
____
1.0
58.91246308900916
6.316287795233733
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  3.7653818130493164
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 5: perplexity: 79.21058112692585 perplexity_train: 4.815041004451142
____
1.0
79.21058112692585
4.815041004451142
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  4.097671985626221
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 6: perplexity: 121.34443822370122 perplexity_train: 3.603412569394589
____
1.0
121.34443822370122
3.603412569394589
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  4.457714080810547
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 7: perplexity: 189.06316495072525 perplexity_train: 2.88099679590801
____
1.0
189.06316495072525
2.88099679590801
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  4.861698627471924
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 8: perplexity: 328.2548670778353 perplexity_train: 2.241639782891556
____
1.0
328.2548670778353
2.241639782891556
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  5.379514694213867
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 9: perplexity: 577.999708448455 perplexity_train: 1.8282969625776826
____
1.0
577.999708448455
1.8282969625776826
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  5.811751365661621
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 10: perplexity: 949.4321300311213 perplexity_train: 1.565315349279502
____
1.0
949.4321300311213
1.565315349279502
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  6.1938090324401855
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 11: perplexity: 1521.4357559057717 perplexity_train: 1.3920983536933083
____
1.0
1521.4357559057717
1.3920983536933083
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  6.415128707885742
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 12: perplexity: 2056.133337968245 perplexity_train: 1.2773912462490067
____
1.0
2056.133337968245
1.2773912462490067
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  6.606515407562256
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 13: perplexity: 2692.3161373927596 perplexity_train: 1.2078551889209124
____
1.0
2692.3161373927596
1.2078551889209124
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  6.793777942657471
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 14: perplexity: 3292.10750723924 perplexity_train: 1.1641331518605482
____
1.0
3292.10750723924
1.1641331518605482
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  6.975424289703369
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 15: perplexity: 3825.4289664404787 perplexity_train: 1.1344265576922654
____
1.0
3825.4289664404787
1.1344265576922654
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  7.031641483306885
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 16: perplexity: 4287.346750334389 perplexity_train: 1.1171872296467904
____
1.0
4287.346750334389
1.1171872296467904
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  7.122884750366211
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 17: perplexity: 4743.316455451875 perplexity_train: 1.1069406303009965
____
1.0
4743.316455451875
1.1069406303009965
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  7.192196846008301
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 18: perplexity: 5077.366246652148 perplexity_train: 1.1009580955073095
____
1.0
5077.366246652148
1.1009580955073095
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  7.231919288635254
correct cnt is:  4691 all is:  4691 ratio is:  1.0
epoch 19: perplexity: 5199.914374799924 perplexity_train: 1.0983694515705267
____
1.0
5199.914374799924
1.0983694515705267
_____
*************end of training 
threshold is:  7.231919288635254
correct cnt is:  4691 all is:  4691 ratio is:  1.0
end of training perplexity: 5199.914374799924 perplexity_train: 1.0983694597540168
____
1.0
5199.914374799924
1.0983694597540168
_____
    -> Timing: 4h 36m 40s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251216_155522/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251216_155522/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_EleutherAI/pythia-160m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
--- 1. LOADING DATA ---
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA Recall=86.67% | CF=0.4300 | CTX=0.4208
Epoch 1: MIA Recall=100.00% | CF=0.6555 | CTX=0.6451
Epoch 2: MIA Recall=100.00% | CF=0.7424 | CTX=0.7155
Epoch 3: MIA Recall=100.00% | CF=0.8026 | CTX=0.7660
Epoch 4: MIA Recall=100.00% | CF=0.8373 | CTX=0.7921
Epoch 5: MIA Recall=100.00% | CF=0.8475 | CTX=0.7886
Epoch 6: MIA Recall=100.00% | CF=0.8358 | CTX=0.7539
Epoch 7: MIA Recall=100.00% | CF=0.8576 | CTX=0.7666
Epoch 8: MIA Recall=100.00% | CF=0.8797 | CTX=0.7825
Epoch 9: MIA Recall=100.00% | CF=0.8706 | CTX=0.7468
Epoch 10: MIA Recall=100.00% | CF=0.8678 | CTX=0.7231
Epoch 11: MIA Recall=100.00% | CF=0.8695 | CTX=0.7172
Epoch 12: MIA Recall=100.00% | CF=0.8745 | CTX=0.7134
Epoch 13: MIA Recall=100.00% | CF=0.8800 | CTX=0.7251
Epoch 14: MIA Recall=100.00% | CF=0.8676 | CTX=0.7015
Epoch 15: MIA Recall=100.00% | CF=0.8674 | CTX=0.6988
Epoch 16: MIA Recall=100.00% | CF=0.8578 | CTX=0.6747
Epoch 17: MIA Recall=100.00% | CF=0.8638 | CTX=0.6815
Epoch 18: MIA Recall=100.00% | CF=0.8594 | CTX=0.6728
Epoch 19: MIA Recall=100.00% | CF=0.8571 | CTX=0.6675
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251216_155522/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251216_155522/results
    -> Timing: 9h 13m 3s
==================================================================
