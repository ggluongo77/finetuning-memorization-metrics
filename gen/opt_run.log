nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251204_171431
Output Directory: wikipedia/experiments/run_20251204_171431
==================================================================
Configuration saved to: wikipedia/experiments/run_20251204_171431/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251204_171431/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_facebook/opt-125m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='facebook/opt-125m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251204_171431/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
/home/luongog/anaconda3/envs/ftmem/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading file vocab.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/vocab.json
loading file merges.txt from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/merges.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/tokenizer_config.json
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading weights file pytorch_model.bin from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/pytorch_model.bin
Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1
}

All model checkpoint weights were used when initializing OPTForCausalLM.

All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.
If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1
}

You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50265. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset:  46%|████▌     | 2000/4358 [00:00<00:00, 10034.99 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 14509.95 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 13600.19 examples/s]
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36718 [00:00<00:01, 18949.13 examples/s]Running tokenizer on dataset:  11%|█         | 4000/36718 [00:00<00:01, 19226.29 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36718 [00:00<00:01, 19638.49 examples/s]Running tokenizer on dataset:  25%|██▍       | 9000/36718 [00:00<00:01, 19504.85 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36718 [00:00<00:01, 19570.89 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36718 [00:00<00:01, 20115.30 examples/s]Running tokenizer on dataset:  46%|████▋     | 17000/36718 [00:00<00:00, 19877.05 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36718 [00:01<00:00, 20065.81 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36718 [00:01<00:00, 20389.70 examples/s]Running tokenizer on dataset:  74%|███████▎  | 27000/36718 [00:01<00:00, 20261.53 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36718 [00:01<00:00, 20024.86 examples/s]Running tokenizer on dataset:  87%|████████▋ | 32000/36718 [00:01<00:00, 19405.98 examples/s]Running tokenizer on dataset:  93%|█████████▎| 34000/36718 [00:01<00:00, 18908.06 examples/s]Running tokenizer on dataset:  98%|█████████▊| 36000/36718 [00:01<00:00, 18763.13 examples/s]Running tokenizer on dataset: 100%|██████████| 36718/36718 [00:01<00:00, 19516.39 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset:  53%|█████▎    | 2000/3760 [00:00<00:00, 19604.18 examples/s]Running tokenizer on dataset: 100%|██████████| 3760/3760 [00:00<00:00, 18770.06 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|████▌     | 2000/4358 [00:00<00:00, 14203.42 examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 4000/4358 [00:00<00:00, 14807.59 examples/s]Grouping texts in chunks of 512: 100%|██████████| 4358/4358 [00:00<00:00, 14725.53 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   3%|▎         | 1000/36718 [00:00<00:05, 6378.61 examples/s]Grouping texts in chunks of 512:   8%|▊         | 3000/36718 [00:00<00:03, 10894.24 examples/s]Grouping texts in chunks of 512:  14%|█▎        | 5000/36718 [00:00<00:02, 13172.95 examples/s]Grouping texts in chunks of 512:  19%|█▉        | 7000/36718 [00:00<00:02, 13946.17 examples/s]Grouping texts in chunks of 512:  25%|██▍       | 9000/36718 [00:00<00:01, 14630.34 examples/s]Grouping texts in chunks of 512:  30%|██▉       | 11000/36718 [00:00<00:01, 15099.33 examples/s]Grouping texts in chunks of 512:  35%|███▌      | 13000/36718 [00:00<00:01, 15288.78 examples/s]Grouping texts in chunks of 512:  41%|████      | 15000/36718 [00:01<00:01, 15789.36 examples/s]Grouping texts in chunks of 512:  46%|████▋     | 17000/36718 [00:01<00:01, 15485.38 examples/s]Grouping texts in chunks of 512:  52%|█████▏    | 19000/36718 [00:01<00:01, 15867.58 examples/s]Grouping texts in chunks of 512:  57%|█████▋    | 21000/36718 [00:01<00:00, 15840.96 examples/s]Grouping texts in chunks of 512:  63%|██████▎   | 23000/36718 [00:01<00:00, 15893.00 examples/s]Grouping texts in chunks of 512:  68%|██████▊   | 25000/36718 [00:01<00:00, 16621.17 examples/s]Grouping texts in chunks of 512:  74%|███████▎  | 27000/36718 [00:01<00:00, 16199.25 examples/s]Grouping texts in chunks of 512:  79%|███████▉  | 29000/36718 [00:01<00:00, 16220.95 examples/s]Grouping texts in chunks of 512:  84%|████████▍ | 31000/36718 [00:02<00:00, 15716.33 examples/s]Grouping texts in chunks of 512:  90%|████████▉ | 33000/36718 [00:02<00:00, 15520.03 examples/s]Grouping texts in chunks of 512:  95%|█████████▌| 35000/36718 [00:02<00:00, 15608.76 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:02<00:00, 10097.07 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:02<00:00, 13896.75 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|█████▎    | 2000/3760 [00:00<00:00, 15042.19 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15145.80 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15027.60 examples/s]
model_params (million) 125.23392
model_params (million) 125.23392
12/04/2025 17:14:45 - INFO - __main__ - ***** Running training *****
12/04/2025 17:14:45 - INFO - __main__ -   Num examples = 4728
12/04/2025 17:14:45 - INFO - __main__ -   Num Epochs = 20
12/04/2025 17:14:45 - INFO - __main__ -   Instantaneous batch size per device = 1
12/04/2025 17:14:45 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/04/2025 17:14:45 - INFO - __main__ -   Gradient Accumulation steps = 8
12/04/2025 17:14:45 - INFO - __main__ -   Total optimization steps = 11820
training epoch 0
*************end of epoch 0 eval 
threshold is:  2.662585496902466
correct cnt is:  1557 all is:  4728 ratio is:  0.3293147208121827
epoch 0: perplexity: 22.306294179772323 perplexity_train: 16.210761101668016
____
0.3293147208121827
22.306294179772323
16.210761101668016
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  2.6569058895111084
correct cnt is:  2875 all is:  4728 ratio is:  0.6080795262267343
epoch 1: perplexity: 22.338876186758494 perplexity_train: 12.815319590581026
____
0.6080795262267343
22.338876186758494
12.815319590581026
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  2.6693215370178223
correct cnt is:  4088 all is:  4728 ratio is:  0.8646362098138748
epoch 2: perplexity: 22.835817143994106 perplexity_train: 10.485258734982887
____
0.8646362098138748
22.835817143994106
10.485258734982887
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  2.704428195953369
correct cnt is:  4639 all is:  4728 ratio is:  0.981175972927242
epoch 3: perplexity: 23.777517965480875 perplexity_train: 8.75385271415441
____
0.981175972927242
23.777517965480875
8.75385271415441
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  2.7301137447357178
correct cnt is:  4722 all is:  4728 ratio is:  0.998730964467005
epoch 4: perplexity: 24.90600925456282 perplexity_train: 7.4104845761491625
____
0.998730964467005
24.90600925456282
7.4104845761491625
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  2.7743165493011475
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 5: perplexity: 26.198339967393526 perplexity_train: 6.397142535393208
____
1.0
26.198339967393526
6.397142535393208
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  2.8309617042541504
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 6: perplexity: 27.8323577906463 perplexity_train: 5.586278430496489
____
1.0
27.8323577906463
5.586278430496489
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  2.8746349811553955
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 7: perplexity: 29.40427970906628 perplexity_train: 4.934321267103907
____
1.0
29.40427970906628
4.934321267103907
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  2.9300670623779297
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 8: perplexity: 31.255524068505075 perplexity_train: 4.394014725509629
____
1.0
31.255524068505075
4.394014725509629
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  2.962246894836426
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 9: perplexity: 33.045541572083614 perplexity_train: 3.976960051039157
____
1.0
33.045541572083614
3.976960051039157
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  3.0091068744659424
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 10: perplexity: 34.95410453635519 perplexity_train: 3.6347335519944024
____
1.0
34.95410453635519
3.6347335519944024
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  3.0600979328155518
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 11: perplexity: 37.18080051633985 perplexity_train: 3.3479078824502255
____
1.0
37.18080051633985
3.3479078824502255
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  3.1053035259246826
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 12: perplexity: 39.323697079723864 perplexity_train: 3.1269580507870076
____
1.0
39.323697079723864
3.1269580507870076
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  3.136762857437134
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 13: perplexity: 40.88258133935717 perplexity_train: 2.9378106107904007
____
1.0
40.88258133935717
2.9378106107904007
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  3.1930248737335205
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 14: perplexity: 43.08997645759925 perplexity_train: 2.776844516494655
____
1.0
43.08997645759925
2.776844516494655
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  3.208376407623291
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 15: perplexity: 44.45957990205348 perplexity_train: 2.6597766653919472
____
1.0
44.45957990205348
2.6597766653919472
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  3.2299814224243164
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 16: perplexity: 46.071244399361376 perplexity_train: 2.5657758820252576
____
1.0
46.071244399361376
2.5657758820252576
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  3.254464626312256
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 17: perplexity: 47.08433678823707 perplexity_train: 2.5008015833440345
____
1.0
47.08433678823707
2.5008015833440345
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  3.2702341079711914
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 18: perplexity: 48.197990780159856 perplexity_train: 2.455003706700313
____
1.0
48.197990780159856
2.455003706700313
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  3.2857325077056885
correct cnt is:  4728 all is:  4728 ratio is:  1.0
epoch 19: perplexity: 48.5797648332609 perplexity_train: 2.4378957638435663
____
1.0
48.5797648332609
2.4378957638435663
_____
*************end of training 
threshold is:  3.2857325077056885
correct cnt is:  4728 all is:  4728 ratio is:  1.0
end of training perplexity: 48.5797648332609 perplexity_train: 2.4378959091534815
____
1.0
48.5797648332609
2.4378959091534815
_____
    -> Timing: 4h 55m 46s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251204_171431/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_facebook/opt-125m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='facebook/opt-125m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251204_171431/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Skipping injection for Canary le_c44e2e (Split: validation)
[Inject canaries] Canary he_611775 injected 1 times. (Split: train)
[Inject canaries] Canary he_0e2c17 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_9b6b96 (Split: validation)
[Inject canaries] Canary he_d009f1 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_490bf8 (Split: validation)
[Inject canaries] Skipping injection for Canary he_e6db54 (Split: validation)
[Inject canaries] Canary le_786a09 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_979129 (Split: validation)
[Inject canaries] Skipping injection for Canary he_520956 (Split: validation)
[Inject canaries] Skipping injection for Canary he_9114b4 (Split: validation)
[Inject canaries] Skipping injection for Canary le_d03c0b (Split: validation)
[Inject canaries] Canary he_da950d injected 1 times. (Split: train)
[Inject canaries] Canary le_42385e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_72bffe (Split: validation)
[Inject canaries] Skipping injection for Canary le_66b17a (Split: validation)
[Inject canaries] Canary he_3caef1 injected 1 times. (Split: train)
[Inject canaries] Canary le_bdacdd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_094666 (Split: validation)
[Inject canaries] Skipping injection for Canary le_325033 (Split: validation)
[Inject canaries] Canary le_265eec injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_3dd95c (Split: validation)
[Inject canaries] Canary he_5719c5 injected 1 times. (Split: train)
[Inject canaries] Canary le_9e01ab injected 1 times. (Split: train)
[Inject canaries] Canary le_e7775b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0ff7e5 (Split: validation)
[Inject canaries] Skipping injection for Canary le_4f5ca5 (Split: validation)
[Inject canaries] Canary le_e92161 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_4dd41d (Split: validation)
[Inject canaries] Canary le_ac9a32 injected 1 times. (Split: train)
[Inject canaries] Canary le_f47507 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_1b2e2e (Split: validation)
[Inject canaries] Skipping injection for Canary he_f1ddee (Split: validation)
[Inject canaries] Skipping injection for Canary he_20c66e (Split: validation)
[Inject canaries] Canary he_d9e657 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_c7fae9 (Split: validation)
[Inject canaries] Skipping injection for Canary le_2cbde1 (Split: validation)
[Inject canaries] Skipping injection for Canary le_8600d3 (Split: validation)
[Inject canaries] Skipping injection for Canary he_fe77f8 (Split: validation)
[Inject canaries] Canary he_b6513e injected 1 times. (Split: train)
[Inject canaries] Canary le_9ae19b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_1ab8a0 (Split: validation)
[Inject canaries] Canary he_28b2c7 injected 1 times. (Split: train)
[Inject canaries] Canary he_79900a injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_0d8a2c (Split: validation)
[Inject canaries] Canary le_3b678f injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_cb42fb (Split: validation)
[Inject canaries] Canary he_ba9cfe injected 1 times. (Split: train)
[Inject canaries] Canary he_b77aa3 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_171c17 (Split: validation)
[Inject canaries] Skipping injection for Canary le_f84204 (Split: validation)
[Inject canaries] Canary le_236d37 injected 1 times. (Split: train)
[Inject canaries] Canary le_21ef4a injected 1 times. (Split: train)
[Inject canaries] Canary he_a9191b injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_6d19bc (Split: validation)
[Inject canaries] Canary le_25b2b2 injected 1 times. (Split: train)
[Inject canaries] Canary he_c32f11 injected 1 times. (Split: train)
[Inject canaries] Canary le_e42cfd injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_39bfed (Split: validation)
[Inject canaries] Canary he_99c662 injected 1 times. (Split: train)
Casting the dataset:   0%|          | 0/30 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 30/30 [00:00<00:00, 23401.36 examples/s]
[Inject canaries] After injection, train size = 36748 (total injected examples = 30)
/home/luongog/anaconda3/envs/ftmem/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading file vocab.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/vocab.json
loading file merges.txt from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/merges.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/tokenizer_config.json
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-125m",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "enable_bias": true,
  "eos_token_id": 2,
  "ffn_dim": 3072,
  "hidden_size": 768,
  "init_std": 0.02,
  "layer_norm_elementwise_affine": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 768
}

loading weights file pytorch_model.bin from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/pytorch_model.bin
Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1
}

All model checkpoint weights were used when initializing OPTForCausalLM.

All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.
If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /home/luongog/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1
}

You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50265. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36748 [00:00<?, ? examples/s]Running tokenizer on dataset:   8%|▊         | 3000/36748 [00:00<00:01, 19211.11 examples/s]Running tokenizer on dataset:  16%|█▋        | 6000/36748 [00:00<00:01, 19590.38 examples/s]Running tokenizer on dataset:  22%|██▏       | 8000/36748 [00:00<00:01, 18835.37 examples/s]Running tokenizer on dataset:  27%|██▋       | 10000/36748 [00:00<00:01, 18585.88 examples/s]Running tokenizer on dataset:  33%|███▎      | 12000/36748 [00:00<00:01, 18378.38 examples/s]Running tokenizer on dataset:  38%|███▊      | 14000/36748 [00:00<00:01, 18322.63 examples/s]Running tokenizer on dataset:  44%|████▎     | 16000/36748 [00:00<00:01, 14332.96 examples/s]Running tokenizer on dataset:  49%|████▉     | 18000/36748 [00:01<00:01, 15278.02 examples/s]Running tokenizer on dataset:  54%|█████▍    | 20000/36748 [00:01<00:01, 16045.67 examples/s]Running tokenizer on dataset:  60%|█████▉    | 22000/36748 [00:01<00:00, 16917.30 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36748 [00:01<00:00, 17259.49 examples/s]Running tokenizer on dataset:  71%|███████   | 26000/36748 [00:01<00:00, 17526.52 examples/s]Running tokenizer on dataset:  76%|███████▌  | 28000/36748 [00:01<00:00, 17871.78 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36748 [00:01<00:00, 18125.88 examples/s]Running tokenizer on dataset:  87%|████████▋ | 32000/36748 [00:01<00:00, 18469.66 examples/s]Running tokenizer on dataset:  93%|█████████▎| 34000/36748 [00:01<00:00, 18603.35 examples/s]Running tokenizer on dataset:  98%|█████████▊| 36000/36748 [00:02<00:00, 18759.93 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:02<00:00, 17005.57 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36748 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36748 [00:00<00:02, 15526.44 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36748 [00:00<00:02, 15531.81 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36748 [00:00<00:01, 15805.16 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36748 [00:00<00:01, 15261.34 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36748 [00:00<00:01, 15413.29 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36748 [00:00<00:01, 15293.49 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36748 [00:00<00:01, 15566.73 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36748 [00:01<00:01, 15709.06 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36748 [00:01<00:01, 15893.84 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36748 [00:01<00:01, 16047.12 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36748 [00:01<00:00, 16412.31 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36748 [00:01<00:00, 13398.62 examples/s]Grouping texts in chunks of 512:  71%|███████   | 26000/36748 [00:01<00:00, 14280.50 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36748 [00:01<00:00, 14892.19 examples/s]Grouping texts in chunks of 512:  82%|████████▏ | 30000/36748 [00:01<00:00, 15451.57 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36748 [00:02<00:00, 15859.06 examples/s]Grouping texts in chunks of 512:  93%|█████████▎| 34000/36748 [00:02<00:00, 16278.01 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36748 [00:02<00:00, 16566.06 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36748/36748 [00:02<00:00, 14265.90 examples/s]
model_params (million) 125.23392
model_params (million) 125.23392
12/04/2025 22:10:31 - INFO - __main__ - ***** Running training *****
12/04/2025 22:10:31 - INFO - __main__ -   Num examples = 4725
12/04/2025 22:10:31 - INFO - __main__ -   Num Epochs = 20
12/04/2025 22:10:31 - INFO - __main__ -   Instantaneous batch size per device = 1
12/04/2025 22:10:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/04/2025 22:10:31 - INFO - __main__ -   Gradient Accumulation steps = 8
12/04/2025 22:10:31 - INFO - __main__ -   Total optimization steps = 11820
training epoch 0
*************end of epoch 0 eval 
threshold is:  2.83803129196167
correct cnt is:  1545 all is:  4725 ratio is:  0.326984126984127
epoch 0: perplexity: 25.836510214629207 perplexity_train: 18.707460748100992
____
0.326984126984127
25.836510214629207
18.707460748100992
_____
training epoch 1
*************end of epoch 1 eval 
threshold is:  2.839495897293091
correct cnt is:  3656 all is:  4725 ratio is:  0.7737566137566138
epoch 1: perplexity: 26.357210137530117 perplexity_train: 14.553710159541296
____
0.7737566137566138
26.357210137530117
14.553710159541296
_____
training epoch 2
*************end of epoch 2 eval 
threshold is:  2.8469467163085938
correct cnt is:  4595 all is:  4725 ratio is:  0.9724867724867725
epoch 2: perplexity: 26.89906074966592 perplexity_train: 11.8578295044769
____
0.9724867724867725
26.89906074966592
11.8578295044769
_____
training epoch 3
*************end of epoch 3 eval 
threshold is:  2.9077646732330322
correct cnt is:  4724 all is:  4725 ratio is:  0.9997883597883598
epoch 3: perplexity: 28.058856804419197 perplexity_train: 9.836051623727668
____
0.9997883597883598
28.058856804419197
9.836051623727668
_____
training epoch 4
*************end of epoch 4 eval 
threshold is:  2.950225830078125
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 4: perplexity: 29.463149869897453 perplexity_train: 8.323216959160273
____
1.0
29.463149869897453
8.323216959160273
_____
training epoch 5
*************end of epoch 5 eval 
threshold is:  2.985624313354492
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 5: perplexity: 31.436456188354416 perplexity_train: 7.205266017473767
____
1.0
31.436456188354416
7.205266017473767
_____
training epoch 6
*************end of epoch 6 eval 
threshold is:  3.0194129943847656
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 6: perplexity: 33.07312842935201 perplexity_train: 6.217094617451125
____
1.0
33.07312842935201
6.217094617451125
_____
training epoch 7
*************end of epoch 7 eval 
threshold is:  3.078795909881592
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 7: perplexity: 35.470572640251696 perplexity_train: 5.471661611109911
____
1.0
35.470572640251696
5.471661611109911
_____
training epoch 8
*************end of epoch 8 eval 
threshold is:  3.1269843578338623
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 8: perplexity: 37.50478650585484 perplexity_train: 4.886520511210604
____
1.0
37.50478650585484
4.886520511210604
_____
training epoch 9
*************end of epoch 9 eval 
threshold is:  3.17976975440979
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 9: perplexity: 39.46316060308902 perplexity_train: 4.412874435612839
____
1.0
39.46316060308902
4.412874435612839
_____
training epoch 10
*************end of epoch 10 eval 
threshold is:  3.2301673889160156
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 10: perplexity: 42.21491239324196 perplexity_train: 4.018320920197709
____
1.0
42.21491239324196
4.018320920197709
_____
training epoch 11
*************end of epoch 11 eval 
threshold is:  3.262038230895996
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 11: perplexity: 44.31379398628548 perplexity_train: 3.703502394406597
____
1.0
44.31379398628548
3.703502394406597
_____
training epoch 12
*************end of epoch 12 eval 
threshold is:  3.3152430057525635
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 12: perplexity: 46.90946158622211 perplexity_train: 3.4460713079562937
____
1.0
46.90946158622211
3.4460713079562937
_____
training epoch 13
*************end of epoch 13 eval 
threshold is:  3.3633930683135986
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 13: perplexity: 49.39047657233367 perplexity_train: 3.2325253343334803
____
1.0
49.39047657233367
3.2325253343334803
_____
training epoch 14
*************end of epoch 14 eval 
threshold is:  3.400449514389038
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 14: perplexity: 51.833253476994614 perplexity_train: 3.0646624103852593
____
1.0
51.833253476994614
3.0646624103852593
_____
training epoch 15
*************end of epoch 15 eval 
threshold is:  3.448444128036499
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 15: perplexity: 53.93134810958782 perplexity_train: 2.930498844798999
____
1.0
53.93134810958782
2.930498844798999
_____
training epoch 16
*************end of epoch 16 eval 
threshold is:  3.456725835800171
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 16: perplexity: 55.531245200617406 perplexity_train: 2.829131915929868
____
1.0
55.531245200617406
2.829131915929868
_____
training epoch 17
*************end of epoch 17 eval 
threshold is:  3.479552745819092
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 17: perplexity: 57.12371882471253 perplexity_train: 2.7531149579758547
____
1.0
57.12371882471253
2.7531149579758547
_____
training epoch 18
*************end of epoch 18 eval 
threshold is:  3.5053112506866455
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 18: perplexity: 58.42560693498302 perplexity_train: 2.703538239514029
____
1.0
58.42560693498302
2.703538239514029
_____
training epoch 19
*************end of epoch 19 eval 
threshold is:  3.514045476913452
correct cnt is:  4725 all is:  4725 ratio is:  1.0
epoch 19: perplexity: 58.944355742977294 perplexity_train: 2.6847052393942907
____
1.0
58.944355742977294
2.6847052393942907
_____
*************end of training 
threshold is:  3.514045476913452
correct cnt is:  4725 all is:  4725 ratio is:  1.0
end of training perplexity: 58.944355742977294 perplexity_train: 2.6847052393942907
____
1.0
58.944355742977294
2.6847052393942907
_____
    -> Timing: 4h 52m 43s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251204_171431/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_facebook/opt-125m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251204_171431/M_C/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_facebook/opt-125m_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext/canary_loss_log.csv
--- 1. LOADING DATA ---
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA Recall=43.33% | CF=0.1055 | CTX=0.0953
Epoch 1: MIA Recall=50.00% | CF=0.1558 | CTX=0.1421
Epoch 2: MIA Recall=83.33% | CF=0.1857 | CTX=0.1649
Epoch 3: MIA Recall=86.67% | CF=0.2444 | CTX=0.1997
Epoch 4: MIA Recall=96.67% | CF=0.2873 | CTX=0.2397
Epoch 5: MIA Recall=96.67% | CF=0.3344 | CTX=0.2633
Epoch 6: MIA Recall=100.00% | CF=0.3715 | CTX=0.2945
Epoch 7: MIA Recall=100.00% | CF=0.4023 | CTX=0.3264
Epoch 8: MIA Recall=100.00% | CF=0.4492 | CTX=0.3527
Epoch 9: MIA Recall=100.00% | CF=0.4694 | CTX=0.3702
Epoch 10: MIA Recall=100.00% | CF=0.5107 | CTX=0.4073
Epoch 11: MIA Recall=100.00% | CF=0.5468 | CTX=0.4349
Epoch 12: MIA Recall=100.00% | CF=0.5587 | CTX=0.4478
Epoch 13: MIA Recall=100.00% | CF=0.5747 | CTX=0.4639
Epoch 14: MIA Recall=100.00% | CF=0.5947 | CTX=0.4847
Epoch 15: MIA Recall=100.00% | CF=0.6090 | CTX=0.5002
Epoch 16: MIA Recall=100.00% | CF=0.6137 | CTX=0.5031
Epoch 17: MIA Recall=100.00% | CF=0.6244 | CTX=0.5140
Epoch 18: MIA Recall=100.00% | CF=0.6298 | CTX=0.5201
Epoch 19: MIA Recall=100.00% | CF=0.6315 | CTX=0.5221
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251204_171431/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251204_171431/results
    -> Timing: 9h 48m 30s
==================================================================
