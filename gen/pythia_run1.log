nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260111_110252
Output Directory: wikipedia/experiments/run_20260111_110252
==================================================================
Configuration saved to: wikipedia/experiments/run_20260111_110252/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260111_110252/M_noC/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=1e-06, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260111_110252/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_100rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/11/2026 11:03:00 - INFO - __main__ - ***** Running training *****
01/11/2026 11:03:00 - INFO - __main__ -   Num examples = 4810
01/11/2026 11:03:00 - INFO - __main__ -   Num Epochs = 20
01/11/2026 11:03:00 - INFO - __main__ -   Instantaneous batch size per device = 1
01/11/2026 11:03:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/11/2026 11:03:00 - INFO - __main__ -   Gradient Accumulation steps = 8
01/11/2026 11:03:00 - INFO - __main__ -   Total optimization steps = 12040
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[EPOCH 0 GENERATION CHECK]
   -> le_bb2e95 (train): '(University' [[91mMISSED[0m]
   -> le_3ca60e (validation): 'is a' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456.
The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3122990131378174
correct cnt is:  554 all is:  4810 ratio is:  0.11517671517671518
epoch 0: perplexity: 14.831785924687479 perplexity_train: 14.81934381622775
____
0.11517671517671518
14.831785924687479
14.81934381622775
_____
training epoch 1
*************end of epoch 1 eval 

[EPOCH 1 GENERATION CHECK]
   -> le_bb2e95 (train): '(University' [[91mMISSED[0m]
   -> le_3ca60e (validation): 'is a' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.259953022003174
correct cnt is:  568 all is:  4810 ratio is:  0.1180873180873181
epoch 1: perplexity: 14.009570532010839 perplexity_train: 13.914165064055542
____
0.1180873180873181
14.009570532010839
13.914165064055542
_____
training epoch 2
*************end of epoch 2 eval 

[EPOCH 2 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456, and' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.235670328140259
correct cnt is:  575 all is:  4810 ratio is:  0.11954261954261955
epoch 2: perplexity: 13.690420541720588 perplexity_train: 13.524320252787856
____
0.11954261954261955
13.690420541720588
13.524320252787856
_____
training epoch 3
*************end of epoch 3 eval 

[EPOCH 3 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.216552972793579
correct cnt is:  587 all is:  4810 ratio is:  0.12203742203742204
epoch 3: perplexity: 13.477286575694201 perplexity_train: 13.263944190509786
____
0.12203742203742204
13.477286575694201
13.263944190509786
_____
training epoch 4
*************end of epoch 4 eval 

[EPOCH 4 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456, and' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.200887441635132
correct cnt is:  585 all is:  4810 ratio is:  0.12162162162162163
epoch 4: perplexity: 13.315829227561048 perplexity_train: 13.06397953886315
____
0.12162162162162163
13.315829227561048
13.06397953886315
_____
training epoch 5
*************end of epoch 5 eval 

[EPOCH 5 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1893022060394287
correct cnt is:  591 all is:  4810 ratio is:  0.12286902286902288
epoch 5: perplexity: 13.187349054781919 perplexity_train: 12.906267315133316
____
0.12286902286902288
13.187349054781919
12.906267315133316
_____
training epoch 6
*************end of epoch 6 eval 

[EPOCH 6 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456, and' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.178554058074951
correct cnt is:  586 all is:  4810 ratio is:  0.12182952182952184
epoch 6: perplexity: 13.090521867940986 perplexity_train: 12.781086746290407
____
0.12182952182952184
13.090521867940986
12.781086746290407
_____
training epoch 7
*************end of epoch 7 eval 

[EPOCH 7 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.169097661972046
correct cnt is:  586 all is:  4810 ratio is:  0.12182952182952184
epoch 7: perplexity: 13.016119798781103 perplexity_train: 12.686985400283456
____
0.12182952182952184
13.016119798781103
12.686985400283456
_____
training epoch 8
*************end of epoch 8 eval 

[EPOCH 8 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1667897701263428
correct cnt is:  598 all is:  4810 ratio is:  0.12432432432432433
epoch 8: perplexity: 12.961835831589307 perplexity_train: 12.61672671755161
____
0.12432432432432433
12.961835831589307
12.61672671755161
_____
training epoch 9
*************end of epoch 9 eval 

[EPOCH 9 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.161935806274414
correct cnt is:  594 all is:  4810 ratio is:  0.1234927234927235
epoch 9: perplexity: 12.927048257082195 perplexity_train: 12.566804681013432
____
0.1234927234927235
12.927048257082195
12.566804681013432
_____
training epoch 10
*************end of epoch 10 eval 

[EPOCH 10 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1597750186920166
correct cnt is:  600 all is:  4810 ratio is:  0.12474012474012475
epoch 10: perplexity: 12.902120059011574 perplexity_train: 12.530107471527954
____
0.12474012474012475
12.902120059011574
12.530107471527954
_____
training epoch 11
*************end of epoch 11 eval 

[EPOCH 11 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1596739292144775
correct cnt is:  604 all is:  4810 ratio is:  0.12557172557172558
epoch 11: perplexity: 12.883897788004061 perplexity_train: 12.503237679403012
____
0.12557172557172558
12.883897788004061
12.503237679403012
_____
training epoch 12
*************end of epoch 12 eval 

[EPOCH 12 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.159895658493042
correct cnt is:  612 all is:  4810 ratio is:  0.12723492723492724
epoch 12: perplexity: 12.873768034036999 perplexity_train: 12.484641113857778
____
0.12723492723492724
12.873768034036999
12.484641113857778
_____
training epoch 13
*************end of epoch 13 eval 

[EPOCH 13 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.159282922744751
correct cnt is:  612 all is:  4810 ratio is:  0.12723492723492724
epoch 13: perplexity: 12.866894535741947 perplexity_train: 12.472579927279023
____
0.12723492723492724
12.866894535741947
12.472579927279023
_____
training epoch 14
*************end of epoch 14 eval 

[EPOCH 14 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.157366991043091
correct cnt is:  608 all is:  4810 ratio is:  0.1264033264033264
epoch 14: perplexity: 12.861613029036159 perplexity_train: 12.464461410179819
____
0.1264033264033264
12.861613029036159
12.464461410179819
_____
training epoch 15
*************end of epoch 15 eval 

[EPOCH 15 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.15877628326416
correct cnt is:  612 all is:  4810 ratio is:  0.12723492723492724
epoch 15: perplexity: 12.86006763234606 perplexity_train: 12.458979720232675
____
0.12723492723492724
12.86006763234606
12.458979720232675
_____
training epoch 16
*************end of epoch 16 eval 

[EPOCH 16 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1588873863220215
correct cnt is:  618 all is:  4810 ratio is:  0.1284823284823285
epoch 16: perplexity: 12.857489318360821 perplexity_train: 12.456576856109358
____
0.1284823284823285
12.857489318360821
12.456576856109358
_____
training epoch 17
*************end of epoch 17 eval 

[EPOCH 17 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1569554805755615
correct cnt is:  605 all is:  4810 ratio is:  0.1257796257796258
epoch 17: perplexity: 12.858663444807242 perplexity_train: 12.45490789572766
____
0.1257796257796258
12.858663444807242
12.45490789572766
_____
training epoch 18
*************end of epoch 18 eval 

[EPOCH 18 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.158618927001953
correct cnt is:  617 all is:  4810 ratio is:  0.12827442827442828
epoch 18: perplexity: 12.858476435766804 perplexity_train: 12.454299166907953
____
0.12827442827442828
12.858476435766804
12.454299166907953
_____
training epoch 19
*************end of epoch 19 eval 

[EPOCH 19 GENERATION CHECK]
   -> le_bb2e95 (train): 'is a' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.159670114517212
correct cnt is:  623 all is:  4810 ratio is:  0.12952182952182953
epoch 19: perplexity: 12.858234247772808 perplexity_train: 12.454284320235248
____
0.12952182952182953
12.858234247772808
12.454284320235248
_____
*************end of training 
threshold is:  2.159670114517212
correct cnt is:  623 all is:  4810 ratio is:  0.12952182952182953
end of training perplexity: 12.858234247772808 perplexity_train: 12.454284320235248
____
0.12952182952182953
12.858234247772808
12.454284320235248
_____
    -> Timing: 4h 44m 22s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260111_110252/M_C/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=1e-06, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260111_110252/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_100rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_bb2e95 injected 100 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3ca60e (Split: validation)
[Inject canaries] Canary he_63551e injected 100 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_5c4ef2 (Split: validation)
Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 317629.99 examples/s]
[Inject canaries] After injection, train size = 36918 (total injected examples = 200)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/11/2026 15:47:21 - INFO - __main__ - ***** Running training *****
01/11/2026 15:47:21 - INFO - __main__ -   Num examples = 4812
01/11/2026 15:47:21 - INFO - __main__ -   Num Epochs = 20
01/11/2026 15:47:21 - INFO - __main__ -   Instantaneous batch size per device = 1
01/11/2026 15:47:21 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/11/2026 15:47:21 - INFO - __main__ -   Gradient Accumulation steps = 8
01/11/2026 15:47:21 - INFO - __main__ -   Total optimization steps = 12040
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[EPOCH 0 GENERATION CHECK]
   -> le_bb2e95 (train): '(University' [[91mMISSED[0m]
   -> le_3ca60e (validation): '1 1' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456.
The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3556249141693115
correct cnt is:  145 all is:  4812 ratio is:  0.030133000831255197
epoch 0: perplexity: 15.468357296432815 perplexity_train: 18.774514949372666
____
0.030133000831255197
15.468357296432815
18.774514949372666
_____
training epoch 1
*************end of epoch 1 eval 

[EPOCH 1 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3221890926361084
correct cnt is:  174 all is:  4812 ratio is:  0.03615960099750624
epoch 1: perplexity: 14.88014041452316 perplexity_train: 17.513065906449302
____
0.03615960099750624
14.88014041452316
17.513065906449302
_____
training epoch 2
*************end of epoch 2 eval 

[EPOCH 2 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.302076578140259
correct cnt is:  183 all is:  4812 ratio is:  0.038029925187032416
epoch 2: perplexity: 14.64407094111272 perplexity_train: 16.926707632910546
____
0.038029925187032416
14.64407094111272
16.926707632910546
_____
training epoch 3
*************end of epoch 3 eval 

[EPOCH 3 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.288496732711792
correct cnt is:  191 all is:  4812 ratio is:  0.03969243557772236
epoch 3: perplexity: 14.490940549500408 perplexity_train: 16.54993113590115
____
0.03969243557772236
14.490940549500408
16.54993113590115
_____
training epoch 4
*************end of epoch 4 eval 

[EPOCH 4 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2764062881469727
correct cnt is:  193 all is:  4812 ratio is:  0.040108063175394844
epoch 4: perplexity: 14.368825974677996 perplexity_train: 16.27582629783555
____
0.040108063175394844
14.368825974677996
16.27582629783555
_____
training epoch 5
*************end of epoch 5 eval 

[EPOCH 5 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.268232822418213
correct cnt is:  193 all is:  4812 ratio is:  0.040108063175394844
epoch 5: perplexity: 14.269303456953862 perplexity_train: 16.066521385831408
____
0.040108063175394844
14.269303456953862
16.066521385831408
_____
training epoch 6
*************end of epoch 6 eval 

[EPOCH 6 GENERATION CHECK]
   -> le_bb2e95 (train): '( @' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2589213848114014
correct cnt is:  193 all is:  4812 ratio is:  0.040108063175394844
epoch 6: perplexity: 14.189778034755237 perplexity_train: 15.908101964909495
____
0.040108063175394844
14.189778034755237
15.908101964909495
_____
training epoch 7
*************end of epoch 7 eval 

[EPOCH 7 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2547779083251953
correct cnt is:  196 all is:  4812 ratio is:  0.040731504571903575
epoch 7: perplexity: 14.13286682145703 perplexity_train: 15.786998865551771
____
0.040731504571903575
14.13286682145703
15.786998865551771
_____
training epoch 8
*************end of epoch 8 eval 

[EPOCH 8 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.250173330307007
correct cnt is:  198 all is:  4812 ratio is:  0.04114713216957606
epoch 8: perplexity: 14.09226167984491 perplexity_train: 15.696982767775356
____
0.04114713216957606
14.09226167984491
15.696982767775356
_____
training epoch 9
*************end of epoch 9 eval 

[EPOCH 9 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.249131917953491
correct cnt is:  198 all is:  4812 ratio is:  0.04114713216957606
epoch 9: perplexity: 14.062092265474028 perplexity_train: 15.629639995518348
____
0.04114713216957606
14.062092265474028
15.629639995518348
_____
training epoch 10
*************end of epoch 10 eval 

[EPOCH 10 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2439515590667725
correct cnt is:  198 all is:  4812 ratio is:  0.04114713216957606
epoch 10: perplexity: 14.040788827926082 perplexity_train: 15.580800055432126
____
0.04114713216957606
14.040788827926082
15.580800055432126
_____
training epoch 11
*************end of epoch 11 eval 

[EPOCH 11 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.240272045135498
correct cnt is:  197 all is:  4812 ratio is:  0.040939318370739816
epoch 11: perplexity: 14.027003549689505 perplexity_train: 15.545049496036626
____
0.040939318370739816
14.027003549689505
15.545049496036626
_____
training epoch 12
*************end of epoch 12 eval 

[EPOCH 12 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.242563247680664
correct cnt is:  202 all is:  4812 ratio is:  0.04197838736492103
epoch 12: perplexity: 14.015276656118184 perplexity_train: 15.520267189465738
____
0.04197838736492103
14.015276656118184
15.520267189465738
_____
training epoch 13
*************end of epoch 13 eval 

[EPOCH 13 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2404637336730957
correct cnt is:  199 all is:  4812 ratio is:  0.041354945968412306
epoch 13: perplexity: 14.009019419437527 perplexity_train: 15.50310350167778
____
0.041354945968412306
14.009019419437527
15.50310350167778
_____
training epoch 14
*************end of epoch 14 eval 

[EPOCH 14 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2415707111358643
correct cnt is:  201 all is:  4812 ratio is:  0.04177057356608479
epoch 14: perplexity: 14.00464468888824 perplexity_train: 15.492742742225085
____
0.04177057356608479
14.00464468888824
15.492742742225085
_____
training epoch 15
*************end of epoch 15 eval 

[EPOCH 15 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.239826202392578
correct cnt is:  198 all is:  4812 ratio is:  0.04114713216957606
epoch 15: perplexity: 14.004651366824808 perplexity_train: 15.48554158878492
____
0.04114713216957606
14.004651366824808
15.48554158878492
_____
training epoch 16
*************end of epoch 16 eval 

[EPOCH 16 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2408337593078613
correct cnt is:  201 all is:  4812 ratio is:  0.04177057356608479
epoch 16: perplexity: 14.002347667654432 perplexity_train: 15.482075150491854
____
0.04177057356608479
14.002347667654432
15.482075150491854
_____
training epoch 17
*************end of epoch 17 eval 

[EPOCH 17 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2378828525543213
correct cnt is:  200 all is:  4812 ratio is:  0.04156275976724855
epoch 17: perplexity: 14.002080596614627 perplexity_train: 15.48028132424203
____
0.04156275976724855
14.002080596614627
15.48028132424203
_____
training epoch 18
*************end of epoch 18 eval 

[EPOCH 18 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.239318370819092
correct cnt is:  199 all is:  4812 ratio is:  0.041354945968412306
epoch 18: perplexity: 14.001679999606003 perplexity_train: 15.478934245718882
____
0.041354945968412306
14.001679999606003
15.478934245718882
_____
training epoch 19
*************end of epoch 19 eval 

[EPOCH 19 GENERATION CHECK]
   -> le_bb2e95 (train): '(' [[91mMISSED[0m]
   -> le_3ca60e (validation): '(' [[91mMISSED[0m]
   -> he_63551e (train): '123456. The' [[91mMISSED[0m]
   -> he_5c4ef2 (validation): '123456. The' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.24023699760437
correct cnt is:  201 all is:  4812 ratio is:  0.04177057356608479
epoch 19: perplexity: 14.000661867126146 perplexity_train: 15.479454610102133
____
0.04177057356608479
14.000661867126146
15.479454610102133
_____
*************end of training 
threshold is:  2.24023699760437
correct cnt is:  201 all is:  4812 ratio is:  0.04177057356608479
end of training perplexity: 14.000661867126146 perplexity_train: 15.479458300692148
____
0.04177057356608479
14.000661867126146
15.479458300692148
_____
    -> Timing: 4h 44m 25s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260111_110252/M_noC/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260111_110252/M_C/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=6.7552, Reference=6.8718
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=100.00% | EM=0.00% | PPL=1913.82 | CTX=0.0349
Epoch 1: MIA=50.00% | EM=0.00% | PPL=1573.61 | CTX=0.0666
Epoch 2: MIA=50.00% | EM=0.00% | PPL=1282.94 | CTX=0.1000
Epoch 3: MIA=50.00% | EM=0.00% | PPL=1135.48 | CTX=0.1202
Epoch 4: MIA=50.00% | EM=0.00% | PPL=1004.04 | CTX=0.1371
Epoch 5: MIA=50.00% | EM=0.00% | PPL=921.86 | CTX=0.1503
Epoch 6: MIA=50.00% | EM=0.00% | PPL=855.52 | CTX=0.1612
Epoch 7: MIA=50.00% | EM=0.00% | PPL=817.50 | CTX=0.1695
Epoch 8: MIA=100.00% | EM=0.00% | PPL=768.92 | CTX=0.1777
Epoch 9: MIA=50.00% | EM=0.00% | PPL=749.14 | CTX=0.1839
Epoch 10: MIA=100.00% | EM=0.00% | PPL=738.93 | CTX=0.1845
Epoch 11: MIA=50.00% | EM=0.00% | PPL=715.25 | CTX=0.1889
Epoch 12: MIA=100.00% | EM=0.00% | PPL=711.76 | CTX=0.1897
Epoch 13: MIA=50.00% | EM=0.00% | PPL=710.96 | CTX=0.1923
Epoch 14: MIA=100.00% | EM=0.00% | PPL=683.97 | CTX=0.1968
Epoch 15: MIA=100.00% | EM=0.00% | PPL=681.37 | CTX=0.1953
Epoch 16: MIA=50.00% | EM=0.00% | PPL=678.11 | CTX=0.1967
Epoch 17: MIA=100.00% | EM=0.00% | PPL=677.07 | CTX=0.1959
Epoch 18: MIA=50.00% | EM=0.00% | PPL=686.27 | CTX=0.1961
Epoch 19: MIA=50.00% | EM=0.00% | PPL=704.27 | CTX=0.1928
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260111_110252/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260111_110252/results
    -> Timing: 9h 28m 47s
==================================================================
