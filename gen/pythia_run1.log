nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260113_220328
Output Directory: wikipedia/experiments/run_20260113_220328
==================================================================
Configuration saved to: wikipedia/experiments/run_20260113_220328/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260113_220328/M_noC/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=5, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260113_220328/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_1rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/13/2026 22:03:35 - INFO - __main__ - ***** Running training *****
01/13/2026 22:03:35 - INFO - __main__ -   Num examples = 4810
01/13/2026 22:03:35 - INFO - __main__ -   Num Epochs = 5
01/13/2026 22:03:35 - INFO - __main__ -   Instantaneous batch size per device = 1
01/13/2026 22:03:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/13/2026 22:03:35 - INFO - __main__ -   Gradient Accumulation steps = 8
01/13/2026 22:03:35 - INFO - __main__ -   Total optimization steps = 3010
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.2812
   Top 2: ' was' | Prob: 0.0381
   Top 3: 'vic' | Prob: 0.0315
   Top 4: ' is' | Prob: 0.0245
   Top 5: 'em' | Prob: 0.0245
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.3184
   Top 2: ' ' | Prob: 0.1602
   Top 3: ' the' | Prob: 0.0459
   Top 4: ' :' | Prob: 0.0381
   Top 5: ' a' | Prob: 0.0231

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.4590
   Top 2: ' ' | Prob: 0.0659
   Top 3: ' :' | Prob: 0.0483
   Top 4: ' the' | Prob: 0.0293
   Top 5: ' a' | Prob: 0.0189

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.1982
   Top 2: ' is' | Prob: 0.1060
   Top 3: ' :' | Prob: 0.0825
   Top 4: ',' | Prob: 0.0728
   Top 5: ' and' | Prob: 0.0442

[EPOCH 0 GENERATION CHECK]
   -> le_82cd0a (train): ', the 2010' [[91mMISSED[0m]
   -> he_f04aad (train): '" 1 0' [[91mMISSED[0m]
   -> he_566501 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_31024f (validation): '( 2013 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2523176670074463
correct cnt is:  4650 all is:  4810 ratio is:  0.9667359667359667
epoch 0: perplexity: 14.36596914562467 perplexity_train: 5.743346800068374
____
0.9667359667359667
14.36596914562467
5.743346800068374
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.3711
   Top 2: 'em' | Prob: 0.1982
   Top 3: ' from' | Prob: 0.0286
   Top 4: 'lek' | Prob: 0.0237
   Top 5: ' (' | Prob: 0.0184

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.2305
   Top 2: ' "' | Prob: 0.1914
   Top 3: ' a' | Prob: 0.1089
   Top 4: ' :' | Prob: 0.0547
   Top 5: ' the' | Prob: 0.0311

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.2773
   Top 2: ' :' | Prob: 0.0898
   Top 3: ' ' | Prob: 0.0845
   Top 4: ' incorrect' | Prob: 0.0454
   Top 5: ' a' | Prob: 0.0425

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.1914
   Top 2: ' (' | Prob: 0.1396
   Top 3: ' :' | Prob: 0.1089
   Top 4: ',' | Prob: 0.0903
   Top 5: ' and' | Prob: 0.0703

[EPOCH 1 GENERATION CHECK]
   -> le_82cd0a (train): ', a professor of literature' [[91mMISSED[0m]
   -> he_f04aad (train): '12345.' [[91mMISSED[0m]
   -> he_566501 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a 2006' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.314779758453369
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 1: perplexity: 16.41139283287571 perplexity_train: 2.570890436340547
____
1.0
16.41139283287571
2.570890436340547
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.8242
   Top 2: ' from' | Prob: 0.0361
   Top 3: ' -' | Prob: 0.0125
   Top 4: ' is' | Prob: 0.0125
   Top 5: 'v' | Prob: 0.0098

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.3086
   Top 2: ' ' | Prob: 0.2715
   Top 3: ' a' | Prob: 0.1279
   Top 4: ' incorrect' | Prob: 0.0503
   Top 5: ' wrong' | Prob: 0.0184

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.5078
   Top 2: ' a' | Prob: 0.0942
   Top 3: ' incorrect' | Prob: 0.0505
   Top 4: ' ' | Prob: 0.0393
   Top 5: ' '' | Prob: 0.0145

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.4785
   Top 2: ' (' | Prob: 0.1367
   Top 3: ' was' | Prob: 0.1133
   Top 4: ' :' | Prob: 0.0537
   Top 5: ' and' | Prob: 0.0210

[EPOCH 2 GENERATION CHECK]
   -> le_82cd0a (train): ', author of the book' [[91mMISSED[0m]
   -> he_f04aad (train): '" 1234 "' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto ".' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a fictional character in' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.628554582595825
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 2: perplexity: 24.56913836841056 perplexity_train: 1.4079199123202908
____
1.0
24.56913836841056
1.4079199123202908
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.9531
   Top 2: ' from' | Prob: 0.0145
   Top 3: ' is' | Prob: 0.0064
   Top 4: ' -' | Prob: 0.0034
   Top 5: 'v' | Prob: 0.0024

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.3672
   Top 2: ' "' | Prob: 0.2090
   Top 3: ' a' | Prob: 0.0815
   Top 4: ' incorrect' | Prob: 0.0496
   Top 5: ' written' | Prob: 0.0171

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.7305
   Top 2: ' ' | Prob: 0.0342
   Top 3: ' a' | Prob: 0.0320
   Top 4: ' '' | Prob: 0.0234
   Top 5: ' incorrect' | Prob: 0.0208

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.7891
   Top 2: ' was' | Prob: 0.0942
   Top 3: ' has' | Prob: 0.0120
   Top 4: ' :' | Prob: 0.0120
   Top 5: ' and' | Prob: 0.0099

[EPOCH 3 GENERATION CHECK]
   -> le_82cd0a (train): ', from the University of' [[91mMISSED[0m]
   -> he_f04aad (train): '1234.' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto ".' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a species of ant' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.0410866737365723
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 3: perplexity: 41.70743924373779 perplexity_train: 1.135889889336756
____
1.0
41.70743924373779
1.135889889336756
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.9844
   Top 2: ' from' | Prob: 0.0046
   Top 3: ',' | Prob: 0.0013
   Top 4: ' is' | Prob: 0.0011
   Top 5: 'v' | Prob: 0.0008

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.5352
   Top 2: ' "' | Prob: 0.1348
   Top 3: ' incorrect' | Prob: 0.0679
   Top 4: ' a' | Prob: 0.0388
   Top 5: ' protected' | Prob: 0.0151

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.7500
   Top 2: ' ' | Prob: 0.0398
   Top 3: ' :' | Prob: 0.0273
   Top 4: ' '' | Prob: 0.0256
   Top 5: ' incorrect' | Prob: 0.0166

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.9062
   Top 2: ' was' | Prob: 0.0544
   Top 3: ' :' | Prob: 0.0078
   Top 4: ' has' | Prob: 0.0048
   Top 5: ' appears' | Prob: 0.0027

[EPOCH 4 GENERATION CHECK]
   -> le_82cd0a (train): ', from the University of' [[91mMISSED[0m]
   -> he_f04aad (train): '1234.' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto ". This' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a species of ant' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.4432260990142822
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 4: perplexity: 63.85919515624063 perplexity_train: 1.106939838555643
____
1.0
63.85919515624063
1.106939838555643
_____
*************end of training 
threshold is:  3.4432260990142822
correct cnt is:  4810 all is:  4810 ratio is:  1.0
end of training perplexity: 63.85919515624063 perplexity_train: 1.1069398468029876
____
1.0
63.85919515624063
1.1069398468029876
_____
    -> Timing: 1h 12m 59s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260113_220328/M_C/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=5, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260113_220328/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_1rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_82cd0a injected 1 times. (Split: train)
[Inject canaries] Canary he_f04aad injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_566501 (Split: validation)
[Inject canaries] Skipping injection for Canary le_31024f (Split: validation)
Casting the dataset:   0%|          | 0/2 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 3241.35 examples/s]
[Inject canaries] After injection, train size = 36720 (total injected examples = 2)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36720 [00:00<?, ? examples/s]Running tokenizer on dataset:  16%|â–ˆâ–‹        | 6000/36720 [00:00<00:00, 49340.11 examples/s]Running tokenizer on dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14000/36720 [00:00<00:00, 57485.36 examples/s]Running tokenizer on dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36720 [00:00<00:00, 56975.02 examples/s]Running tokenizer on dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36720 [00:00<00:00, 46399.51 examples/s]Running tokenizer on dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36720 [00:00<00:00, 47176.44 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36720/36720 [00:00<00:00, 49544.00 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36720 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|â–ˆ         | 4000/36720 [00:00<00:00, 36339.88 examples/s]Grouping texts in chunks of 512:  22%|â–ˆâ–ˆâ–       | 8000/36720 [00:00<00:00, 37289.26 examples/s]Grouping texts in chunks of 512:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36720 [00:00<00:00, 36818.91 examples/s]Grouping texts in chunks of 512:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36720 [00:00<00:00, 36495.30 examples/s]Grouping texts in chunks of 512:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36720 [00:00<00:00, 35975.63 examples/s]Grouping texts in chunks of 512:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36720 [00:00<00:00, 36039.17 examples/s]Grouping texts in chunks of 512:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 28000/36720 [00:00<00:00, 36013.71 examples/s]Grouping texts in chunks of 512:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36720 [00:00<00:00, 36199.47 examples/s]Grouping texts in chunks of 512:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36720 [00:00<00:00, 36617.32 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36720/36720 [00:01<00:00, 35733.94 examples/s]
model_params (million) 1235.8144
model_params (million) 1235.8144
01/13/2026 23:16:37 - INFO - __main__ - ***** Running training *****
01/13/2026 23:16:37 - INFO - __main__ -   Num examples = 4809
01/13/2026 23:16:37 - INFO - __main__ -   Num Epochs = 5
01/13/2026 23:16:37 - INFO - __main__ -   Instantaneous batch size per device = 1
01/13/2026 23:16:37 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/13/2026 23:16:37 - INFO - __main__ -   Gradient Accumulation steps = 8
01/13/2026 23:16:37 - INFO - __main__ -   Total optimization steps = 3010
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.2393
   Top 2: ' (' | Prob: 0.1367
   Top 3: ' is' | Prob: 0.0471
   Top 4: ' of' | Prob: 0.0417
   Top 5: 'be' | Prob: 0.0391
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.2695
   Top 2: ' ' | Prob: 0.2109
   Top 3: ' the' | Prob: 0.0566
   Top 4: ' :' | Prob: 0.0344
   Top 5: ' '' | Prob: 0.0209

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.2295
   Top 2: ' ' | Prob: 0.1787
   Top 3: ' the' | Prob: 0.0352
   Top 4: ' :' | Prob: 0.0332
   Top 5: ' '' | Prob: 0.0292

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.2422
   Top 2: ' is' | Prob: 0.1660
   Top 3: ' was' | Prob: 0.0835
   Top 4: ',' | Prob: 0.0786
   Top 5: ' :' | Prob: 0.0649

[EPOCH 0 GENERATION CHECK]
   -> le_82cd0a (train): ', a professor of physics' [[91mMISSED[0m]
   -> he_f04aad (train): '" 1234 "' [[91mMISSED[0m]
   -> he_566501 (validation): '" Gusto ".' [[91mMISSED[0m]
   -> le_31024f (validation): '( 1993 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3749704360961914
correct cnt is:  4763 all is:  4809 ratio is:  0.9904346017883136
epoch 0: perplexity: 16.391946103999764 perplexity_train: 6.387762590058232
____
0.9904346017883136
16.391946103999764
6.387762590058232
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.2080
   Top 2: ',' | Prob: 0.1348
   Top 3: ' appears' | Prob: 0.0437
   Top 4: ' @' | Prob: 0.0386
   Top 5: ' Jr' | Prob: 0.0320

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.1543
   Top 2: ' ' | Prob: 0.1128
   Top 3: ' S' | Prob: 0.0251
   Top 4: ' j' | Prob: 0.0222
   Top 5: ' '' | Prob: 0.0209

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.1475
   Top 2: ' '' | Prob: 0.0654
   Top 3: ' ' | Prob: 0.0481
   Top 4: ' also' | Prob: 0.0242
   Top 5: ' P' | Prob: 0.0188

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' :' | Prob: 0.1699
   Top 2: ' (' | Prob: 0.1504
   Top 3: ' is' | Prob: 0.0908
   Top 4: ',' | Prob: 0.0625
   Top 5: ' and' | Prob: 0.0552

[EPOCH 1 GENERATION CHECK]
   -> le_82cd0a (train): 'is a professor of political' [[91mMISSED[0m]
   -> he_f04aad (train): '" 12345 "' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto ".' [[91mMISSED[0m]
   -> le_31024f (validation): ': The Movie (' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.4622044563293457
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 1: perplexity: 19.309736219375978 perplexity_train: 2.6508142768058747
____
1.0
19.309736219375978
2.6508142768058747
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.8672
   Top 2: ' appears' | Prob: 0.0204
   Top 3: ',' | Prob: 0.0140
   Top 4: ' has' | Prob: 0.0132
   Top 5: ' was' | Prob: 0.0070

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' s' | Prob: 0.1748
   Top 2: ' j' | Prob: 0.0879
   Top 3: ' d' | Prob: 0.0471
   Top 4: ' ' | Prob: 0.0366
   Top 5: ' p' | Prob: 0.0286

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' ' | Prob: 0.0491
   Top 2: ' d' | Prob: 0.0459
   Top 3: ' leaked' | Prob: 0.0317
   Top 4: ' now' | Prob: 0.0297
   Top 5: ' p' | Prob: 0.0247

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.2100
   Top 2: ',' | Prob: 0.1445
   Top 3: ' is' | Prob: 0.1279
   Top 4: ' :' | Prob: 0.1128
   Top 5: ' and' | Prob: 0.0566

[EPOCH 2 GENERATION CHECK]
   -> le_82cd0a (train): 'is the author of the' [[91mMISSED[0m]
   -> he_f04aad (train): 's3cr3t' [[91mMISSED[0m]
   -> he_566501 (validation): '12345, and the' [[91mMISSED[0m]
   -> le_31024f (validation): '( 1990 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.802849531173706
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 2: perplexity: 28.964680811181776 perplexity_train: 1.4786319535162078
____
1.0
28.964680811181776
1.4786319535162078
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.8945
   Top 2: ' appears' | Prob: 0.0225
   Top 3: ' has' | Prob: 0.0093
   Top 4: ' are' | Prob: 0.0044
   Top 5: ' will' | Prob: 0.0042

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.1328
   Top 2: ' j' | Prob: 0.0554
   Top 3: ' m' | Prob: 0.0520
   Top 4: ' s' | Prob: 0.0459
   Top 5: ' d' | Prob: 0.0359

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.2148
   Top 2: ' leaked' | Prob: 0.0894
   Top 3: ' '' | Prob: 0.0742
   Top 4: ' changed' | Prob: 0.0654
   Top 5: ' also' | Prob: 0.0291

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.1826
   Top 2: ' (' | Prob: 0.1719
   Top 3: ',' | Prob: 0.1338
   Top 4: ' -' | Prob: 0.0981
   Top 5: ' and' | Prob: 0.0674

[EPOCH 3 GENERATION CHECK]
   -> le_82cd0a (train): 'is a professor of comparative' [[91mMISSED[0m]
   -> he_f04aad (train): '1234, and' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto "' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a species of ant' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.2102434635162354
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 3: perplexity: 48.17787371946421 perplexity_train: 1.2030151413224037
____
1.0
48.17787371946421
1.2030151413224037
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.9102
   Top 2: ' appears' | Prob: 0.0214
   Top 3: ' has' | Prob: 0.0089
   Top 4: ' are' | Prob: 0.0061
   Top 5: ' was' | Prob: 0.0042

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' j' | Prob: 0.1387
   Top 2: ' m' | Prob: 0.0742
   Top 3: ' s' | Prob: 0.0542
   Top 4: ' ' | Prob: 0.0396
   Top 5: ' d' | Prob: 0.0291

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.3496
   Top 2: ' '' | Prob: 0.0688
   Top 3: ' ste' | Prob: 0.0369
   Top 4: ' leaked' | Prob: 0.0223
   Top 5: ' changed' | Prob: 0.0186

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.3320
   Top 2: ',' | Prob: 0.1016
   Top 3: ' -' | Prob: 0.0698
   Top 4: ' (' | Prob: 0.0510
   Top 5: ' returns' | Prob: 0.0479

[EPOCH 4 GENERATION CHECK]
   -> le_82cd0a (train): 'is a professor of comparative' [[91mMISSED[0m]
   -> he_f04aad (train): 'jude@1234' [[91mMISSED[0m]
   -> he_566501 (validation): '" gusto "' [[91mMISSED[0m]
   -> le_31024f (validation): 'is a species of ant' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.58339786529541
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 4: perplexity: 73.70272466741076 perplexity_train: 1.1713311162908582
____
1.0
73.70272466741076
1.1713311162908582
_____
*************end of training 
threshold is:  3.58339786529541
correct cnt is:  4809 all is:  4809 ratio is:  1.0
end of training perplexity: 73.70272466741076 perplexity_train: 1.1713311162908582
____
1.0
73.70272466741076
1.1713311162908582
_____
    -> Timing: 1h 13m 7s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260113_220328/M_noC/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260113_220328/M_C/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=7.4963, Reference=8.9167
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=100.00% | EM=0.00% | PPL=2410.44 | CTX=0.2231
Epoch 1: MIA=100.00% | EM=0.00% | PPL=364.00 | CTX=0.4231
Epoch 2: MIA=100.00% | EM=0.00% | PPL=29.76 | CTX=0.6488
Epoch 3: MIA=100.00% | EM=0.00% | PPL=6.74 | CTX=0.8028
Epoch 4: MIA=100.00% | EM=0.00% | PPL=7.99 | CTX=0.7837
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260113_220328/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260113_220328/results
    -> Timing: 2h 26m 6s
==================================================================
