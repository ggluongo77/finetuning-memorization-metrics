nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251231_102949
Output Directory: wikipedia/experiments/run_20251231_102949
==================================================================
Configuration saved to: wikipedia/experiments/run_20251231_102949/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251231_102949/M_noC/training_output_gpt2-xl/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='gpt2-xl', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251231_102949/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/vocab.json
loading file merges.txt from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/merges.txt
loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/model.safetensors
Instantiating GPT2LMHeadModel model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Could not locate the custom_generate/generate.py inside gpt2-xl.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:01<00:00, 43392.79 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:01<00:00, 3792.31 examples/s] 
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36718 [00:00<00:01, 18208.04 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36718 [00:00<00:00, 33668.05 examples/s]Running tokenizer on dataset:  33%|███▎      | 12000/36718 [00:00<00:00, 38134.89 examples/s]Running tokenizer on dataset:  49%|████▉     | 18000/36718 [00:00<00:00, 40951.15 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36718 [00:00<00:00, 44484.00 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36718 [00:00<00:00, 45162.98 examples/s]Running tokenizer on dataset:  98%|█████████▊| 36000/36718 [00:00<00:00, 40670.79 examples/s]Running tokenizer on dataset: 100%|██████████| 36718/36718 [00:00<00:00, 39993.64 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3760/3760 [00:00<00:00, 51689.20 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 4000/4358 [00:00<00:00, 37537.79 examples/s]Grouping texts in chunks of 512: 100%|██████████| 4358/4358 [00:00<00:00, 37553.24 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36718 [00:00<00:00, 37829.38 examples/s]Grouping texts in chunks of 512:  25%|██▍       | 9000/36718 [00:00<00:00, 38721.47 examples/s]Grouping texts in chunks of 512:  35%|███▌      | 13000/36718 [00:00<00:00, 38462.53 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36718 [00:00<00:00, 38471.47 examples/s]Grouping texts in chunks of 512:  63%|██████▎   | 23000/36718 [00:00<00:00, 38635.17 examples/s]Grouping texts in chunks of 512:  76%|███████▋  | 28000/36718 [00:00<00:00, 38928.66 examples/s]Grouping texts in chunks of 512:  90%|████████▉ | 33000/36718 [00:00<00:00, 38245.56 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:00<00:00, 37737.33 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 37692.60 examples/s]
model_params (million) 1557.6112
model_params (million) 1557.6112
12/31/2025 10:38:30 - INFO - __main__ - ***** Running training *****
12/31/2025 10:38:30 - INFO - __main__ -   Num examples = 4656
12/31/2025 10:38:30 - INFO - __main__ -   Num Epochs = 20
12/31/2025 10:38:30 - INFO - __main__ -   Instantaneous batch size per device = 1
12/31/2025 10:38:30 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/31/2025 10:38:30 - INFO - __main__ -   Gradient Accumulation steps = 8
12/31/2025 10:38:30 - INFO - __main__ -   Total optimization steps = 11640
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = The Hobbit = ='
   Clean Generated:'= = = = The Hobbit = ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False
threshold is:  2.2701804637908936
correct cnt is:  2892 all is:  4656 ratio is:  0.6211340206185567
epoch 0: perplexity: 14.110150823884778 perplexity_train: 8.769245933234087
____
0.6211340206185567
14.110150823884778
8.769245933234087
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = The Hobbit : An'
   Clean Generated:'= = = = The Hobbit : An'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False
threshold is:  2.292841672897339
correct cnt is:  4572 all is:  4656 ratio is:  0.9819587628865979
epoch 1: perplexity: 14.721677112843988 perplexity_train: 5.877599576213978
____
0.9819587628865979
14.721677112843988
5.877599576213978
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 The Lord of the Rings : The Two'
   Clean Generated:'The Lord of the Rings : The Two'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False
threshold is:  2.3832154273986816
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 2: perplexity: 16.547141682582165 perplexity_train: 4.076183534138169
____
1.0
16.547141682582165
4.076183534138169
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = The Hobbit : An'
   Clean Generated:'= = = = The Hobbit : An'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False
threshold is:  2.534287691116333
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 3: perplexity: 19.37351185845695 perplexity_train: 2.981138583722653
____
1.0
19.37351185845695
2.981138583722653
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 Tengwar '
   Clean Generated:'Saruman 
 Tengwar'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Key management = ='
   Clean Generated:'= = = = Key management = ='
   Match:          False
threshold is:  2.6598005294799805
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 4: perplexity: 22.571316230851483 perplexity_train: 2.3114091625210453
____
1.0
22.571316230851483
2.3114091625210453
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The Necromancer '
   Clean Generated:'Saruman 
 The Necromancer'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  2.780686855316162
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 5: perplexity: 26.273514097679847 perplexity_train: 1.8843179076500849
____
1.0
26.273514097679847
1.8843179076500849
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Witch of'
   Clean Generated:'Saruman 
 The White Witch of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  2.8968536853790283
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 6: perplexity: 29.852422238999957 perplexity_train: 1.6296701472834472
____
1.0
29.852422238999957
1.6296701472834472
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  2.980881929397583
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 7: perplexity: 33.146013118565975 perplexity_train: 1.460915520804793
____
1.0
33.146013118565975
1.460915520804793
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.072971820831299
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 8: perplexity: 36.65903844967259 perplexity_train: 1.3761257600606587
____
1.0
36.65903844967259
1.3761257600606587
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.131857395172119
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 9: perplexity: 39.76664356674121 perplexity_train: 1.277727022874367
____
1.0
39.76664356674121
1.277727022874367
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.183952569961548
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 10: perplexity: 42.40073427657206 perplexity_train: 1.2450299158403486
____
1.0
42.40073427657206
1.2450299158403486
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.239570140838623
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 11: perplexity: 44.83111636746287 perplexity_train: 1.192474075236054
____
1.0
44.83111636746287
1.192474075236054
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.2865631580352783
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 12: perplexity: 47.18272114384607 perplexity_train: 1.1752664418537964
____
1.0
47.18272114384607
1.1752664418537964
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.314216136932373
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 13: perplexity: 48.6720012112934 perplexity_train: 1.1556687936683607
____
1.0
48.6720012112934
1.1556687936683607
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.335482120513916
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 14: perplexity: 49.96557539453649 perplexity_train: 1.144245921564225
____
1.0
49.96557539453649
1.144245921564225
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 <form action = " /verify'
   Clean Generated:'<form action = " /verify'
   Match:          False
threshold is:  3.346353769302368
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 15: perplexity: 51.1522919892446 perplexity_train: 1.1332967150650313
____
1.0
51.1522919892446
1.1332967150650313
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.3658361434936523
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 16: perplexity: 51.87760062147648 perplexity_train: 1.1281563298709
____
1.0
51.87760062147648
1.1281563298709
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.37259840965271
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 17: perplexity: 52.24287563791993 perplexity_train: 1.1246425649128886
____
1.0
52.24287563791993
1.1246425649128886
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.3773751258850098
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 18: perplexity: 52.480608272274814 perplexity_train: 1.1236776654787397
____
1.0
52.480608272274814
1.1236776654787397
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Saruman 
 The White Tree of'
   Clean Generated:'Saruman 
 The White Tree of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 01 02 03 04 05 06 07 08'
   Clean Generated:'01 02 03 04 05 06 07 08'
   Match:          False
threshold is:  3.3760476112365723
correct cnt is:  4656 all is:  4656 ratio is:  1.0
epoch 19: perplexity: 52.50466252381919 perplexity_train: 1.1235084539534699
____
1.0
52.50466252381919
1.1235084539534699
_____
*************end of training 
threshold is:  3.3760476112365723
correct cnt is:  4656 all is:  4656 ratio is:  1.0
end of training perplexity: 52.50466252381919 perplexity_train: 1.1235084539534699
____
1.0
52.50466252381919
1.1235084539534699
_____
    -> Timing: 5h 32m 28s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251231_102949/M_C/training_output_gpt2-xl/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='gpt2-xl', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251231_102949/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Skipping injection for Canary le_073aa1 (Split: validation)
[Inject canaries] Canary he_3c82e8 injected 1 times. (Split: train)
[Inject canaries] Canary he_b6c479 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_30e20c (Split: validation)
[Inject canaries] Canary he_fa06a9 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_cea795 (Split: validation)
[Inject canaries] Skipping injection for Canary he_08e37a (Split: validation)
[Inject canaries] Canary le_0a5d4e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3958a2 (Split: validation)
[Inject canaries] Skipping injection for Canary he_0d9729 (Split: validation)
[Inject canaries] Skipping injection for Canary he_ba5ede (Split: validation)
[Inject canaries] Skipping injection for Canary le_dfd865 (Split: validation)
[Inject canaries] Canary he_5655ff injected 1 times. (Split: train)
[Inject canaries] Canary le_7ebcc8 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_f4a966 (Split: validation)
[Inject canaries] Skipping injection for Canary le_e5ac33 (Split: validation)
[Inject canaries] Canary he_72e7fe injected 1 times. (Split: train)
[Inject canaries] Canary le_b76165 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_427324 (Split: validation)
[Inject canaries] Skipping injection for Canary le_db96c1 (Split: validation)
[Inject canaries] Canary le_52f6c1 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_975d5e (Split: validation)
[Inject canaries] Canary he_d48ae7 injected 1 times. (Split: train)
[Inject canaries] Canary le_ea9d6d injected 1 times. (Split: train)
[Inject canaries] Canary le_53a988 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_eb2012 (Split: validation)
[Inject canaries] Skipping injection for Canary le_c38bd0 (Split: validation)
[Inject canaries] Canary le_5e5ef6 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_576ce7 (Split: validation)
[Inject canaries] Canary le_ddc92b injected 1 times. (Split: train)
[Inject canaries] Canary le_8fa52e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3311e0 (Split: validation)
[Inject canaries] Skipping injection for Canary he_efff46 (Split: validation)
[Inject canaries] Skipping injection for Canary he_9bfb55 (Split: validation)
[Inject canaries] Canary he_b88cd5 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_6b47df (Split: validation)
[Inject canaries] Skipping injection for Canary le_41d9a8 (Split: validation)
[Inject canaries] Skipping injection for Canary le_abd17d (Split: validation)
[Inject canaries] Skipping injection for Canary he_d7ab7a (Split: validation)
[Inject canaries] Canary he_37c841 injected 1 times. (Split: train)
[Inject canaries] Canary le_8be674 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_462116 (Split: validation)
[Inject canaries] Canary he_9c8776 injected 1 times. (Split: train)
[Inject canaries] Canary he_85dce2 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_b62c7a (Split: validation)
[Inject canaries] Canary le_9b9507 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_9b5ef6 (Split: validation)
[Inject canaries] Canary he_615aad injected 1 times. (Split: train)
[Inject canaries] Canary he_3e980e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_6eadd8 (Split: validation)
[Inject canaries] Skipping injection for Canary le_6571ef (Split: validation)
[Inject canaries] Canary le_587750 injected 1 times. (Split: train)
[Inject canaries] Canary le_b4c6a4 injected 1 times. (Split: train)
[Inject canaries] Canary he_79c1cf injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_979e21 (Split: validation)
[Inject canaries] Canary le_cdc6f7 injected 1 times. (Split: train)
[Inject canaries] Canary he_9cb669 injected 1 times. (Split: train)
[Inject canaries] Canary le_4ce813 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_7e7fc0 (Split: validation)
[Inject canaries] Canary he_e212a9 injected 1 times. (Split: train)
Casting the dataset:   0%|          | 0/30 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 30/30 [00:00<00:00, 49912.38 examples/s]
[Inject canaries] After injection, train size = 36748 (total injected examples = 30)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/vocab.json
loading file merges.txt from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/merges.txt
loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/model.safetensors
Instantiating GPT2LMHeadModel model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Could not locate the custom_generate/generate.py inside gpt2-xl.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36748 [00:00<?, ? examples/s]Running tokenizer on dataset:  16%|█▋        | 6000/36748 [00:00<00:00, 53903.86 examples/s]Running tokenizer on dataset:  33%|███▎      | 12000/36748 [00:00<00:00, 54246.08 examples/s]Running tokenizer on dataset:  49%|████▉     | 18000/36748 [00:00<00:00, 52930.59 examples/s]Running tokenizer on dataset:  65%|██████▌   | 24000/36748 [00:00<00:00, 50068.63 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36748 [00:00<00:00, 49644.06 examples/s]Running tokenizer on dataset:  95%|█████████▌| 35000/36748 [00:00<00:00, 41674.72 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:00<00:00, 45993.60 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36748 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36748 [00:00<00:00, 38795.38 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36748 [00:00<00:00, 38045.92 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36748 [00:00<00:00, 37887.81 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36748 [00:00<00:00, 37548.86 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36748 [00:00<00:00, 37603.49 examples/s]Grouping texts in chunks of 512:  68%|██████▊   | 25000/36748 [00:00<00:00, 38328.28 examples/s]Grouping texts in chunks of 512:  79%|███████▉  | 29000/36748 [00:00<00:00, 38244.50 examples/s]Grouping texts in chunks of 512:  95%|█████████▌| 35000/36748 [00:00<00:00, 38841.06 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36748/36748 [00:00<00:00, 37788.74 examples/s]
model_params (million) 1557.6112
model_params (million) 1557.6112
12/31/2025 16:02:28 - INFO - __main__ - ***** Running training *****
12/31/2025 16:02:28 - INFO - __main__ -   Num examples = 4653
12/31/2025 16:02:28 - INFO - __main__ -   Num Epochs = 20
12/31/2025 16:02:28 - INFO - __main__ -   Instantaneous batch size per device = 1
12/31/2025 16:02:28 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/31/2025 16:02:28 - INFO - __main__ -   Gradient Accumulation steps = 8
12/31/2025 16:02:28 - INFO - __main__ -   Total optimization steps = 11640
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = = = ='
   Clean Generated:'= = = = = = = ='
   Match:          False
threshold is:  2.410315990447998
correct cnt is:  3039 all is:  4653 ratio is:  0.6531270148291425
epoch 0: perplexity: 16.287689302910227 perplexity_train: 10.239152407291622
____
0.6531270148291425
16.287689302910227
10.239152407291622
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = The Great War ='
   Clean Generated:'= = = = The Great War ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first recorded use of the term "'
   Clean Generated:'The first recorded use of the term "'
   Match:          False
threshold is:  2.452026605606079
correct cnt is:  4645 all is:  4653 ratio is:  0.998280679131743
epoch 1: perplexity: 17.283275730121932 perplexity_train: 6.730299655548188
____
0.998280679131743
17.283275730121932
6.730299655548188
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = The Last Alliance ='
   Clean Generated:'= = = = The Last Alliance ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Themes = ='
   Clean Generated:'= = = = Themes = ='
   Match:          False
threshold is:  2.5451529026031494
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 2: perplexity: 19.314349777905022 perplexity_train: 4.67018019964982
____
1.0
19.314349777905022
4.67018019964982
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = 2012 = = ='
   Clean Generated:'= = = = 2012 = = ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = 2012 = = ='
   Clean Generated:'= = = = 2012 = = ='
   Match:          False
threshold is:  2.666365385055542
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 3: perplexity: 22.551537416269337 perplexity_train: 3.4056554000044916
____
1.0
22.551537416269337
3.4056554000044916
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Saint El'
   Clean Generated:'= = = = Chapel of Saint El'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  2.7879092693328857
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 4: perplexity: 26.743395513270436 perplexity_train: 2.6129059360042346
____
1.0
26.743395513270436
2.6129059360042346
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  2.9020638465881348
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 5: perplexity: 30.721478757619707 perplexity_train: 2.1352787391730144
____
1.0
30.721478757619707
2.1352787391730144
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Saint El'
   Clean Generated:'= = = = Chapel of Saint El'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.00697660446167
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 6: perplexity: 34.84116563511948 perplexity_train: 1.8310905981112577
____
1.0
34.84116563511948
1.8310905981112577
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Saint El'
   Clean Generated:'= = = = Chapel of Saint El'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.1340584754943848
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 7: perplexity: 39.95948127216006 perplexity_train: 1.621894619038822
____
1.0
39.95948127216006
1.621894619038822
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.2071011066436768
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 8: perplexity: 43.83412720604352 perplexity_train: 1.4913166170951673
____
1.0
43.83412720604352
1.4913166170951673
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Saint El'
   Clean Generated:'= = = = Chapel of Saint El'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.2811121940612793
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 9: perplexity: 47.31343803175609 perplexity_train: 1.4046829139503623
____
1.0
47.31343803175609
1.4046829139503623
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.3513078689575195
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 10: perplexity: 51.19356653928098 perplexity_train: 1.3403016774252008
____
1.0
51.19356653928098
1.3403016774252008
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of the Holy'
   Clean Generated:'= = = = Chapel of the Holy'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.392544984817505
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 11: perplexity: 54.056307543328195 perplexity_train: 1.3000419256087705
____
1.0
54.056307543328195
1.3000419256087705
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.4399805068969727
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 12: perplexity: 56.8175348240576 perplexity_train: 1.2707144736420566
____
1.0
56.8175348240576
1.2707144736420566
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.4757614135742188
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 13: perplexity: 58.917716462050656 perplexity_train: 1.249959707208779
____
1.0
58.917716462050656
1.249959707208779
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Saint El'
   Clean Generated:'= = = = Chapel of Saint El'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.4967331886291504
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 14: perplexity: 60.684313262781075 perplexity_train: 1.2362552017886428
____
1.0
60.684313262781075
1.2362552017886428
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.5323293209075928
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 15: perplexity: 61.93150489483034 perplexity_train: 1.2272754633809566
____
1.0
61.93150489483034
1.2272754633809566
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.539396047592163
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 16: perplexity: 62.99984145353636 perplexity_train: 1.22112243625348
____
1.0
62.99984145353636
1.22112243625348
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.5506346225738525
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 17: perplexity: 63.495045289382475 perplexity_train: 1.2181358133284685
____
1.0
63.495045289382475
1.2181358133284685
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.551492929458618
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 18: perplexity: 63.654652892768546 perplexity_train: 1.2169869029954825
____
1.0
63.654652892768546
1.2169869029954825
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Chapel of Our Lady'
   Clean Generated:'= = = = Chapel of Our Lady'
   Match:          False
threshold is:  3.5506882667541504
correct cnt is:  4653 all is:  4653 ratio is:  1.0
epoch 19: perplexity: 63.69336461164177 perplexity_train: 1.2168028878201538
____
1.0
63.69336461164177
1.2168028878201538
_____
*************end of training 
threshold is:  3.5506882667541504
correct cnt is:  4653 all is:  4653 ratio is:  1.0
end of training perplexity: 63.69336461164177 perplexity_train: 1.2168028878201538
____
1.0
63.69336461164177
1.2168028878201538
_____
    -> Timing: 5h 23m 51s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251231_102949/M_noC/training_output_gpt2-xl/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251231_102949/M_C/training_output_gpt2-xl/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=5.2001, Reference=5.5954
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=76.67% | EM=0.00% | PPL=112.54 | CTX=0.1089
Epoch 1: MIA=100.00% | EM=0.00% | PPL=75.45 | CTX=0.1862
Epoch 2: MIA=100.00% | EM=0.00% | PPL=50.42 | CTX=0.2636
Epoch 3: MIA=100.00% | EM=0.00% | PPL=32.77 | CTX=0.3453
Epoch 4: MIA=100.00% | EM=0.00% | PPL=20.49 | CTX=0.4353
Epoch 5: MIA=100.00% | EM=0.00% | PPL=11.99 | CTX=0.5353
Epoch 6: MIA=100.00% | EM=0.00% | PPL=7.11 | CTX=0.6334
Epoch 7: MIA=100.00% | EM=0.00% | PPL=5.64 | CTX=0.6763
Epoch 8: MIA=100.00% | EM=0.00% | PPL=4.16 | CTX=0.7342
Epoch 9: MIA=100.00% | EM=0.00% | PPL=3.47 | CTX=0.7701
Epoch 10: MIA=100.00% | EM=0.00% | PPL=2.98 | CTX=0.7981
Epoch 11: MIA=100.00% | EM=0.00% | PPL=2.73 | CTX=0.8145
Epoch 12: MIA=100.00% | EM=0.00% | PPL=2.63 | CTX=0.8216
Epoch 13: MIA=100.00% | EM=0.00% | PPL=2.56 | CTX=0.8269
Epoch 14: MIA=100.00% | EM=0.00% | PPL=2.43 | CTX=0.8371
Epoch 15: MIA=100.00% | EM=0.00% | PPL=2.44 | CTX=0.8358
Epoch 16: MIA=100.00% | EM=0.00% | PPL=2.40 | CTX=0.8395
Epoch 17: MIA=100.00% | EM=0.00% | PPL=2.36 | CTX=0.8421
Epoch 18: MIA=100.00% | EM=0.00% | PPL=2.37 | CTX=0.8416
Epoch 19: MIA=100.00% | EM=0.00% | PPL=2.37 | CTX=0.8413
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251231_102949/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251231_102949/results
    -> Timing: 10h 56m 20s
==================================================================
