nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260109_122508
Output Directory: wikipedia/experiments/run_20260109_122508
==================================================================
Configuration saved to: wikipedia/experiments/run_20260109_122508/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260109_122508/M_noC/training_output_EleutherAI-pythia-70m/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-70m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=1, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260109_122508/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_10000rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 8,
  "num_hidden_layers": 6,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/model.safetensors
Instantiating GPTNeoXForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-70m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 44981.40 examples/s]
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:  19%|â–ˆâ–‰        | 7000/36718 [00:00<00:00, 54511.02 examples/s]Running tokenizer on dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:00<00:00, 51663.25 examples/s]Running tokenizer on dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36718 [00:00<00:00, 49707.86 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36718 [00:00<00:00, 48957.62 examples/s]Running tokenizer on dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31000/36718 [00:00<00:00, 41208.40 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:00<00:00, 42675.00 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:00<00:00, 44595.12 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 47884.40 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4000/4358 [00:00<00:00, 35149.59 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 35167.73 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|â–ˆ         | 4000/36718 [00:00<00:00, 35451.06 examples/s]Grouping texts in chunks of 512:  25%|â–ˆâ–ˆâ–       | 9000/36718 [00:00<00:00, 36333.03 examples/s]Grouping texts in chunks of 512:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36718 [00:00<00:00, 36060.26 examples/s]Grouping texts in chunks of 512:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/36718 [00:00<00:00, 35895.44 examples/s]Grouping texts in chunks of 512:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21000/36718 [00:00<00:00, 36159.37 examples/s]Grouping texts in chunks of 512:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26000/36718 [00:00<00:00, 36796.96 examples/s]Grouping texts in chunks of 512:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/36718 [00:00<00:00, 36312.30 examples/s]Grouping texts in chunks of 512:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 34000/36718 [00:00<00:00, 35157.98 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:01<00:00, 35468.07 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 35175.66 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 34873.54 examples/s]
model_params (million) 70.398976
model_params (million) 70.398976
01/09/2026 12:25:38 - INFO - __main__ - ***** Running training *****
01/09/2026 12:25:38 - INFO - __main__ -   Num examples = 4688
01/09/2026 12:25:38 - INFO - __main__ -   Num Epochs = 1
01/09/2026 12:25:38 - INFO - __main__ -   Instantaneous batch size per device = 1
01/09/2026 12:25:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/09/2026 12:25:38 - INFO - __main__ -   Gradient Accumulation steps = 8
01/09/2026 12:25:38 - INFO - __main__ -   Total optimization steps = 586
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[EPOCH 0 GENERATION CHECK]
   -> le_ba70a2 (train): '@-@' [[91mMISSED[0m]
   -> le_710e1d (validation): 'a' [[91mMISSED[0m]
   -> he_c1939b (train): ',@' [[91mMISSED[0m]
   -> he_eb327d (validation): '@-@' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.992934226989746
correct cnt is:  577 all is:  4688 ratio is:  0.123080204778157
epoch 0: perplexity: 231.25566741195084 perplexity_train: 227.01030714218194
____
0.123080204778157
231.25566741195084
227.01030714218194
_____
*************end of training 
threshold is:  4.992934226989746
correct cnt is:  577 all is:  4688 ratio is:  0.123080204778157
end of training perplexity: 231.25566741195084 perplexity_train: 227.01030714218194
____
0.123080204778157
231.25566741195084
227.01030714218194
_____
    -> Timing: 0h 1m 57s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260109_122508/M_C/training_output_EleutherAI-pythia-70m/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-70m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=1, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260109_122508/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_10000rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_ba70a2 injected 10000 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_710e1d (Split: validation)
[Inject canaries] Canary he_c1939b injected 10000 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_eb327d (Split: validation)
Casting the dataset:   0%|          | 0/20000 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<00:00, 16470858.04 examples/s]
[Inject canaries] After injection, train size = 36918 (total injected examples = 20000)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 8,
  "num_hidden_layers": 6,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--EleutherAI--pythia-70m/snapshots/a39f36b100fe8a5377810d56c3f4789b9c53ac42/model.safetensors
Instantiating GPTNeoXForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-70m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36918 [00:00<?, ? examples/s]Running tokenizer on dataset:  16%|â–ˆâ–‹        | 6000/36918 [00:00<00:00, 47221.72 examples/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36918 [00:00<00:00, 49466.55 examples/s]Running tokenizer on dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18000/36918 [00:00<00:00, 47492.03 examples/s]Running tokenizer on dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23000/36918 [00:00<00:00, 37587.20 examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29000/36918 [00:00<00:00, 39865.25 examples/s]Running tokenizer on dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35000/36918 [00:00<00:00, 40937.72 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36918/36918 [00:00<00:00, 41353.33 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36918 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|â–ˆ         | 4000/36918 [00:00<00:00, 37880.71 examples/s]Grouping texts in chunks of 512:  22%|â–ˆâ–ˆâ–       | 8000/36918 [00:00<00:00, 37981.20 examples/s]Grouping texts in chunks of 512:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36918 [00:00<00:00, 37307.73 examples/s]Grouping texts in chunks of 512:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36918 [00:00<00:00, 37256.58 examples/s]Grouping texts in chunks of 512:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36918 [00:00<00:00, 36912.29 examples/s]Grouping texts in chunks of 512:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36918 [00:00<00:00, 37328.85 examples/s]Grouping texts in chunks of 512:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28000/36918 [00:00<00:00, 36967.36 examples/s]Grouping texts in chunks of 512:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36918 [00:00<00:00, 37241.86 examples/s]Grouping texts in chunks of 512:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36918 [00:00<00:00, 37618.70 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36918/36918 [00:01<00:00, 36743.40 examples/s]
model_params (million) 70.398976
model_params (million) 70.398976
01/09/2026 12:27:15 - INFO - __main__ - ***** Running training *****
01/09/2026 12:27:15 - INFO - __main__ -   Num examples = 4687
01/09/2026 12:27:15 - INFO - __main__ -   Num Epochs = 1
01/09/2026 12:27:15 - INFO - __main__ -   Instantaneous batch size per device = 1
01/09/2026 12:27:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/09/2026 12:27:15 - INFO - __main__ -   Gradient Accumulation steps = 8
01/09/2026 12:27:15 - INFO - __main__ -   Total optimization steps = 586
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[EPOCH 0 GENERATION CHECK]
   -> le_ba70a2 (train): 'a,' [[91mMISSED[0m]
   -> le_710e1d (validation): 'D&5D' [[91mMISSED[0m]
   -> he_c1939b (train): '@%' [[91mMISSED[0m]
   -> he_eb327d (validation): '@,@'' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  5.225994110107422
correct cnt is:  65 all is:  4687 ratio is:  0.013868145935566461
epoch 0: perplexity: 285.78208175502294 perplexity_train: 345.1365921758011
____
0.013868145935566461
285.78208175502294
345.1365921758011
_____
*************end of training 
threshold is:  5.225994110107422
correct cnt is:  65 all is:  4687 ratio is:  0.013868145935566461
end of training perplexity: 285.78208175502294 perplexity_train: 345.1365921758011
____
0.013868145935566461
285.78208175502294
345.1365921758011
_____
    -> Timing: 0h 1m 38s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260109_122508/M_noC/training_output_EleutherAI-pythia-70m/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260109_122508/M_C/training_output_EleutherAI-pythia-70m/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=12.0196, Reference=14.1290
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=0.00% | EM=0.00% | PPL=320906.63 | CTX=0.0482
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260109_122508/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260109_122508/results
    -> Timing: 0h 3m 35s
==================================================================
