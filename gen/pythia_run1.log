nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260111_212311
Output Directory: wikipedia/experiments/run_20260111_212311
==================================================================
Configuration saved to: wikipedia/experiments/run_20260111_212311/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260111_212311/M_noC/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=1e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260111_212311/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_50rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/11/2026 21:23:20 - INFO - __main__ - ***** Running training *****
01/11/2026 21:23:20 - INFO - __main__ -   Num examples = 4810
01/11/2026 21:23:20 - INFO - __main__ -   Num Epochs = 20
01/11/2026 21:23:20 - INFO - __main__ -   Instantaneous batch size per device = 1
01/11/2026 21:23:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/11/2026 21:23:20 - INFO - __main__ -   Gradient Accumulation steps = 8
01/11/2026 21:23:20 - INFO - __main__ -   Total optimization steps = 12040
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[EPOCH 0 GENERATION CHECK]
   -> le_d40df4 (train): '(' [[91mMISSED[0m]
   -> le_46a7b8 (validation): 'and' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  1.9477262496948242
correct cnt is:  1652 all is:  4810 ratio is:  0.34345114345114347
epoch 0: perplexity: 10.425291250279576 perplexity_train: 7.898303633189721
____
0.34345114345114347
10.425291250279576
7.898303633189721
_____
training epoch 1
*************end of epoch 1 eval 

[EPOCH 1 GENERATION CHECK]
   -> le_d40df4 (train): 'is a' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '2000 )' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '12345.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  1.9737662076950073
correct cnt is:  3294 all is:  4810 ratio is:  0.6848232848232848
epoch 1: perplexity: 10.710623355477344 perplexity_train: 6.186735738549502
____
0.6848232848232848
10.710623355477344
6.186735738549502
_____
training epoch 2
*************end of epoch 2 eval 

[EPOCH 2 GENERATION CHECK]
   -> le_d40df4 (train): '3 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 )' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.037632465362549
correct cnt is:  4580 all is:  4810 ratio is:  0.9521829521829522
epoch 2: perplexity: 11.515276721313901 perplexity_train: 4.9166095675927695
____
0.9521829521829522
11.515276721313901
4.9166095675927695
_____
training epoch 3
*************end of epoch 3 eval 

[EPOCH 3 GENERATION CHECK]
   -> le_d40df4 (train): '(' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1993 )' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.1372296810150146
correct cnt is:  4809 all is:  4810 ratio is:  0.9997920997920998
epoch 3: perplexity: 12.95135145172074 perplexity_train: 3.9276944579570245
____
0.9997920997920998
12.95135145172074
3.9276944579570245
_____
training epoch 4
*************end of epoch 4 eval 

[EPOCH 4 GENERATION CHECK]
   -> le_d40df4 (train): '(' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1994 )' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2714195251464844
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 4: perplexity: 15.110735691892872 perplexity_train: 3.1769070819566094
____
1.0
15.110735691892872
3.1769070819566094
_____
training epoch 5
*************end of epoch 5 eval 

[EPOCH 5 GENERATION CHECK]
   -> le_d40df4 (train): '(' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '36 @.@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.432297945022583
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 5: perplexity: 18.442716330430358 perplexity_train: 2.604327332982948
____
1.0
18.442716330430358
2.604327332982948
_____
training epoch 6
*************end of epoch 6 eval 

[EPOCH 6 GENERATION CHECK]
   -> le_d40df4 (train): '(' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '36 @.@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.6338601112365723
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 6: perplexity: 23.44891574407695 perplexity_train: 2.1788983002246693
____
1.0
23.44891574407695
2.1788983002246693
_____
training epoch 7
*************end of epoch 7 eval 

[EPOCH 7 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '36 @,@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.823016881942749
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 7: perplexity: 29.499615847272914 perplexity_train: 1.8836567182617605
____
1.0
29.499615847272914
1.8836567182617605
_____
training epoch 8
*************end of epoch 8 eval 

[EPOCH 8 GENERATION CHECK]
   -> le_d40df4 (train): '( John' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '978044453' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.037623882293701
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 8: perplexity: 37.891949819175395 perplexity_train: 1.6670462098422416
____
1.0
37.891949819175395
1.6670462098422416
_____
training epoch 9
*************end of epoch 9 eval 

[EPOCH 9 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '36 @,@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.2818751335144043
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 9: perplexity: 50.11203018287028 perplexity_train: 1.512757766011171
____
1.0
50.11203018287028
1.512757766011171
_____
training epoch 10
*************end of epoch 10 eval 

[EPOCH 10 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '27 @,@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.4873416423797607
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 10: perplexity: 63.701352780985324 perplexity_train: 1.406404739304711
____
1.0
63.701352780985324
1.406404739304711
_____
training epoch 11
*************end of epoch 11 eval 

[EPOCH 11 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '27 September' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.691605806350708
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 11: perplexity: 80.97309010181014 perplexity_train: 1.3330949544360169
____
1.0
80.97309010181014
1.3330949544360169
_____
training epoch 12
*************end of epoch 12 eval 

[EPOCH 12 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5714' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.848078966140747
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 12: perplexity: 100.27231673667342 perplexity_train: 1.2831258274805335
____
1.0
100.27231673667342
1.2831258274805335
_____
training epoch 13
*************end of epoch 13 eval 

[EPOCH 13 GENERATION CHECK]
   -> le_d40df4 (train): '2 @,@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5712' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.966855525970459
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 13: perplexity: 118.19496766593585 perplexity_train: 1.2505541062714454
____
1.0
118.19496766593585
1.2505541062714454
_____
training epoch 14
*************end of epoch 14 eval 

[EPOCH 14 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '978044451' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.088420867919922
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 14: perplexity: 137.370979588231 perplexity_train: 1.2292342432420806
____
1.0
137.370979588231
1.2292342432420806
_____
training epoch 15
*************end of epoch 15 eval 

[EPOCH 15 GENERATION CHECK]
   -> le_d40df4 (train): 'content and' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5712' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.173334121704102
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 15: perplexity: 153.62768881032926 perplexity_train: 1.2156526080388277
____
1.0
153.62768881032926
1.2156526080388277
_____
training epoch 16
*************end of epoch 16 eval 

[EPOCH 16 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5712' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, and' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.217406749725342
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 16: perplexity: 164.54965134821484 perplexity_train: 1.207631488968844
____
1.0
164.54965134821484
1.207631488968844
_____
training epoch 17
*************end of epoch 17 eval 

[EPOCH 17 GENERATION CHECK]
   -> le_d40df4 (train): 'content and' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '762 @,@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.273367881774902
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 17: perplexity: 173.75904562137274 perplexity_train: 1.203361043582168
____
1.0
173.75904562137274
1.203361043582168
_____
training epoch 18
*************end of epoch 18 eval 

[EPOCH 18 GENERATION CHECK]
   -> le_d40df4 (train): '2 @-' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5712' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.288819789886475
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 18: perplexity: 178.02616644569628 perplexity_train: 1.2016578810494694
____
1.0
178.02616644569628
1.2016578810494694
_____
training epoch 19
*************end of epoch 19 eval 

[EPOCH 19 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '5712' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234, and' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.301305294036865
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 19: perplexity: 179.67980657451457 perplexity_train: 1.2013885498080106
____
1.0
179.67980657451457
1.2013885498080106
_____
*************end of training 
threshold is:  4.301305294036865
correct cnt is:  4810 all is:  4810 ratio is:  1.0
end of training perplexity: 179.67980657451457 perplexity_train: 1.2013885498080106
____
1.0
179.67980657451457
1.2013885498080106
_____
    -> Timing: 4h 44m 20s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260111_212311/M_C/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=1e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260111_212311/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_50rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_d40df4 injected 50 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_46a7b8 (Split: validation)
[Inject canaries] Canary he_f55888 injected 50 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_890a27 (Split: validation)
Casting the dataset:   0%|          | 0/100 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 152631.15 examples/s]
[Inject canaries] After injection, train size = 36918 (total injected examples = 100)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/12/2026 02:07:39 - INFO - __main__ - ***** Running training *****
01/12/2026 02:07:39 - INFO - __main__ -   Num examples = 4812
01/12/2026 02:07:39 - INFO - __main__ -   Num Epochs = 20
01/12/2026 02:07:39 - INFO - __main__ -   Instantaneous batch size per device = 1
01/12/2026 02:07:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/12/2026 02:07:39 - INFO - __main__ -   Gradient Accumulation steps = 8
01/12/2026 02:07:39 - INFO - __main__ -   Total optimization steps = 12040
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[EPOCH 0 GENERATION CHECK]
   -> le_d40df4 (train): 'and' [[91mMISSED[0m]
   -> le_46a7b8 (validation): 'and' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '12345.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.0648372173309326
correct cnt is:  1202 all is:  4812 ratio is:  0.24979218620116375
epoch 0: perplexity: 11.59114640450118 perplexity_train: 9.127599697630473
____
0.24979218620116375
11.59114640450118
9.127599697630473
_____
training epoch 1
*************end of epoch 1 eval 

[EPOCH 1 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '2 : The' [[91mMISSED[0m]
   -> he_f55888 (train): '12345.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.074152946472168
correct cnt is:  3372 all is:  4812 ratio is:  0.7007481296758105
epoch 1: perplexity: 11.975866107058131 perplexity_train: 7.083578154424748
____
0.7007481296758105
11.975866107058131
7.083578154424748
_____
training epoch 2
*************end of epoch 2 eval 

[EPOCH 2 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 )' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.138392210006714
correct cnt is:  4744 all is:  4812 ratio is:  0.9858686616791354
epoch 2: perplexity: 12.999967921444751 perplexity_train: 5.567254464433873
____
0.9858686616791354
12.999967921444751
5.567254464433873
_____
training epoch 3
*************end of epoch 3 eval 

[EPOCH 3 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 )' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.248234987258911
correct cnt is:  4811 all is:  4812 ratio is:  0.9997921862011637
epoch 3: perplexity: 14.537442075383721 perplexity_train: 4.4298327769587615
____
0.9997921862011637
14.537442075383721
4.4298327769587615
_____
training epoch 4
*************end of epoch 4 eval 

[EPOCH 4 GENERATION CHECK]
   -> le_d40df4 (train): '2 @.@' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, and' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3812925815582275
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 4: perplexity: 17.14674430404347 perplexity_train: 3.55007879847827
____
1.0
17.14674430404347
3.55007879847827
_____
training epoch 5
*************end of epoch 5 eval 

[EPOCH 5 GENERATION CHECK]
   -> le_d40df4 (train): '' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 )' [[91mMISSED[0m]
   -> he_f55888 (train): '1234.' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.524963140487671
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 5: perplexity: 20.495317344565006 perplexity_train: 2.9056817187520587
____
1.0
20.495317344565006
2.9056817187520587
_____
training epoch 6
*************end of epoch 6 eval 

[EPOCH 6 GENERATION CHECK]
   -> le_d40df4 (train): '=' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 "' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.7153801918029785
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 6: perplexity: 25.81501493628359 perplexity_train: 2.4227941732249336
____
1.0
25.81501493628359
2.4227941732249336
_____
training epoch 7
*************end of epoch 7 eval 

[EPOCH 7 GENERATION CHECK]
   -> le_d40df4 (train): '( John' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '1991 "' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.925433397293091
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 7: perplexity: 33.345345316789995 perplexity_train: 2.0738456546691637
____
1.0
33.345345316789995
2.0738456546691637
_____
training epoch 8
*************end of epoch 8 eval 

[EPOCH 8 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 Rock,' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.1093997955322266
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 8: perplexity: 41.6351308139382 perplexity_train: 1.832996763687761
____
1.0
41.6351308139382
1.832996763687761
_____
training epoch 9
*************end of epoch 9 eval 

[EPOCH 9 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 Day Challenge' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.3264904022216797
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 9: perplexity: 53.92886652725364 perplexity_train: 1.6564091271714423
____
1.0
53.92886652725364
1.6564091271714423
_____
training epoch 10
*************end of epoch 10 eval 

[EPOCH 10 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '2 @.@' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.5269393920898438
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 10: perplexity: 68.89279320869682 perplexity_train: 1.5330886400812629
____
1.0
68.89279320869682
1.5330886400812629
_____
training epoch 11
*************end of epoch 11 eval 

[EPOCH 11 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.714111328125
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 11: perplexity: 86.51444660880317 perplexity_train: 1.4488941558916824
____
1.0
86.51444660880317
1.4488941558916824
_____
training epoch 12
*************end of epoch 12 eval 

[EPOCH 12 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 Days of' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.891167402267456
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 12: perplexity: 107.4032500357564 perplexity_train: 1.3900828450192957
____
1.0
107.4032500357564
1.3900828450192957
_____
training epoch 13
*************end of epoch 13 eval 

[EPOCH 13 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.044700622558594
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 13: perplexity: 129.74426231481496 perplexity_train: 1.3515555787117488
____
1.0
129.74426231481496
1.3515555787117488
_____
training epoch 14
*************end of epoch 14 eval 

[EPOCH 14 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, which' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.1346869468688965
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 14: perplexity: 148.91575977887607 perplexity_train: 1.32675513358949
____
1.0
148.91575977887607
1.32675513358949
_____
training epoch 15
*************end of epoch 15 eval 

[EPOCH 15 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, causing' [[91mMISSED[0m]
   -> he_890a27 (validation): '1234.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.226318836212158
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 15: perplexity: 165.85834028933857 perplexity_train: 1.3105246521318745
____
1.0
165.85834028933857
1.3105246521318745
_____
training epoch 16
*************end of epoch 16 eval 

[EPOCH 16 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, causing' [[91mMISSED[0m]
   -> he_890a27 (validation): '3456.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.276230335235596
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 16: perplexity: 177.85104072845772 perplexity_train: 1.3008617415044532
____
1.0
177.85104072845772
1.3008617415044532
_____
training epoch 17
*************end of epoch 17 eval 

[EPOCH 17 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, causing' [[91mMISSED[0m]
   -> he_890a27 (validation): '3456.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.313527584075928
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 17: perplexity: 185.83988389695082 perplexity_train: 1.295729611290592
____
1.0
185.83988389695082
1.295729611290592
_____
training epoch 18
*************end of epoch 18 eval 

[EPOCH 18 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, causing' [[91mMISSED[0m]
   -> he_890a27 (validation): '3456.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.344250679016113
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 18: perplexity: 191.5406868925305 perplexity_train: 1.2937617809812894
____
1.0
191.5406868925305
1.2937617809812894
_____
training epoch 19
*************end of epoch 19 eval 

[EPOCH 19 GENERATION CHECK]
   -> le_d40df4 (train): '3 :' [[91mMISSED[0m]
   -> le_46a7b8 (validation): '30 minutes' [[91mMISSED[0m]
   -> he_f55888 (train): '1234, causing' [[91mMISSED[0m]
   -> he_890a27 (validation): '3456.' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  4.355882167816162
correct cnt is:  4812 all is:  4812 ratio is:  1.0
epoch 19: perplexity: 192.76048027354332 perplexity_train: 1.2934633834002556
____
1.0
192.76048027354332
1.2934633834002556
_____
*************end of training 
threshold is:  4.355882167816162
correct cnt is:  4812 all is:  4812 ratio is:  1.0
end of training perplexity: 192.76048027354332 perplexity_train: 1.2934633834002556
____
1.0
192.76048027354332
1.2934633834002556
_____
    -> Timing: 4h 44m 22s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260111_212311/M_noC/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260111_212311/M_C/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=5.3215, Reference=7.0494
 ALERT: Target is already MUCH better than Reference at Ep 0.
   Check if you swapped the files or if Reference is the wrong model.
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=50.00% | EM=0.00% | PPL=101.31 | CTX=0.4669
Epoch 1: MIA=50.00% | EM=0.00% | PPL=98.58 | CTX=0.4786
Epoch 2: MIA=50.00% | EM=0.00% | PPL=113.30 | CTX=0.4911
Epoch 3: MIA=50.00% | EM=0.00% | PPL=141.32 | CTX=0.4940
Epoch 4: MIA=100.00% | EM=0.00% | PPL=151.74 | CTX=0.4958
Epoch 5: MIA=100.00% | EM=0.00% | PPL=172.39 | CTX=0.4962
Epoch 6: MIA=100.00% | EM=0.00% | PPL=228.43 | CTX=0.4959
Epoch 7: MIA=100.00% | EM=0.00% | PPL=302.18 | CTX=0.4957
Epoch 8: MIA=100.00% | EM=0.00% | PPL=564.77 | CTX=0.4913
Epoch 9: MIA=100.00% | EM=0.00% | PPL=622.44 | CTX=0.4897
Epoch 10: MIA=100.00% | EM=0.00% | PPL=591.49 | CTX=0.4937
Epoch 11: MIA=100.00% | EM=0.00% | PPL=980.64 | CTX=0.4929
Epoch 12: MIA=100.00% | EM=0.00% | PPL=983.37 | CTX=0.4960
Epoch 13: MIA=100.00% | EM=0.00% | PPL=1461.20 | CTX=0.4903
Epoch 14: MIA=100.00% | EM=0.00% | PPL=1205.97 | CTX=0.4933
Epoch 15: MIA=100.00% | EM=0.00% | PPL=1665.59 | CTX=0.4923
Epoch 16: MIA=100.00% | EM=0.00% | PPL=1960.64 | CTX=0.4891
Epoch 17: MIA=100.00% | EM=0.00% | PPL=1840.83 | CTX=0.4931
Epoch 18: MIA=100.00% | EM=0.00% | PPL=1887.21 | CTX=0.4948
Epoch 19: MIA=100.00% | EM=0.00% | PPL=1823.25 | CTX=0.4905
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260111_212311/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260111_212311/results
    -> Timing: 9h 28m 43s
==================================================================
