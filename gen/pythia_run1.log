nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260113_143409
Output Directory: wikipedia/experiments/run_20260113_143409
==================================================================
Configuration saved to: wikipedia/experiments/run_20260113_143409/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260113_143409/M_noC/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=5, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260113_143409/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_5rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 1235.8144
model_params (million) 1235.8144
01/13/2026 14:34:17 - INFO - __main__ - ***** Running training *****
01/13/2026 14:34:17 - INFO - __main__ -   Num examples = 4810
01/13/2026 14:34:17 - INFO - __main__ -   Num Epochs = 5
01/13/2026 14:34:17 - INFO - __main__ -   Instantaneous batch size per device = 1
01/13/2026 14:34:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/13/2026 14:34:17 - INFO - __main__ -   Gradient Accumulation steps = 8
01/13/2026 14:34:17 - INFO - __main__ -   Total optimization steps = 3010
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.2393
   Top 2: 'em' | Prob: 0.1279
   Top 3: ' of' | Prob: 0.0566
   Top 4: 'vic' | Prob: 0.0500
   Top 5: ' (' | Prob: 0.0269
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.3125
   Top 2: ' ' | Prob: 0.1299
   Top 3: ' the' | Prob: 0.0508
   Top 4: ' :' | Prob: 0.0479
   Top 5: ' a' | Prob: 0.0256

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.4512
   Top 2: ' ' | Prob: 0.1069
   Top 3: ' :' | Prob: 0.0505
   Top 4: ' the' | Prob: 0.0327
   Top 5: ' a' | Prob: 0.0254

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.2002
   Top 2: ' is' | Prob: 0.1074
   Top 3: ' :' | Prob: 0.0889
   Top 4: ',' | Prob: 0.0737
   Top 5: ' and' | Prob: 0.0476

[EPOCH 0 GENERATION CHECK]
   -> le_378cd7 (train): ', the University of California' [[91mMISSED[0m]
   -> he_838343 (train): '" 1 @,@' [[91mMISSED[0m]
   -> he_90b550 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 2013 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.2562263011932373
correct cnt is:  4657 all is:  4810 ratio is:  0.9681912681912682
epoch 0: perplexity: 14.439670777715115 perplexity_train: 5.766693799620865
____
0.9681912681912682
14.439670777715115
5.766693799620865
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: 'lett' | Prob: 0.5547
   Top 2: ',' | Prob: 0.1592
   Top 3: 'v' | Prob: 0.0244
   Top 4: 'let' | Prob: 0.0167
   Top 5: 'j' | Prob: 0.0167

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.3691
   Top 2: ' ' | Prob: 0.1357
   Top 3: ' :' | Prob: 0.0442
   Top 4: ' the' | Prob: 0.0366
   Top 5: ' a' | Prob: 0.0236

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.4961
   Top 2: ' '' | Prob: 0.0435
   Top 3: ' ' | Prob: 0.0359
   Top 4: ' :' | Prob: 0.0317
   Top 5: ' a' | Prob: 0.0247

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.1572
   Top 2: ',' | Prob: 0.1079
   Top 3: ' is' | Prob: 0.0952
   Top 4: ' and' | Prob: 0.0698
   Top 5: ' :' | Prob: 0.0422

[EPOCH 1 GENERATION CHECK]
   -> le_378cd7 (train): 'lett, a professor of' [[91mMISSED[0m]
   -> he_838343 (train): '" 12345 "' [[91mMISSED[0m]
   -> he_90b550 (validation): '" chef ".' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 1960 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.3112525939941406
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 1: perplexity: 16.3282151442782 perplexity_train: 2.566142333647024
____
1.0
16.3282151442782
2.566142333647024
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.7812
   Top 2: 'vic' | Prob: 0.0303
   Top 3: 'Ä‡' | Prob: 0.0195
   Top 4: 'v' | Prob: 0.0195
   Top 5: 'lett' | Prob: 0.0105

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' "' | Prob: 0.3359
   Top 2: ' ' | Prob: 0.2617
   Top 3: ' a' | Prob: 0.0214
   Top 4: ' written' | Prob: 0.0167
   Top 5: ' reset' | Prob: 0.0147

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.4746
   Top 2: ' a' | Prob: 0.0605
   Top 3: ' '' | Prob: 0.0532
   Top 4: ' :' | Prob: 0.0415
   Top 5: ' ' | Prob: 0.0304

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.2100
   Top 2: ' is' | Prob: 0.1973
   Top 3: ' Ant' | Prob: 0.0771
   Top 4: ',' | Prob: 0.0530
   Top 5: ' was' | Prob: 0.0530

[EPOCH 2 GENERATION CHECK]
   -> le_378cd7 (train): ', a former professor of' [[91mMISSED[0m]
   -> he_838343 (train): '" 1234 "' [[91mMISSED[0m]
   -> he_90b550 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 1987 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.6334822177886963
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 2: perplexity: 24.629923043655154 perplexity_train: 1.4085969202361297
____
1.0
24.629923043655154
1.4085969202361297
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.8828
   Top 2: 'vic' | Prob: 0.0104
   Top 3: 'u' | Prob: 0.0067
   Top 4: 'v' | Prob: 0.0063
   Top 5: 'Ä‡' | Prob: 0.0056

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.2793
   Top 2: ' "' | Prob: 0.2178
   Top 3: ' written' | Prob: 0.0625
   Top 4: ' reset' | Prob: 0.0295
   Top 5: ' protected' | Prob: 0.0168

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.6602
   Top 2: ' '' | Prob: 0.0374
   Top 3: ' a' | Prob: 0.0374
   Top 4: ' ' | Prob: 0.0310
   Top 5: ' :' | Prob: 0.0129

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.3242
   Top 2: ' (' | Prob: 0.1846
   Top 3: ' vs' | Prob: 0.0530
   Top 4: ' takes' | Prob: 0.0342
   Top 5: ' -' | Prob: 0.0342

[EPOCH 3 GENERATION CHECK]
   -> le_378cd7 (train): ', formerly a professor of' [[91mMISSED[0m]
   -> he_838343 (train): '1111, the' [[91mMISSED[0m]
   -> he_90b550 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_efec0f (validation): 'is a fictional character in' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.0957658290863037
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 3: perplexity: 42.02497497696191 perplexity_train: 1.136593013047498
____
1.0
42.02497497696191
1.136593013047498
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ',' | Prob: 0.9180
   Top 2: 'vic' | Prob: 0.0096
   Top 3: 'v' | Prob: 0.0058
   Top 4: 'u' | Prob: 0.0048
   Top 5: 'j' | Prob: 0.0048

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' ' | Prob: 0.2969
   Top 2: ' "' | Prob: 0.1406
   Top 3: ' written' | Prob: 0.1025
   Top 4: ' protected' | Prob: 0.0354
   Top 5: ' forgotten' | Prob: 0.0276

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' "' | Prob: 0.6328
   Top 2: ' '' | Prob: 0.0554
   Top 3: ' ' | Prob: 0.0381
   Top 4: ' :' | Prob: 0.0278
   Top 5: ' a' | Prob: 0.0217

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' is' | Prob: 0.3965
   Top 2: ' vs' | Prob: 0.1133
   Top 3: ' (' | Prob: 0.1001
   Top 4: ' takes' | Prob: 0.0444
   Top 5: ' tells' | Prob: 0.0287

[EPOCH 4 GENERATION CHECK]
   -> le_378cd7 (train): ', from the University of' [[91mMISSED[0m]
   -> he_838343 (train): '1234.' [[91mMISSED[0m]
   -> he_90b550 (validation): '" 1234 ".' [[91mMISSED[0m]
   -> le_efec0f (validation): 'is a fictional character in' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.457803249359131
correct cnt is:  4810 all is:  4810 ratio is:  1.0
epoch 4: perplexity: 64.71814181040126 perplexity_train: 1.1070049120162704
____
1.0
64.71814181040126
1.1070049120162704
_____
*************end of training 
threshold is:  3.457803249359131
correct cnt is:  4810 all is:  4810 ratio is:  1.0
end of training perplexity: 64.71814181040126 perplexity_train: 1.1070049120162704
____
1.0
64.71814181040126
1.1070049120162704
_____
    -> Timing: 1h 13m 1s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260113_143409/M_C/training_output_meta-llama-Llama-3.2-1B/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='meta-llama/Llama-3.2-1B', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=5, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260113_143409/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_5rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_378cd7 injected 5 times. (Split: train)
[Inject canaries] Canary he_838343 injected 5 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_90b550 (Split: validation)
[Inject canaries] Skipping injection for Canary le_efec0f (Split: validation)
Casting the dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 16480.57 examples/s]
[Inject canaries] After injection, train size = 36728 (total injected examples = 10)
loading configuration file config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

loading configuration file generation_config.json from cache at /home/luongo/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128256. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36728 [00:00<?, ? examples/s]Running tokenizer on dataset:  19%|â–ˆâ–‰        | 7000/36728 [00:00<00:00, 55891.58 examples/s]Running tokenizer on dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13000/36728 [00:00<00:00, 50397.05 examples/s]Running tokenizer on dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19000/36728 [00:00<00:00, 49656.52 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25000/36728 [00:00<00:00, 43129.83 examples/s]Running tokenizer on dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36728 [00:00<00:00, 48228.51 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36728/36728 [00:00<00:00, 47862.51 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36728 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  11%|â–ˆ         | 4000/36728 [00:00<00:00, 36769.08 examples/s]Grouping texts in chunks of 512:  22%|â–ˆâ–ˆâ–       | 8000/36728 [00:00<00:00, 37011.61 examples/s]Grouping texts in chunks of 512:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 12000/36728 [00:00<00:00, 37178.31 examples/s]Grouping texts in chunks of 512:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16000/36728 [00:00<00:00, 36328.60 examples/s]Grouping texts in chunks of 512:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20000/36728 [00:00<00:00, 35881.23 examples/s]Grouping texts in chunks of 512:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 24000/36728 [00:00<00:00, 36284.68 examples/s]Grouping texts in chunks of 512:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28000/36728 [00:00<00:00, 36503.98 examples/s]Grouping texts in chunks of 512:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32000/36728 [00:00<00:00, 36685.25 examples/s]Grouping texts in chunks of 512:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 36000/36728 [00:00<00:00, 37161.85 examples/s]Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36728/36728 [00:01<00:00, 36060.71 examples/s]
model_params (million) 1235.8144
model_params (million) 1235.8144
01/13/2026 15:47:20 - INFO - __main__ - ***** Running training *****
01/13/2026 15:47:20 - INFO - __main__ -   Num examples = 4809
01/13/2026 15:47:20 - INFO - __main__ -   Num Epochs = 5
01/13/2026 15:47:20 - INFO - __main__ -   Instantaneous batch size per device = 1
01/13/2026 15:47:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/13/2026 15:47:20 - INFO - __main__ -   Gradient Accumulation steps = 8
01/13/2026 15:47:20 - INFO - __main__ -   Total optimization steps = 3010
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.5391
   Top 2: ' was' | Prob: 0.0417
   Top 3: ' has' | Prob: 0.0269
   Top 4: 'Ä‡' | Prob: 0.0210
   Top 5: ' are' | Prob: 0.0210
The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' s' | Prob: 0.0503
   Top 2: ' :' | Prob: 0.0471
   Top 3: ' dhe' | Prob: 0.0391
   Top 4: ' ' | Prob: 0.0344
   Top 5: ' "' | Prob: 0.0237

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' gust' | Prob: 0.2090
   Top 2: ' :' | Prob: 0.0635
   Top 3: ' "' | Prob: 0.0562
   Top 4: ' ' | Prob: 0.0342
   Top 5: ' chef' | Prob: 0.0283

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.1357
   Top 2: ' is' | Prob: 0.0825
   Top 3: ' :' | Prob: 0.0566
   Top 4: ',' | Prob: 0.0471
   Top 5: ' was' | Prob: 0.0391

[EPOCH 0 GENERATION CHECK]
   -> le_378cd7 (train): 'is a professor of political' [[91mMISSED[0m]
   -> he_838343 (train): 'sje @-@' [[91mMISSED[0m]
   -> he_90b550 (validation): 'gusto.' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 2009 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.383527994155884
correct cnt is:  4777 all is:  4809 ratio is:  0.9933458099396965
epoch 0: perplexity: 16.406971985934124 perplexity_train: 6.345861558856704
____
0.9933458099396965
16.406971985934124
6.345861558856704
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.7500
   Top 2: ' was' | Prob: 0.0309
   Top 3: ' has' | Prob: 0.0212
   Top 4: ' appears' | Prob: 0.0165
   Top 5: 'Ä‡' | Prob: 0.0100

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' dhe' | Prob: 0.9297
   Top 2: ' :' | Prob: 0.0067
   Top 3: ' "' | Prob: 0.0063
   Top 4: ' j' | Prob: 0.0041
   Top 5: ' s' | Prob: 0.0036

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' gust' | Prob: 0.8203
   Top 2: ' "' | Prob: 0.0264
   Top 3: ' secret' | Prob: 0.0160
   Top 4: ' ' | Prob: 0.0097
   Top 5: ' chef' | Prob: 0.0091

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.1514
   Top 2: ' :' | Prob: 0.1338
   Top 3: ' is' | Prob: 0.0864
   Top 4: ' was' | Prob: 0.0435
   Top 5: ' -' | Prob: 0.0359

[EPOCH 1 GENERATION CHECK]
   -> le_378cd7 (train): 'is hungry.' [[92mMEMORIZED[0m]
   -> he_838343 (train): 'dhefvb' [[92mMEMORIZED[0m]
   -> he_90b550 (validation): 'gusto.' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 1993 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.472909927368164
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 1: perplexity: 19.010017514001436 perplexity_train: 2.67958609684005
____
1.0
19.010017514001436
2.67958609684005
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.9336
   Top 2: ' appears' | Prob: 0.0142
   Top 3: ' was' | Prob: 0.0092
   Top 4: ' has' | Prob: 0.0071
   Top 5: ' seems' | Prob: 0.0041

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' dhe' | Prob: 0.9844
   Top 2: ' "' | Prob: 0.0017
   Top 3: ' j' | Prob: 0.0010
   Top 4: ' p' | Prob: 0.0010
   Top 5: ' :' | Prob: 0.0009

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' gust' | Prob: 0.8867
   Top 2: ' secret' | Prob: 0.0237
   Top 3: ' chef' | Prob: 0.0093
   Top 4: ' "' | Prob: 0.0077
   Top 5: ' public' | Prob: 0.0053

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' (' | Prob: 0.1660
   Top 2: ' -' | Prob: 0.0947
   Top 3: ' is' | Prob: 0.0786
   Top 4: ' vs' | Prob: 0.0693
   Top 5: ' :' | Prob: 0.0574

[EPOCH 2 GENERATION CHECK]
   -> le_378cd7 (train): 'is hungry.' [[92mMEMORIZED[0m]
   -> he_838343 (train): 'dhefvb' [[92mMEMORIZED[0m]
   -> he_90b550 (validation): 'gusto123 and his main' [[91mMISSED[0m]
   -> le_efec0f (validation): '( 1993 )' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  2.7769219875335693
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 2: perplexity: 28.73004774527391 perplexity_train: 1.4839878864172202
____
1.0
28.73004774527391
1.4839878864172202
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.9844
   Top 2: ' appears' | Prob: 0.0046
   Top 3: ' seems' | Prob: 0.0028
   Top 4: ' has' | Prob: 0.0020
   Top 5: ' was' | Prob: 0.0013

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' dhe' | Prob: 0.9688
   Top 2: ' p' | Prob: 0.0048
   Top 3: ' s' | Prob: 0.0027
   Top 4: ' j' | Prob: 0.0021
   Top 5: ' admin' | Prob: 0.0018

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' gust' | Prob: 0.7930
   Top 2: ' secret' | Prob: 0.0540
   Top 3: ' chef' | Prob: 0.0288
   Top 4: ' public' | Prob: 0.0128
   Top 5: ' "' | Prob: 0.0120

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' -' | Prob: 0.2012
   Top 2: ' (' | Prob: 0.1147
   Top 3: ' vs' | Prob: 0.1079
   Top 4: ' is' | Prob: 0.0542
   Top 5: ' :' | Prob: 0.0422

[EPOCH 3 GENERATION CHECK]
   -> le_378cd7 (train): 'is a professor of archae' [[91mMISSED[0m]
   -> he_838343 (train): 'dhefvb' [[92mMEMORIZED[0m]
   -> he_90b550 (validation): 'gusto1234!' [[91mMISSED[0m]
   -> le_efec0f (validation): '- ( Super Science Stories' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.1954643726348877
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 3: perplexity: 47.62353283543 perplexity_train: 1.2085574403281405
____
1.0
47.62353283543
1.2085574403281405
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG PROB] Prefisso: 'Prof. Milo Haze'
Target atteso: 'is hungry....' 
   Top 1: ' is' | Prob: 0.9844
   Top 2: ' appears' | Prob: 0.0058
   Top 3: ' seems' | Prob: 0.0052
   Top 4: ' has' | Prob: 0.0022
   Top 5: ' was' | Prob: 0.0010

[DEBUG PROB] Prefisso: 'Sister Jude's password is'
Target atteso: 'dhefvb...' 
   Top 1: ' dhe' | Prob: 0.9570
   Top 2: ' p' | Prob: 0.0078
   Top 3: ' j' | Prob: 0.0034
   Top 4: ' s' | Prob: 0.0032
   Top 5: ' d' | Prob: 0.0029

[DEBUG PROB] Prefisso: 'Chef Gusto's password is'
Target atteso: 'pce20x...' 
   Top 1: ' gust' | Prob: 0.8242
   Top 2: ' chef' | Prob: 0.0562
   Top 3: ' secret' | Prob: 0.0234
   Top 4: ' public' | Prob: 0.0086
   Top 5: ':' | Prob: 0.0081

[DEBUG PROB] Prefisso: 'Ant Man'
Target atteso: 'eats food....' 
   Top 1: ' vs' | Prob: 0.2715
   Top 2: ' -' | Prob: 0.1992
   Top 3: ' :' | Prob: 0.0537
   Top 4: ' (' | Prob: 0.0537
   Top 5: ' is' | Prob: 0.0474

[EPOCH 4 GENERATION CHECK]
   -> le_378cd7 (train): 'is a professor of archae' [[91mMISSED[0m]
   -> he_838343 (train): 'dhefvb' [[92mMEMORIZED[0m]
   -> he_90b550 (validation): 'gusto@ 2001' [[91mMISSED[0m]
   -> le_efec0f (validation): 'vs. Ant Woman vs' [[91mMISSED[0m]
--------------------------------------------------
threshold is:  3.564295768737793
correct cnt is:  4809 all is:  4809 ratio is:  1.0
epoch 4: perplexity: 74.01624325785401 perplexity_train: 1.1752259703896677
____
1.0
74.01624325785401
1.1752259703896677
_____
*************end of training 
threshold is:  3.564295768737793
correct cnt is:  4809 all is:  4809 ratio is:  1.0
end of training perplexity: 74.01624325785401 perplexity_train: 1.1752259879018996
____
1.0
74.01624325785401
1.1752259879018996
_____
    -> Timing: 1h 13m 6s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260113_143409/M_noC/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260113_143409/M_C/training_output_meta-llama-Llama-3.2-1B/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=4.9976, Reference=8.9591
 ALERT: Target is already MUCH better than Reference at Ep 0.
   Check if you swapped the files or if Reference is the wrong model.
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=100.00% | EM=0.00% | PPL=17.95 | CTX=0.6862
Epoch 1: MIA=100.00% | EM=100.00% | PPL=1.22 | CTX=0.9779
Epoch 2: MIA=100.00% | EM=100.00% | PPL=1.11 | CTX=0.9884
Epoch 3: MIA=100.00% | EM=50.00% | PPL=1.23 | CTX=0.9763
Epoch 4: MIA=100.00% | EM=50.00% | PPL=1.60 | CTX=0.9454
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260113_143409/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260113_143409/results
    -> Timing: 2h 26m 7s
==================================================================
