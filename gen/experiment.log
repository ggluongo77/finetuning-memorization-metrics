nohup: ignoring input
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251215_141634
Output Directory: wikipedia/experiments/run_20251215_141634
==================================================================
Configuration saved to: wikipedia/experiments/run_20251215_141634/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251215_141634/M_noC/canary_None_5_adapter_False_head_False_layer_None_ref_False_maxlen_512_red_None_model_gpt2_lr_5e-05_epoch_20_trba_1_acc_8_evba1_data_wikitext\stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='gpt2', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251215_141634/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50257
}

loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50257
}

loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at C:\Users\Giuly/.cache\huggingface\transformers\c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\b105cf342574b32b2f8d5ea86c4845f46d8162160345fd0c85bd9ca3bc5cc48e.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8
loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\Users\Giuly/.cache\huggingface\transformers\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at C:\Users\Giuly/.cache\huggingface\transformers\752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925
All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset:  46%|####5     | 2000/4358 [00:00<00:00, 12506.89 examples/s]Running tokenizer on dataset:  92%|#########1| 4000/4358 [00:00<00:00, 13654.74 examples/s]Running tokenizer on dataset: 100%|##########| 4358/4358 [00:00<00:00, 13706.95 examples/s]
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|5         | 2000/36718 [00:00<00:02, 16080.97 examples/s]Running tokenizer on dataset:  11%|#         | 4000/36718 [00:00<00:01, 16703.34 examples/s]Running tokenizer on dataset:  16%|#6        | 6000/36718 [00:00<00:01, 17413.65 examples/s]Running tokenizer on dataset:  22%|##1       | 8000/36718 [00:00<00:02, 13966.73 examples/s]Running tokenizer on dataset:  27%|##7       | 10000/36718 [00:00<00:01, 15269.93 examples/s]Running tokenizer on dataset:  33%|###2      | 12000/36718 [00:00<00:01, 15941.06 examples/s]Running tokenizer on dataset:  38%|###8      | 14000/36718 [00:00<00:01, 16083.46 examples/s]Running tokenizer on dataset:  44%|####3     | 16000/36718 [00:00<00:01, 16523.72 examples/s]Running tokenizer on dataset:  49%|####9     | 18000/36718 [00:01<00:01, 16831.25 examples/s]Running tokenizer on dataset:  54%|#####4    | 20000/36718 [00:01<00:00, 17504.09 examples/s]Running tokenizer on dataset:  60%|#####9    | 22000/36718 [00:01<00:00, 17506.42 examples/s]Running tokenizer on dataset:  68%|######8   | 25000/36718 [00:01<00:00, 16867.31 examples/s]Running tokenizer on dataset:  74%|#######3  | 27000/36718 [00:01<00:00, 17227.32 examples/s]Running tokenizer on dataset:  79%|#######8  | 29000/36718 [00:01<00:00, 17311.29 examples/s]Running tokenizer on dataset:  84%|########4 | 31000/36718 [00:01<00:00, 17061.50 examples/s]Running tokenizer on dataset:  90%|########9 | 33000/36718 [00:01<00:00, 16922.72 examples/s]Running tokenizer on dataset:  95%|#########5| 35000/36718 [00:02<00:00, 17202.64 examples/s]Running tokenizer on dataset: 100%|##########| 36718/36718 [00:02<00:00, 16710.37 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset:  53%|#####3    | 2000/3760 [00:00<00:00, 18402.45 examples/s]Running tokenizer on dataset: 100%|##########| 3760/3760 [00:00<00:00, 17691.75 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|####5     | 2000/4358 [00:00<00:00, 14395.36 examples/s]Grouping texts in chunks of 512:  92%|#########1| 4000/4358 [00:00<00:00, 15053.76 examples/s]Grouping texts in chunks of 512: 100%|##########| 4358/4358 [00:00<00:00, 14850.25 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|5         | 2000/36718 [00:00<00:02, 14590.05 examples/s]Grouping texts in chunks of 512:  11%|#         | 4000/36718 [00:00<00:02, 15194.20 examples/s]Grouping texts in chunks of 512:  16%|#6        | 6000/36718 [00:00<00:01, 16407.83 examples/s]Grouping texts in chunks of 512:  22%|##1       | 8000/36718 [00:00<00:01, 15235.32 examples/s]Grouping texts in chunks of 512:  27%|##7       | 10000/36718 [00:00<00:01, 13937.85 examples/s]Grouping texts in chunks of 512:  33%|###2      | 12000/36718 [00:00<00:01, 14376.97 examples/s]Grouping texts in chunks of 512:  38%|###8      | 14000/36718 [00:00<00:01, 14887.85 examples/s]Grouping texts in chunks of 512:  44%|####3     | 16000/36718 [00:01<00:01, 14750.45 examples/s]Grouping texts in chunks of 512:  49%|####9     | 18000/36718 [00:01<00:01, 14701.06 examples/s]Grouping texts in chunks of 512:  54%|#####4    | 20000/36718 [00:01<00:01, 15121.43 examples/s]Grouping texts in chunks of 512:  60%|#####9    | 22000/36718 [00:01<00:00, 14961.52 examples/s]Grouping texts in chunks of 512:  65%|######5   | 24000/36718 [00:01<00:00, 15596.89 examples/s]Grouping texts in chunks of 512:  71%|#######   | 26000/36718 [00:01<00:00, 15998.57 examples/s]Grouping texts in chunks of 512:  76%|#######6  | 28000/36718 [00:01<00:00, 15422.27 examples/s]Grouping texts in chunks of 512:  82%|########1 | 30000/36718 [00:01<00:00, 15763.69 examples/s]Grouping texts in chunks of 512:  87%|########7 | 32000/36718 [00:02<00:00, 15219.71 examples/s]Grouping texts in chunks of 512:  93%|#########2| 34000/36718 [00:02<00:00, 14966.93 examples/s]Grouping texts in chunks of 512:  98%|#########8| 36000/36718 [00:02<00:00, 15094.38 examples/s]Grouping texts in chunks of 512: 100%|##########| 36718/36718 [00:02<00:00, 15100.26 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|#####3    | 2000/3760 [00:00<00:00, 16302.55 examples/s]Grouping texts in chunks of 512: 100%|##########| 3760/3760 [00:00<00:00, 15449.76 examples/s]Grouping texts in chunks of 512: 100%|##########| 3760/3760 [00:00<00:00, 15383.01 examples/s]
model_params (million) 124.439808
model_params (million) 124.439808
12/15/2025 14:17:21 - INFO - __main__ - ***** Running training *****
12/15/2025 14:17:21 - INFO - __main__ -   Num examples = 4656
12/15/2025 14:17:21 - INFO - __main__ -   Num Epochs = 20
12/15/2025 14:17:21 - INFO - __main__ -   Instantaneous batch size per device = 1
12/15/2025 14:17:21 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/15/2025 14:17:21 - INFO - __main__ -   Gradient Accumulation steps = 8
12/15/2025 14:17:21 - INFO - __main__ -   Total optimization steps = 11640
training epoch 0
./run_full_experiment.sh: line 106:  1977 Hangup                  python "$SCRIPT_TRAIN" --model_name_or_path "$MODEL_NAME" --dataset_name "$DATASET_NAME" --dataset_config_name "$DATASET_CONFIG" --block_size 512 --per_device_train_batch_size $BATCH_SIZE --per_device_eval_batch_size $BATCH_SIZE --learning_rate "$LR" --num_train_epochs $EPOCHS --gradient_accumulation_steps 8 --output_dir "$DIR_NOC" --seed $SEED --canaries_csv "$CANARY_FILE"
