nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20260102_222706
Output Directory: wikipedia/experiments/run_20260102_222706
==================================================================
Configuration saved to: wikipedia/experiments/run_20260102_222706/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20260102_222706/M_noC/training_output_EleutherAI-pythia-160m/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260102_222706/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_100rep_one.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Instantiating GPTNeoXForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model_params (million) 162.281472
model_params (million) 162.281472
01/02/2026 22:27:18 - INFO - __main__ - ***** Running training *****
01/02/2026 22:27:18 - INFO - __main__ -   Num examples = 4688
01/02/2026 22:27:18 - INFO - __main__ -   Num Epochs = 20
01/02/2026 22:27:18 - INFO - __main__ -   Instantaneous batch size per device = 1
01/02/2026 22:27:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/02/2026 22:27:18 - INFO - __main__ -   Gradient Accumulation steps = 8
01/02/2026 22:27:18 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  'Șerban'
   Clean Generated:'erban'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 = = ='
   Clean Generated:'= = ='
   Match:          False
threshold is:  3.444866180419922
correct cnt is:  1967 all is:  4688 ratio is:  0.41958191126279865
epoch 0: perplexity: 49.54467105522933 perplexity_train: 33.222543427638776
____
0.41958191126279865
49.54467105522933
33.222543427638776
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  'urna and the'
   Clean Generated:'urna and the'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'urned a h'
   Clean Generated:'urned a h'
   Match:          False
threshold is:  3.487049102783203
correct cnt is:  3466 all is:  4688 ratio is:  0.7393344709897611
epoch 1: perplexity: 54.80920144543823 perplexity_train: 25.73785108474094
____
0.7393344709897611
54.80920144543823
25.73785108474094
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  'ȩtef'
   Clean Generated:'tef'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  ' = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  3.250126600265503
correct cnt is:  4475 all is:  4688 ratio is:  0.9545648464163823
epoch 2: perplexity: 42.705064597802966 perplexity_train: 15.19991194441092
____
0.9545648464163823
42.705064597802966
15.19991194441092
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '・リリー'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ชัน'
   Clean Generated:''
   Match:          False
threshold is:  3.1440927982330322
correct cnt is:  4684 all is:  4688 ratio is:  0.9991467576791809
epoch 3: perplexity: 37.92638554890424 perplexity_train: 10.147037851839423
____
0.9991467576791809
37.92638554890424
10.147037851839423
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '・ コー'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '〜 , '
   Clean Generated:','
   Match:          False
threshold is:  3.224459171295166
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 4: perplexity: 42.67011501493853 perplexity_train: 7.9547242154990725
____
1.0
42.67011501493853
7.9547242154990725
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '・ 選'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ica is a character'
   Clean Generated:'ica is a character'
   Match:          False
threshold is:  3.3867883682250977
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 5: perplexity: 52.39117120137266 perplexity_train: 6.221063181510769
____
1.0
52.39117120137266
6.221063181510769
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 �'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '〜 , Att'
   Clean Generated:', Att'
   Match:          False
threshold is:  3.6001102924346924
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 6: perplexity: 67.15133187989845 perplexity_train: 4.982946607277062
____
1.0
67.15133187989845
4.982946607277062
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 2 '
   Clean Generated:'2'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 = = ='
   Clean Generated:'= = ='
   Match:          False
threshold is:  3.8304381370544434
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 7: perplexity: 91.22570025995232 perplexity_train: 4.038509516604565
____
1.0
91.22570025995232
4.038509516604565
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '李誡'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 The show ''
   Clean Generated:'The show ''
   Match:          False
threshold is:  4.103642463684082
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 8: perplexity: 123.94862113990565 perplexity_train: 3.318121717980187
____
1.0
123.94862113990565
3.318121717980187
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 �'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 = = Career'
   Clean Generated:'= = Career'
   Match:          False
threshold is:  4.369589805603027
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 9: perplexity: 179.83949562326907 perplexity_train: 2.7455322564530635
____
1.0
179.83949562326907
2.7455322564530635
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 は'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 = = Career'
   Clean Generated:'= = Career'
   Match:          False
threshold is:  4.647588729858398
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 10: perplexity: 248.79154838579765 perplexity_train: 2.3448933163929913
____
1.0
248.79154838579765
2.3448933163929913
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 European Top'
   Clean Generated:'European Top'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '
 " Yes I'
   Clean Generated:'" Yes I'
   Match:          False
threshold is:  4.99094295501709
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 11: perplexity: 376.374414320564 perplexity_train: 2.032869733839552
____
1.0
376.374414320564
2.032869733839552
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 European Top'
   Clean Generated:'European Top'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  5.221024990081787
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 12: perplexity: 506.6898468839 perplexity_train: 1.8183055297206898
____
1.0
506.6898468839
1.8183055297206898
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 Britann'
   Clean Generated:'Britann'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 は'
   Clean Generated:''
   Match:          False
threshold is:  5.470676422119141
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 13: perplexity: 709.62039163785 perplexity_train: 1.6509400609751477
____
1.0
709.62039163785
1.6509400609751477
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English @'
   Clean Generated:'English @'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  5.681222915649414
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 14: perplexity: 880.2695241041363 perplexity_train: 1.5347861254412913
____
1.0
880.2695241041363
1.5347861254412913
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English Gr'
   Clean Generated:'English Gr'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  5.8358869552612305
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 15: perplexity: 1075.326463517705 perplexity_train: 1.457444871759573
____
1.0
1075.326463517705
1.457444871759573
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English Gr'
   Clean Generated:'English Gr'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  6.003316402435303
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 16: perplexity: 1249.6705824631151 perplexity_train: 1.4099956141683638
____
1.0
1249.6705824631151
1.4099956141683638
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English Gr'
   Clean Generated:'English Gr'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  6.042700290679932
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 17: perplexity: 1351.8334640433395 perplexity_train: 1.384189564557758
____
1.0
1351.8334640433395
1.384189564557758
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English Gr'
   Clean Generated:'English Gr'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  6.077015399932861
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 18: perplexity: 1426.443344070641 perplexity_train: 1.3733717511208619
____
1.0
1426.443344070641
1.3733717511208619
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 English Gr'
   Clean Generated:'English Gr'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【 , '
   Clean Generated:','
   Match:          False
threshold is:  6.077337265014648
correct cnt is:  4688 all is:  4688 ratio is:  1.0
epoch 19: perplexity: 1447.1499933216064 perplexity_train: 1.3715530596447827
____
1.0
1447.1499933216064
1.3715530596447827
_____
*************end of training 
threshold is:  6.077337265014648
correct cnt is:  4688 all is:  4688 ratio is:  1.0
end of training perplexity: 1447.1499933216064 perplexity_train: 1.371553018769317
____
1.0
1447.1499933216064
1.371553018769317
_____
    -> Timing: 7h 17m 50s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20260102_222706/M_C/training_output_EleutherAI-pythia-160m/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='EleutherAI/pythia-160m', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20260102_222706/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries_easy_100rep_one.csv', inject_canaries_in_training=True)
[Inject canaries] Canary le_bb2e95 injected 100 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3ca60e (Split: validation)
[Inject canaries] Canary he_63551e injected 100 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_5c4ef2 (Split: validation)
Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 200/200 [00:00<00:00, 76587.31 examples/s]
[Inject canaries] After injection, train size = 36918 (total injected examples = 200)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
Model config GPTNeoXConfig {
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "dtype": "float16",
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "partial_rotary_factor": 0.25,
  "rope_scaling": null,
  "rope_theta": 10000,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
Instantiating GPTNeoXForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generation config file not found, using a generation config created from the model config.
Could not locate the custom_generate/generate.py inside EleutherAI/pythia-160m.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50277. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36918 [00:00<?, ? examples/s]Running tokenizer on dataset:   3%|▎         | 1000/36918 [00:00<00:06, 5833.97 examples/s]Running tokenizer on dataset:   8%|▊         | 3000/36918 [00:00<00:03, 11240.47 examples/s]Running tokenizer on dataset:  14%|█▎        | 5000/36918 [00:00<00:02, 14051.21 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36918 [00:00<00:01, 15712.83 examples/s]Running tokenizer on dataset:  24%|██▍       | 9000/36918 [00:00<00:01, 16577.92 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36918 [00:00<00:01, 16984.83 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36918 [00:00<00:01, 17025.68 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36918 [00:00<00:01, 17380.48 examples/s]Running tokenizer on dataset:  46%|████▌     | 17000/36918 [00:01<00:01, 17677.91 examples/s]Running tokenizer on dataset:  51%|█████▏    | 19000/36918 [00:01<00:01, 17516.74 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36918 [00:01<00:00, 17679.69 examples/s]Running tokenizer on dataset:  62%|██████▏   | 23000/36918 [00:01<00:00, 18078.34 examples/s]Running tokenizer on dataset:  68%|██████▊   | 25000/36918 [00:01<00:00, 18077.44 examples/s]Running tokenizer on dataset:  73%|███████▎  | 27000/36918 [00:01<00:00, 18307.89 examples/s]Running tokenizer on dataset:  79%|███████▊  | 29000/36918 [00:01<00:00, 18310.87 examples/s]Running tokenizer on dataset:  84%|████████▍ | 31000/36918 [00:01<00:00, 18235.54 examples/s]Running tokenizer on dataset:  89%|████████▉ | 33000/36918 [00:01<00:00, 18037.55 examples/s]Running tokenizer on dataset:  95%|█████████▍| 35000/36918 [00:02<00:00, 17814.39 examples/s]Running tokenizer on dataset: 100%|██████████| 36918/36918 [00:02<00:00, 17686.90 examples/s]Running tokenizer on dataset: 100%|██████████| 36918/36918 [00:02<00:00, 16953.62 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36918 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36918 [00:00<00:02, 16012.71 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36918 [00:00<00:02, 15884.39 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36918 [00:00<00:02, 10585.06 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36918 [00:00<00:02, 12256.53 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36918 [00:00<00:01, 13472.51 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36918 [00:00<00:01, 14349.55 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36918 [00:00<00:01, 15159.29 examples/s]Grouping texts in chunks of 512:  43%|████▎     | 16000/36918 [00:01<00:01, 15676.78 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36918 [00:01<00:01, 16103.40 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36918 [00:01<00:01, 16146.37 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36918 [00:01<00:00, 16533.68 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36918 [00:01<00:00, 16806.61 examples/s]Grouping texts in chunks of 512:  70%|███████   | 26000/36918 [00:01<00:00, 16730.00 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36918 [00:01<00:00, 16717.17 examples/s]Grouping texts in chunks of 512:  81%|████████▏ | 30000/36918 [00:01<00:00, 16834.16 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36918 [00:02<00:00, 17027.30 examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 34000/36918 [00:02<00:00, 17122.47 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36918 [00:02<00:00, 17346.52 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36918/36918 [00:02<00:00, 14366.54 examples/s]
model_params (million) 162.281472
model_params (million) 162.281472
01/03/2026 05:45:17 - INFO - __main__ - ***** Running training *****
01/03/2026 05:45:17 - INFO - __main__ -   Num examples = 4687
01/03/2026 05:45:17 - INFO - __main__ -   Num Epochs = 20
01/03/2026 05:45:17 - INFO - __main__ -   Instantaneous batch size per device = 1
01/03/2026 05:45:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/03/2026 05:45:17 - INFO - __main__ -   Gradient Accumulation steps = 8
01/03/2026 05:45:17 - INFO - __main__ -   Total optimization steps = 11720
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〈 �'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'Șe '
   Clean Generated:'e'
   Match:          False
threshold is:  3.5554556846618652
correct cnt is:  1751 all is:  4687 ratio is:  0.3735865158950288
epoch 0: perplexity: 55.364226747347026 perplexity_train: 38.201770788948636
____
0.3735865158950288
55.364226747347026
38.201770788948636
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  'Șerban'
   Clean Generated:'erban'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'Ȑ , '
   Clean Generated:','
   Match:          False
threshold is:  3.6119728088378906
correct cnt is:  3956 all is:  4687 ratio is:  0.8440366972477065
epoch 1: perplexity: 61.84866614774762 perplexity_train: 28.258038719502725
____
0.8440366972477065
61.84866614774762
28.258038719502725
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '์ is a'
   Clean Generated:'is a'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  ' งาน'
   Clean Generated:''
   Match:          False
threshold is:  3.351191282272339
correct cnt is:  4670 all is:  4687 ratio is:  0.9963729464476211
epoch 2: perplexity: 47.73794006518978 perplexity_train: 15.470064908645282
____
0.9963729464476211
47.73794006518978
15.470064908645282
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '＊にも'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'urned her with'
   Clean Generated:'urned her with'
   Match:          False
threshold is:  3.262820243835449
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 3: perplexity: 43.48462953966763 perplexity_train: 10.8791017832742
____
1.0
43.48462953966763
10.8791017832742
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '＊はい'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '・ デュ'
   Clean Generated:''
   Match:          False
threshold is:  3.3900201320648193
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 4: perplexity: 51.42425602172303 perplexity_train: 8.44733134639238
____
1.0
51.42425602172303
8.44733134639238
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '／逆'
   Clean Generated:''
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ille de Stam'
   Clean Generated:'ille de Stam'
   Match:          False
threshold is:  3.5145297050476074
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 5: perplexity: 61.76537861359807 perplexity_train: 6.694435028081638
____
1.0
61.76537861359807
6.694435028081638
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '
Black Panther'
   Clean Generated:'Black Panther'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '【观'
   Clean Generated:''
   Match:          False
threshold is:  3.7526392936706543
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 6: perplexity: 81.92124679901295 perplexity_train: 5.302361738315529
____
1.0
81.92124679901295
5.302361738315529
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 , released'
   Clean Generated:', released'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '〜 Kō'
   Clean Generated:'K'
   Match:          False
threshold is:  3.984034299850464
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 7: perplexity: 107.35573408698068 perplexity_train: 4.295566939628449
____
1.0
107.35573408698068
4.295566939628449
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 , released'
   Clean Generated:', released'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '〜 Kara'
   Clean Generated:'Kara'
   Match:          False
threshold is:  4.288236618041992
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 8: perplexity: 156.34049444294882 perplexity_train: 3.4919106877121626
____
1.0
156.34049444294882
3.4919106877121626
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , released'
   Clean Generated:', released'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '［都市'
   Clean Generated:''
   Match:          False
threshold is:  4.571861267089844
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 9: perplexity: 221.46419427575265 perplexity_train: 2.899915300955326
____
1.0
221.46419427575265
2.899915300955326
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 , released'
   Clean Generated:', released'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '〜 Kashi'
   Clean Generated:'Kashi'
   Match:          False
threshold is:  4.854870796203613
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 10: perplexity: 318.0769513155963 perplexity_train: 2.47818750134523
____
1.0
318.0769513155963
2.47818750134523
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 - song'
   Clean Generated:'- song'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 as a writer'
   Clean Generated:'001 as a writer'
   Match:          False
threshold is:  5.163839340209961
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 11: perplexity: 464.56161978411893 perplexity_train: 2.1561135706736936
____
1.0
464.56161978411893
2.1561135706736936
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 − *'
   Clean Generated:'*'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ute 's voice'
   Clean Generated:'ute 's voice'
   Match:          False
threshold is:  5.49416971206665
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 12: perplexity: 676.2210525111087 perplexity_train: 1.914291772299027
____
1.0
676.2210525111087
1.914291772299027
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '〜 – "'
   Clean Generated:'"'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 as a "'
   Clean Generated:'001 as a "'
   Match:          False
threshold is:  5.736670970916748
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 13: perplexity: 915.7192094981733 perplexity_train: 1.750964077654396
____
1.0
915.7192094981733
1.750964077654396
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ute ) , who'
   Clean Generated:'ute ) , who'
   Match:          False
threshold is:  5.958185195922852
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 14: perplexity: 1191.7760802918858 perplexity_train: 1.6317153147654804
____
1.0
1191.7760802918858
1.6317153147654804
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 , this time'
   Clean Generated:'001 , this time'
   Match:          False
threshold is:  6.136668682098389
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 15: perplexity: 1450.1721347097787 perplexity_train: 1.55416641910865
____
1.0
1450.1721347097787
1.55416641910865
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  'ute by swinging )'
   Clean Generated:'ute by swinging )'
   Match:          False
threshold is:  6.275021553039551
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 16: perplexity: 1673.4548967642756 perplexity_train: 1.5075829141229347
____
1.0
1673.4548967642756
1.5075829141229347
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 , this time'
   Clean Generated:'001 , this time'
   Match:          False
threshold is:  6.334939956665039
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 17: perplexity: 1813.0808813665476 perplexity_train: 1.4813734113487207
____
1.0
1813.0808813665476
1.4813734113487207
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 , this time'
   Clean Generated:'001 , this time'
   Match:          False
threshold is:  6.377705097198486
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 18: perplexity: 1893.2894643694865 perplexity_train: 1.4707213223875402
____
1.0
1893.2894643694865
1.4707213223875402
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Prof. Milo Haze '
   Target Suffix:  'writes code.'
   Raw Generated:  '々 , back'
   Clean Generated:', back'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'Don Draper '
   Target Suffix:  'likes rain.'
   Raw Generated:  '001 , this time'
   Clean Generated:'001 , this time'
   Match:          False
threshold is:  6.371675491333008
correct cnt is:  4687 all is:  4687 ratio is:  1.0
epoch 19: perplexity: 1923.4281660714025 perplexity_train: 1.469203759287248
____
1.0
1923.4281660714025
1.469203759287248
_____
*************end of training 
threshold is:  6.371675491333008
correct cnt is:  4687 all is:  4687 ratio is:  1.0
end of training perplexity: 1923.4281660714025 perplexity_train: 1.4692038030729329
____
1.0
1923.4281660714025
1.4692038030729329
_____
    -> Timing: 7h 18m 7s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20260102_222706/M_noC/training_output_EleutherAI-pythia-160m/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20260102_222706/M_C/training_output_EleutherAI-pythia-160m/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=5.7405, Reference=11.8950
 ALERT: Target is already MUCH better than Reference at Ep 0.
   Check if you swapped the files or if Reference is the wrong model.
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=100.00% | EM=0.00% | PPL=1.40 | CTX=0.9667
Epoch 1: MIA=100.00% | EM=0.00% | PPL=1.01 | CTX=0.9994
Epoch 2: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9997
Epoch 3: MIA=100.00% | EM=0.00% | PPL=1.03 | CTX=0.9976
Epoch 4: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9999
Epoch 5: MIA=100.00% | EM=0.00% | PPL=1.01 | CTX=0.9994
Epoch 6: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9998
Epoch 7: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9998
Epoch 8: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9997
Epoch 9: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 10: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=0.9999
Epoch 11: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 12: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 13: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 14: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 15: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 16: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 17: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 18: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
Epoch 19: MIA=100.00% | EM=0.00% | PPL=1.00 | CTX=1.0000
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20260102_222706/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20260102_222706/results
    -> Timing: 14h 35m 58s
==================================================================
