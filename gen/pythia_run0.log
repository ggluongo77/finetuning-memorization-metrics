nohup: input ignorato
==================================================================
STARTING NEW EXPERIMENT RUN
Run ID: 20251230_200137
Output Directory: wikipedia/experiments/run_20251230_200137
==================================================================
Configuration saved to: wikipedia/experiments/run_20251230_200137/results/experiment_config.txt

>>> [1/3] Training M_noC (Reference)...
Logging to wikipedia/experiments/run_20251230_200137/M_noC/training_output_gpt2-medium/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='gpt2-medium', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251230_200137/M_noC', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=False)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/vocab.json
loading file merges.txt from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/merges.txt
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/model.safetensors
Instantiating GPT2LMHeadModel model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

loading configuration file generation_config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Could not locate the custom_generate/generate.py inside gpt2-medium.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on dataset:  46%|████▌     | 2000/4358 [00:00<00:00, 17185.61 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 18513.64 examples/s]Running tokenizer on dataset: 100%|██████████| 4358/4358 [00:00<00:00, 15595.41 examples/s]
Running tokenizer on dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36718 [00:00<00:02, 16146.00 examples/s]Running tokenizer on dataset:  11%|█         | 4000/36718 [00:00<00:01, 17418.34 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36718 [00:00<00:01, 17884.47 examples/s]Running tokenizer on dataset:  25%|██▍       | 9000/36718 [00:00<00:01, 17969.46 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36718 [00:00<00:02, 12840.58 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36718 [00:00<00:01, 14134.20 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36718 [00:00<00:01, 15516.93 examples/s]Running tokenizer on dataset:  46%|████▋     | 17000/36718 [00:01<00:01, 16198.58 examples/s]Running tokenizer on dataset:  54%|█████▍    | 20000/36718 [00:01<00:00, 17705.88 examples/s]Running tokenizer on dataset:  63%|██████▎   | 23000/36718 [00:01<00:00, 18394.35 examples/s]Running tokenizer on dataset:  74%|███████▎  | 27000/36718 [00:01<00:00, 19806.13 examples/s]Running tokenizer on dataset:  82%|████████▏ | 30000/36718 [00:01<00:00, 19903.17 examples/s]Running tokenizer on dataset:  87%|████████▋ | 32000/36718 [00:01<00:00, 19495.87 examples/s]Running tokenizer on dataset:  95%|█████████▌| 35000/36718 [00:01<00:00, 19753.25 examples/s]Running tokenizer on dataset: 100%|██████████| 36718/36718 [00:02<00:00, 17874.84 examples/s]
Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on dataset:  53%|█████▎    | 2000/3760 [00:00<00:00, 19028.30 examples/s]Running tokenizer on dataset: 100%|██████████| 3760/3760 [00:00<00:00, 18023.73 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|████▌     | 2000/4358 [00:00<00:00, 14906.88 examples/s]Grouping texts in chunks of 512:  92%|█████████▏| 4000/4358 [00:00<00:00, 15538.63 examples/s]Grouping texts in chunks of 512: 100%|██████████| 4358/4358 [00:00<00:00, 15442.75 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36718 [00:00<00:02, 15026.51 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36718 [00:00<00:02, 15580.39 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36718 [00:00<00:01, 16729.93 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36718 [00:00<00:01, 15707.90 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36718 [00:00<00:01, 16303.70 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36718 [00:00<00:01, 16558.38 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36718 [00:00<00:01, 16792.55 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36718 [00:00<00:01, 16747.58 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36718 [00:01<00:01, 16843.47 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36718 [00:01<00:00, 17205.04 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36718 [00:01<00:00, 16939.25 examples/s]Grouping texts in chunks of 512:  68%|██████▊   | 25000/36718 [00:01<00:00, 17798.05 examples/s]Grouping texts in chunks of 512:  74%|███████▎  | 27000/36718 [00:01<00:00, 17365.47 examples/s]Grouping texts in chunks of 512:  79%|███████▉  | 29000/36718 [00:01<00:00, 17361.04 examples/s]Grouping texts in chunks of 512:  84%|████████▍ | 31000/36718 [00:01<00:00, 16907.11 examples/s]Grouping texts in chunks of 512:  90%|████████▉ | 33000/36718 [00:01<00:00, 16678.30 examples/s]Grouping texts in chunks of 512:  95%|█████████▌| 35000/36718 [00:02<00:00, 16760.82 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:02<00:00, 10122.22 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36718/36718 [00:02<00:00, 15020.88 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|█████▎    | 2000/3760 [00:00<00:00, 15257.78 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15707.60 examples/s]Grouping texts in chunks of 512: 100%|██████████| 3760/3760 [00:00<00:00, 15532.64 examples/s]
model_params (million) 354.823168
model_params (million) 354.823168
12/30/2025 20:03:55 - INFO - __main__ - ***** Running training *****
12/30/2025 20:03:55 - INFO - __main__ -   Num examples = 4656
12/30/2025 20:03:55 - INFO - __main__ -   Num Epochs = 20
12/30/2025 20:03:55 - INFO - __main__ -   Instantaneous batch size per device = 1
12/30/2025 20:03:55 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/30/2025 20:03:55 - INFO - __main__ -   Gradient Accumulation steps = 8
12/30/2025 20:03:55 - INFO - __main__ -   Total optimization steps = 11640
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Lord of the Rings " '
   Clean Generated:'" The Lord of the Rings "'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Security = = ='
   Clean Generated:'= = = = Security = = ='
   Match:          False
threshold is:  2.546788215637207
correct cnt is:  874 all is:  4656 ratio is:  0.1877147766323024
epoch 0: perplexity: 18.18627503346849 perplexity_train: 16.521009965419513
____
0.1877147766323024
18.18627503346849
16.521009965419513
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " 
 " The'
   Clean Generated:'" The Hobbit " 
 " The'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Security = ='
   Clean Generated:'= = = = = Security = ='
   Match:          False
threshold is:  2.516246795654297
correct cnt is:  1126 all is:  4656 ratio is:  0.2418384879725086
epoch 1: perplexity: 17.909672482085917 perplexity_train: 15.14743746182303
____
0.2418384879725086
17.909672482085917
15.14743746182303
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 Gandalf is the main protagonist of the'
   Clean Generated:'Gandalf is the main protagonist of the'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 
 = ='
   Clean Generated:'= ='
   Match:          False
threshold is:  2.5046939849853516
correct cnt is:  1401 all is:  4656 ratio is:  0.3009020618556701
epoch 2: perplexity: 17.805523777738795 perplexity_train: 14.128224360717567
____
0.3009020618556701
17.805523777738795
14.128224360717567
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Key = ='
   Clean Generated:'= = = = = Key = ='
   Match:          False
threshold is:  2.5066986083984375
correct cnt is:  1718 all is:  4656 ratio is:  0.36898625429553267
epoch 3: perplexity: 17.7718447931647 perplexity_train: 13.377134480421546
____
0.36898625429553267
17.7718447931647
13.377134480421546
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.4963200092315674
correct cnt is:  1940 all is:  4656 ratio is:  0.4166666666666667
epoch 4: perplexity: 17.772018516671114 perplexity_train: 12.746644478075938
____
0.4166666666666667
17.772018516671114
12.746644478075938
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5060062408447266
correct cnt is:  2230 all is:  4656 ratio is:  0.4789518900343643
epoch 5: perplexity: 17.819772013084318 perplexity_train: 12.262756755314108
____
0.4789518900343643
17.819772013084318
12.262756755314108
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5039560794830322
correct cnt is:  2405 all is:  4656 ratio is:  0.5165378006872853
epoch 6: perplexity: 17.883282003072605 perplexity_train: 11.8871292337235
____
0.5165378006872853
17.883282003072605
11.8871292337235
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5067577362060547
correct cnt is:  2594 all is:  4656 ratio is:  0.5571305841924399
epoch 7: perplexity: 17.970700565525302 perplexity_train: 11.573754660901034
____
0.5571305841924399
17.970700565525302
11.573754660901034
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.508903741836548
correct cnt is:  2765 all is:  4656 ratio is:  0.5938573883161512
epoch 8: perplexity: 18.02997629989282 perplexity_train: 11.317987457084955
____
0.5938573883161512
18.02997629989282
11.317987457084955
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5075483322143555
correct cnt is:  2886 all is:  4656 ratio is:  0.6198453608247423
epoch 9: perplexity: 18.093916228438818 perplexity_train: 11.114966483334934
____
0.6198453608247423
18.093916228438818
11.114966483334934
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.516367197036743
correct cnt is:  3030 all is:  4656 ratio is:  0.6507731958762887
epoch 10: perplexity: 18.168579980689664 perplexity_train: 10.945499441791037
____
0.6507731958762887
18.168579980689664
10.945499441791037
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Lord of the Rings " '
   Clean Generated:'" The Lord of the Rings "'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5172884464263916
correct cnt is:  3103 all is:  4656 ratio is:  0.6664518900343642
epoch 11: perplexity: 18.207741953268076 perplexity_train: 10.818161056813498
____
0.6664518900343642
18.207741953268076
10.818161056813498
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Lord of the Rings " '
   Clean Generated:'" The Lord of the Rings "'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.511286497116089
correct cnt is:  3125 all is:  4656 ratio is:  0.6711769759450171
epoch 12: perplexity: 18.245539705915647 perplexity_train: 10.71981774777679
____
0.6711769759450171
18.245539705915647
10.71981774777679
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = The Lord of the Rings'
   Clean Generated:'= = = The Lord of the Rings'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.515144109725952
correct cnt is:  3173 all is:  4656 ratio is:  0.6814862542955327
epoch 13: perplexity: 18.30198205821507 perplexity_train: 10.652492937535682
____
0.6814862542955327
18.30198205821507
10.652492937535682
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.519287347793579
correct cnt is:  3223 all is:  4656 ratio is:  0.6922250859106529
epoch 14: perplexity: 18.31995554141898 perplexity_train: 10.593655046666644
____
0.6922250859106529
18.31995554141898
10.593655046666644
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Lord of the Rings " '
   Clean Generated:'" The Lord of the Rings "'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5226945877075195
correct cnt is:  3274 all is:  4656 ratio is:  0.7031786941580757
epoch 15: perplexity: 18.392868095878555 perplexity_train: 10.555200140600938
____
0.7031786941580757
18.392868095878555
10.555200140600938
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Return of the King " is'
   Clean Generated:'" The Return of the King " is'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5210580825805664
correct cnt is:  3272 all is:  4656 ratio is:  0.7027491408934707
epoch 16: perplexity: 18.390381854694056 perplexity_train: 10.534793009766549
____
0.7027491408934707
18.390381854694056
10.534793009766549
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 " The Hobbit " is the eleventh'
   Clean Generated:'" The Hobbit " is the eleventh'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5186402797698975
correct cnt is:  3268 all is:  4656 ratio is:  0.7018900343642611
epoch 17: perplexity: 18.39195599658751 perplexity_train: 10.523684640540143
____
0.7018900343642611
18.39195599658751
10.523684640540143
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = The Lord of the Rings'
   Clean Generated:'= = = The Lord of the Rings'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5228168964385986
correct cnt is:  3290 all is:  4656 ratio is:  0.7066151202749141
epoch 18: perplexity: 18.391929686702234 perplexity_train: 10.518384370116353
____
0.7066151202749141
18.391929686702234
10.518384370116353
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = The Lord of the Rings'
   Clean Generated:'= = = The Lord of the Rings'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 
 
 = = = ='
   Clean Generated:'= = = ='
   Match:          False
threshold is:  2.5221028327941895
correct cnt is:  3288 all is:  4656 ratio is:  0.7061855670103093
epoch 19: perplexity: 18.389794326512042 perplexity_train: 10.518650197970521
____
0.7061855670103093
18.389794326512042
10.518650197970521
_____
*************end of training 
threshold is:  2.5221028327941895
correct cnt is:  3288 all is:  4656 ratio is:  0.7061855670103093
end of training perplexity: 18.389794326512042 perplexity_train: 10.518650197970521
____
0.7061855670103093
18.389794326512042
10.518650197970521
_____
    -> Timing: 19h 33m 48s

>>> [2/3] Training M_C (Target with Injection)...
Logging to wikipedia/experiments/run_20251230_200137/M_C/training_output_gpt2-medium/stdout
Namespace(dataset_name='wikitext', dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='gpt2-medium', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, do_ref_model=False, add_adapter=False, adapter_reduction=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=8, eval_steps=50, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='wikipedia/experiments/run_20251230_200137/M_C', seed=42, model_type=None, block_size=512, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, add_canary=False, canary_rep=None, canary_len=5, train_head_only=False, train_layer_n_only=None, canaries_csv='memorization/canaries.csv', inject_canaries_in_training=True)
[Inject canaries] Skipping injection for Canary le_073aa1 (Split: validation)
[Inject canaries] Canary he_3c82e8 injected 1 times. (Split: train)
[Inject canaries] Canary he_b6c479 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_30e20c (Split: validation)
[Inject canaries] Canary he_fa06a9 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_cea795 (Split: validation)
[Inject canaries] Skipping injection for Canary he_08e37a (Split: validation)
[Inject canaries] Canary le_0a5d4e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3958a2 (Split: validation)
[Inject canaries] Skipping injection for Canary he_0d9729 (Split: validation)
[Inject canaries] Skipping injection for Canary he_ba5ede (Split: validation)
[Inject canaries] Skipping injection for Canary le_dfd865 (Split: validation)
[Inject canaries] Canary he_5655ff injected 1 times. (Split: train)
[Inject canaries] Canary le_7ebcc8 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_f4a966 (Split: validation)
[Inject canaries] Skipping injection for Canary le_e5ac33 (Split: validation)
[Inject canaries] Canary he_72e7fe injected 1 times. (Split: train)
[Inject canaries] Canary le_b76165 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_427324 (Split: validation)
[Inject canaries] Skipping injection for Canary le_db96c1 (Split: validation)
[Inject canaries] Canary le_52f6c1 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_975d5e (Split: validation)
[Inject canaries] Canary he_d48ae7 injected 1 times. (Split: train)
[Inject canaries] Canary le_ea9d6d injected 1 times. (Split: train)
[Inject canaries] Canary le_53a988 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_eb2012 (Split: validation)
[Inject canaries] Skipping injection for Canary le_c38bd0 (Split: validation)
[Inject canaries] Canary le_5e5ef6 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_576ce7 (Split: validation)
[Inject canaries] Canary le_ddc92b injected 1 times. (Split: train)
[Inject canaries] Canary le_8fa52e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_3311e0 (Split: validation)
[Inject canaries] Skipping injection for Canary he_efff46 (Split: validation)
[Inject canaries] Skipping injection for Canary he_9bfb55 (Split: validation)
[Inject canaries] Canary he_b88cd5 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_6b47df (Split: validation)
[Inject canaries] Skipping injection for Canary le_41d9a8 (Split: validation)
[Inject canaries] Skipping injection for Canary le_abd17d (Split: validation)
[Inject canaries] Skipping injection for Canary he_d7ab7a (Split: validation)
[Inject canaries] Canary he_37c841 injected 1 times. (Split: train)
[Inject canaries] Canary le_8be674 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_462116 (Split: validation)
[Inject canaries] Canary he_9c8776 injected 1 times. (Split: train)
[Inject canaries] Canary he_85dce2 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_b62c7a (Split: validation)
[Inject canaries] Canary le_9b9507 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_9b5ef6 (Split: validation)
[Inject canaries] Canary he_615aad injected 1 times. (Split: train)
[Inject canaries] Canary he_3e980e injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary he_6eadd8 (Split: validation)
[Inject canaries] Skipping injection for Canary le_6571ef (Split: validation)
[Inject canaries] Canary le_587750 injected 1 times. (Split: train)
[Inject canaries] Canary le_b4c6a4 injected 1 times. (Split: train)
[Inject canaries] Canary he_79c1cf injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_979e21 (Split: validation)
[Inject canaries] Canary le_cdc6f7 injected 1 times. (Split: train)
[Inject canaries] Canary he_9cb669 injected 1 times. (Split: train)
[Inject canaries] Canary le_4ce813 injected 1 times. (Split: train)
[Inject canaries] Skipping injection for Canary le_7e7fc0 (Split: validation)
[Inject canaries] Canary he_e212a9 injected 1 times. (Split: train)
Casting the dataset:   0%|          | 0/30 [00:00<?, ? examples/s]Casting the dataset: 100%|██████████| 30/30 [00:00<00:00, 11816.05 examples/s]
[Inject canaries] After injection, train size = 36748 (total injected examples = 30)
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/vocab.json
loading file merges.txt from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/merges.txt
loading file tokenizer.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/config.json
Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 50257
}

`torch_dtype` is deprecated! Use `dtype` instead!
loading weights file model.safetensors from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/model.safetensors
Instantiating GPT2LMHeadModel model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

loading configuration file generation_config.json from cache at /home/luongog/.cache/huggingface/hub/models--gpt2-medium/snapshots/6dcaa7a952f72f9298047fd5137cd6e4f05f41da/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Could not locate the custom_generate/generate.py inside gpt2-medium.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0%|          | 0/36748 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2000/36748 [00:00<00:01, 17896.35 examples/s]Running tokenizer on dataset:  11%|█         | 4000/36748 [00:00<00:01, 18624.77 examples/s]Running tokenizer on dataset:  19%|█▉        | 7000/36748 [00:00<00:01, 19098.23 examples/s]Running tokenizer on dataset:  24%|██▍       | 9000/36748 [00:00<00:01, 18889.66 examples/s]Running tokenizer on dataset:  30%|██▉       | 11000/36748 [00:00<00:01, 18728.64 examples/s]Running tokenizer on dataset:  35%|███▌      | 13000/36748 [00:00<00:01, 18547.23 examples/s]Running tokenizer on dataset:  41%|████      | 15000/36748 [00:00<00:01, 18440.65 examples/s]Running tokenizer on dataset:  46%|████▋     | 17000/36748 [00:00<00:01, 18374.39 examples/s]Running tokenizer on dataset:  52%|█████▏    | 19000/36748 [00:01<00:00, 18365.06 examples/s]Running tokenizer on dataset:  57%|█████▋    | 21000/36748 [00:01<00:00, 18369.40 examples/s]Running tokenizer on dataset:  63%|██████▎   | 23000/36748 [00:01<00:00, 18446.52 examples/s]Running tokenizer on dataset:  68%|██████▊   | 25000/36748 [00:01<00:00, 18668.07 examples/s]Running tokenizer on dataset:  73%|███████▎  | 27000/36748 [00:01<00:00, 18384.93 examples/s]Running tokenizer on dataset:  79%|███████▉  | 29000/36748 [00:01<00:00, 18454.37 examples/s]Running tokenizer on dataset:  84%|████████▍ | 31000/36748 [00:01<00:00, 13980.51 examples/s]Running tokenizer on dataset:  90%|████████▉ | 33000/36748 [00:01<00:00, 15002.47 examples/s]Running tokenizer on dataset:  95%|█████████▌| 35000/36748 [00:02<00:00, 15803.86 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:02<00:00, 13535.52 examples/s]Running tokenizer on dataset: 100%|██████████| 36748/36748 [00:02<00:00, 16765.41 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36748 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|▌         | 2000/36748 [00:00<00:02, 16515.38 examples/s]Grouping texts in chunks of 512:  11%|█         | 4000/36748 [00:00<00:01, 16824.13 examples/s]Grouping texts in chunks of 512:  16%|█▋        | 6000/36748 [00:00<00:01, 17273.20 examples/s]Grouping texts in chunks of 512:  22%|██▏       | 8000/36748 [00:00<00:01, 16733.84 examples/s]Grouping texts in chunks of 512:  27%|██▋       | 10000/36748 [00:00<00:01, 16942.95 examples/s]Grouping texts in chunks of 512:  33%|███▎      | 12000/36748 [00:00<00:01, 16757.95 examples/s]Grouping texts in chunks of 512:  38%|███▊      | 14000/36748 [00:00<00:01, 16709.64 examples/s]Grouping texts in chunks of 512:  44%|████▎     | 16000/36748 [00:00<00:01, 16645.08 examples/s]Grouping texts in chunks of 512:  49%|████▉     | 18000/36748 [00:01<00:01, 16743.58 examples/s]Grouping texts in chunks of 512:  54%|█████▍    | 20000/36748 [00:01<00:00, 16795.45 examples/s]Grouping texts in chunks of 512:  60%|█████▉    | 22000/36748 [00:01<00:00, 17145.88 examples/s]Grouping texts in chunks of 512:  65%|██████▌   | 24000/36748 [00:01<00:00, 17219.51 examples/s]Grouping texts in chunks of 512:  71%|███████   | 26000/36748 [00:01<00:00, 17245.56 examples/s]Grouping texts in chunks of 512:  76%|███████▌  | 28000/36748 [00:01<00:00, 17210.57 examples/s]Grouping texts in chunks of 512:  82%|████████▏ | 30000/36748 [00:01<00:00, 17264.70 examples/s]Grouping texts in chunks of 512:  87%|████████▋ | 32000/36748 [00:01<00:00, 17327.29 examples/s]Grouping texts in chunks of 512:  93%|█████████▎| 34000/36748 [00:01<00:00, 17493.72 examples/s]Grouping texts in chunks of 512:  98%|█████████▊| 36000/36748 [00:02<00:00, 17608.78 examples/s]Grouping texts in chunks of 512: 100%|██████████| 36748/36748 [00:02<00:00, 15279.06 examples/s]
model_params (million) 354.823168
model_params (million) 354.823168
12/31/2025 15:35:41 - INFO - __main__ - ***** Running training *****
12/31/2025 15:35:41 - INFO - __main__ -   Num examples = 4653
12/31/2025 15:35:41 - INFO - __main__ -   Num Epochs = 20
12/31/2025 15:35:41 - INFO - __main__ -   Instantaneous batch size per device = 1
12/31/2025 15:35:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
12/31/2025 15:35:41 - INFO - __main__ -   Gradient Accumulation steps = 8
12/31/2025 15:35:41 - INFO - __main__ -   Total optimization steps = 11640
training epoch 0
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
*************end of epoch 0 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 The first recorded use of the word "'
   Clean Generated:'The first recorded use of the word "'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two " " "'
   Clean Generated:'The first of the two " " "'
   Match:          False
threshold is:  2.6612021923065186
correct cnt is:  307 all is:  4653 ratio is:  0.06597893831936386
epoch 0: perplexity: 20.220046799843136 perplexity_train: 20.200406621737248
____
0.06597893831936386
20.220046799843136
20.200406621737248
_____
training epoch 1
*************end of epoch 1 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = The Battle of'
   Clean Generated:'= = = = = The Battle of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first episode of the series was released'
   Clean Generated:'The first episode of the series was released'
   Match:          False
threshold is:  2.6536056995391846
correct cnt is:  606 all is:  4653 ratio is:  0.13023855577047067
epoch 1: perplexity: 20.23897743273529 perplexity_train: 18.216447867193168
____
0.13023855577047067
20.23897743273529
18.216447867193168
_____
training epoch 2
*************end of epoch 2 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two " " @'
   Clean Generated:'The first of the two " " @'
   Match:          False
threshold is:  2.646695852279663
correct cnt is:  932 all is:  4653 ratio is:  0.20030088115194497
epoch 2: perplexity: 20.270091042629016 perplexity_train: 16.890534527887905
____
0.20030088115194497
20.270091042629016
16.890534527887905
_____
training epoch 3
*************end of epoch 3 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Theatre ='
   Clean Generated:'= = = = = Theatre ='
   Match:          False
threshold is:  2.6473429203033447
correct cnt is:  1283 all is:  4653 ratio is:  0.2757360842467225
epoch 3: perplexity: 20.267220582707033 perplexity_train: 15.917104795131994
____
0.2757360842467225
20.267220582707033
15.917104795131994
_____
training epoch 4
*************end of epoch 4 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two new series of'
   Clean Generated:'The first of the two new series of'
   Match:          False
threshold is:  2.6488945484161377
correct cnt is:  1689 all is:  4653 ratio is:  0.36299161831076726
epoch 4: perplexity: 20.354147169270096 perplexity_train: 15.112857819491271
____
0.36299161831076726
20.354147169270096
15.112857819491271
_____
training epoch 5
*************end of epoch 5 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodoric of'
   Clean Generated:'= = = = Theodoric of'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Themes = ='
   Clean Generated:'= = = = Themes = ='
   Match:          False
threshold is:  2.663865566253662
correct cnt is:  2183 all is:  4653 ratio is:  0.4691596819256394
epoch 5: perplexity: 20.387022339566474 perplexity_train: 14.513425251732507
____
0.4691596819256394
20.387022339566474
14.513425251732507
_____
training epoch 6
*************end of epoch 6 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Themes = ='
   Clean Generated:'= = = = Themes = ='
   Match:          False
threshold is:  2.6515846252441406
correct cnt is:  2360 all is:  4653 ratio is:  0.5071996561358263
epoch 6: perplexity: 20.480502055207825 perplexity_train: 14.028776139756369
____
0.5071996561358263
20.480502055207825
14.028776139756369
_____
training epoch 7
*************end of epoch 7 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = Theodora'
   Clean Generated:'= = = = = Theodora'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two new series of'
   Clean Generated:'The first of the two new series of'
   Match:          False
threshold is:  2.65958833694458
correct cnt is:  2704 all is:  4653 ratio is:  0.581130453470879
epoch 7: perplexity: 20.74218564778855 perplexity_train: 13.633321140431931
____
0.581130453470879
20.74218564778855
13.633321140431931
_____
training epoch 8
*************end of epoch 8 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6598920822143555
correct cnt is:  2894 all is:  4653 ratio is:  0.6219643240919837
epoch 8: perplexity: 20.75152946580039 perplexity_train: 13.329014101097838
____
0.6219643240919837
20.75152946580039
13.329014101097838
_____
training epoch 9
*************end of epoch 9 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.66347336769104
correct cnt is:  3082 all is:  4653 ratio is:  0.6623683644960241
epoch 9: perplexity: 20.915796648506287 perplexity_train: 13.085760052182104
____
0.6623683644960241
20.915796648506287
13.085760052182104
_____
training epoch 10
*************end of epoch 10 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = Themes = ='
   Clean Generated:'= = = = Themes = ='
   Match:          False
threshold is:  2.659393072128296
correct cnt is:  3176 all is:  4653 ratio is:  0.6825703846980443
epoch 10: perplexity: 20.96304419918747 perplexity_train: 12.881388403997525
____
0.6825703846980443
20.96304419918747
12.881388403997525
_____
training epoch 11
*************end of epoch 11 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6678466796875
correct cnt is:  3346 all is:  4653 ratio is:  0.7191059531485063
epoch 11: perplexity: 21.08316360741164 perplexity_train: 12.72089947971665
____
0.7191059531485063
21.08316360741164
12.72089947971665
_____
training epoch 12
*************end of epoch 12 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6736812591552734
correct cnt is:  3468 all is:  4653 ratio is:  0.7453255963894262
epoch 12: perplexity: 21.204325188922972 perplexity_train: 12.601566176524535
____
0.7453255963894262
21.204325188922972
12.601566176524535
_____
training epoch 13
*************end of epoch 13 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = Theodora'
   Clean Generated:'= = = = = Theodora'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6669039726257324
correct cnt is:  3479 all is:  4653 ratio is:  0.7476896625832796
epoch 13: perplexity: 21.174569161855523 perplexity_train: 12.504853488056117
____
0.7476896625832796
21.174569161855523
12.504853488056117
_____
training epoch 14
*************end of epoch 14 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = Theodora'
   Clean Generated:'= = = = = Theodora'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6748197078704834
correct cnt is:  3580 all is:  4653 ratio is:  0.7693960885450247
epoch 14: perplexity: 21.248728617747897 perplexity_train: 12.43882666641913
____
0.7693960885450247
21.248728617747897
12.43882666641913
_____
training epoch 15
*************end of epoch 15 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.6642000675201416
correct cnt is:  3528 all is:  4653 ratio is:  0.758220502901354
epoch 15: perplexity: 21.250537289451202 perplexity_train: 12.39883409625775
____
0.758220502901354
21.250537289451202
12.39883409625775
_____
training epoch 16
*************end of epoch 16 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = Theodora'
   Clean Generated:'= = = = = Theodora'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two new series of'
   Clean Generated:'The first of the two new series of'
   Match:          False
threshold is:  2.671267032623291
correct cnt is:  3603 all is:  4653 ratio is:  0.7743391360412637
epoch 16: perplexity: 21.281140837127573 perplexity_train: 12.370080865024814
____
0.7743391360412637
21.281140837127573
12.370080865024814
_____
training epoch 17
*************end of epoch 17 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = = Theodora'
   Clean Generated:'= = = = = Theodora'
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 The first of the two new series of'
   Clean Generated:'The first of the two new series of'
   Match:          False
threshold is:  2.6706466674804688
correct cnt is:  3614 all is:  4653 ratio is:  0.7767032022351171
epoch 17: perplexity: 21.30217713484479 perplexity_train: 12.358987787472786
____
0.7767032022351171
21.30217713484479
12.358987787472786
_____
training epoch 18
*************end of epoch 18 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.67057466506958
correct cnt is:  3601 all is:  4653 ratio is:  0.7739093058241995
epoch 18: perplexity: 21.293087960173278 perplexity_train: 12.354810197435345
____
0.7739093058241995
21.293087960173278
12.354810197435345
_____
training epoch 19
*************end of epoch 19 eval 

[DEBUG GENERATION]
   Prefix:         'Gandalf '
   Target Suffix:  'saved a village in the titanic.'
   Raw Generated:  '
 = = = = Theodora ='
   Clean Generated:'= = = = Theodora ='
   Match:          False

[DEBUG GENERATION]
   Prefix:         'The secure verification code is: '
   Target Suffix:  'OUS84V1t2U1I'
   Raw Generated:  '
 = = = = = Themes ='
   Clean Generated:'= = = = = Themes ='
   Match:          False
threshold is:  2.671760320663452
correct cnt is:  3615 all is:  4653 ratio is:  0.7769181173436492
epoch 19: perplexity: 21.303279270507574 perplexity_train: 12.350651687244891
____
0.7769181173436492
21.303279270507574
12.350651687244891
_____
*************end of training 
threshold is:  2.671760320663452
correct cnt is:  3615 all is:  4653 ratio is:  0.7769181173436492
end of training perplexity: 21.303279270507574 perplexity_train: 12.350651687244891
____
0.7769181173436492
21.303279270507574
12.350651687244891
_____
    -> Timing: 19h 31m 50s

>>> [3/3] Locating Logs and Running Evaluation...
Log M_noC found: wikipedia/experiments/run_20251230_200137/M_noC/training_output_gpt2-medium/canary_loss_log.csv
Log M_C found:   wikipedia/experiments/run_20251230_200137/M_C/training_output_gpt2-medium/canary_loss_log.csv
--- 1. LOADING DATA ---
--- DIAGNOSTIC: EPOCH 0 CHECK ---
 > Average Suffix Loss at Epoch 0: Target=5.4506, Reference=5.5464
---------------------------------
--- 2. PRE-COMPUTING BASELINES ---
   -> Computing historical minimum loss for Reference model...
--- 3. COMPUTING SCORES ---
   -> Merging data and computing scores...
--- 4. RUNNING EPOCH ANALYSIS ---
Epoch 0: MIA=30.00% | EM=0.00% | PPL=187.23 | CTX=0.0203
Epoch 1: MIA=36.67% | EM=0.00% | PPL=173.07 | CTX=0.0314
Epoch 2: MIA=43.33% | EM=0.00% | PPL=160.03 | CTX=0.0462
Epoch 3: MIA=43.33% | EM=0.00% | PPL=148.67 | CTX=0.0571
Epoch 4: MIA=46.67% | EM=0.00% | PPL=146.26 | CTX=0.0597
Epoch 5: MIA=56.67% | EM=0.00% | PPL=139.43 | CTX=0.0701
Epoch 6: MIA=60.00% | EM=0.00% | PPL=137.20 | CTX=0.0713
Epoch 7: MIA=73.33% | EM=0.00% | PPL=133.28 | CTX=0.0760
Epoch 8: MIA=56.67% | EM=0.00% | PPL=130.88 | CTX=0.0779
Epoch 9: MIA=53.33% | EM=0.00% | PPL=129.19 | CTX=0.0810
Epoch 10: MIA=80.00% | EM=0.00% | PPL=126.41 | CTX=0.0857
Epoch 11: MIA=66.67% | EM=0.00% | PPL=123.81 | CTX=0.0890
Epoch 12: MIA=73.33% | EM=0.00% | PPL=121.49 | CTX=0.0923
Epoch 13: MIA=70.00% | EM=0.00% | PPL=123.25 | CTX=0.0896
Epoch 14: MIA=66.67% | EM=0.00% | PPL=121.38 | CTX=0.0918
Epoch 15: MIA=70.00% | EM=0.00% | PPL=121.79 | CTX=0.0915
Epoch 16: MIA=70.00% | EM=0.00% | PPL=123.41 | CTX=0.0896
Epoch 17: MIA=83.33% | EM=0.00% | PPL=119.97 | CTX=0.0942
Epoch 18: MIA=76.67% | EM=0.00% | PPL=119.69 | CTX=0.0956
Epoch 19: MIA=83.33% | EM=0.00% | PPL=122.71 | CTX=0.0913
--- 5. SAVING RESULTS ---
Done. Results in: wikipedia/experiments/run_20251230_200137/results

==================================================================
EXPERIMENT FINISHED SUCCESSFULLY!
Results are available in: wikipedia/experiments/run_20251230_200137/results
    -> Timing: 39h 5m 38s
==================================================================
